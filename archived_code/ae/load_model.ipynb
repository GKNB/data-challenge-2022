{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2899c1cf-cd00-4f58-ad22-55ac5b23c20e",
   "metadata": {},
   "source": [
    "# First load the model and output the performance of each model on val and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038d9792-8cc8-474a-85bb-425396ec82c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in /global/common/software/nersc/pm-2022q2/sw/tensorflow/2.9.0/lib/python3.9/site-packages (from sklearn) (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /global/common/software/nersc/pm-2022q2/sw/tensorflow/2.9.0/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /global/common/software/nersc/pm-2022q2/sw/tensorflow/2.9.0/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /global/common/software/nersc/pm-2022q2/sw/tensorflow/2.9.0/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /global/common/software/nersc/pm-2022q2/sw/tensorflow/2.9.0/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=c5cb02a11952181426a7f88077ec8da0f525e0ad04d8410e7c1e2bdab861b497\n",
      "  Stored in directory: /global/u2/t/tianle/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1f2573-645c-42b0-b22a-3c053e207cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datetime\n",
      "  Downloading DateTime-4.5-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz in /global/common/software/nersc/pm-2022q2/sw/tensorflow/2.9.0/lib/python3.9/site-packages (from datetime) (2022.1)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-5.4.0-cp39-cp39-manylinux2010_x86_64.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 20.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /global/common/software/nersc/pm-2022q2/sw/tensorflow/2.9.0/lib/python3.9/site-packages (from zope.interface->datetime) (61.2.0)\n",
      "Installing collected packages: zope.interface, datetime\n",
      "Successfully installed datetime-4.5 zope.interface-5.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47fe76f-5628-4b34-9e0e-1d5caf125ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "import h5py\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, Dense, Activation, Flatten, Conv2D, Conv2DTranspose, MaxPooling2D, BatchNormalization, Reshape, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28bab18-9333-4fdb-8423-7b8a41f6dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8530, 192, 192, 2)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('processed_data_ae/np_data.h5', 'r') as hf:\n",
    "    data = hf['np_data'][:]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7315461-539c-42ff-ab4c-8c28a1f62287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.913855 -16.306587\n"
     ]
    }
   ],
   "source": [
    "print(np.max(data), np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10c3e72-07e0-42bd-95bd-4ba9adb6ad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5118, 192, 192, 2)\n",
      "(1706, 192, 192, 2)\n",
      "(1706, 192, 192, 2)\n",
      "19.913855 18.198473 19.736868 -16.306587 -16.134562 -16.26034\n"
     ]
    }
   ],
   "source": [
    "#First split data into train+validation and test set\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "#Next split training again into train and validation\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(np.max(X_train), np.max(X_val), np.max(X_test), np.min(X_train), np.min(X_val), np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05a2bb89-01b2-4d64-a2e5-faef62392872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(latent_dim):\n",
    "    '''\n",
    "    return an encoder which encodes the input image into a latent vector with dimension latent_dim\n",
    "    '''\n",
    "    \n",
    "    X_input = Input((192, 192, 2))\n",
    "    \n",
    "    #FIXME Should we add BN layer? I currently add that between conv and relu for the first 4 sets of layers\n",
    "    X = Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    #FIXME Should we add some dropout layer to regularize the model? \n",
    "    #I didn't do that, but need to look at train/val error\n",
    "    \n",
    "    X = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(units=latent_dim)(X)\n",
    "    #FIXME Should we add an activation layer here? I didn't do it\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a60387f-a284-459f-b867-2c945f70daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(latent_dim):\n",
    "    '''\n",
    "    return an encoder which encodes the input image into a latent vector with dimension latent_dim\n",
    "    '''\n",
    "    \n",
    "    X_input = Input((latent_dim))\n",
    "    \n",
    "    X = Dense(units=3*3*64, input_dim=latent_dim)(X_input)\n",
    "    X = Reshape((3,3,64))(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=64, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=64, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=32, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=32, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=16, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=16, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2D(filters=2, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)    \n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6082fe6a-13ed-49c1-a72a-fd92d42218b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb9ffe9-e08e-44f8-9182-ad4abdc89e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 7ms/step - loss: 0.3671\n",
      "val loss:  0.3671454191207886\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.3659\n",
      "test loss:  0.36588191986083984\n"
     ]
    }
   ],
   "source": [
    "encoder_4 = encoder(4)\n",
    "decoder_4 = decoder(4)\n",
    "autoencoder_4 = Autoencoder(encoder_4, decoder_4)\n",
    "autoencoder_4.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_4.load_weights(\"autoencoder_4/ckp/\")\n",
    "print(\"val loss: \", autoencoder_4.evaluate(X_val, X_val))\n",
    "print(\"test loss: \", autoencoder_4.evaluate(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9919c1ba-92af-4b2c-8975-adbda2a3ee1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 7ms/step - loss: 0.2130\n",
      "val loss:  0.2129608690738678\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.2150\n",
      "test loss:  0.21500757336616516\n"
     ]
    }
   ],
   "source": [
    "encoder_18 = encoder(18)\n",
    "decoder_18 = decoder(18)\n",
    "autoencoder_18 = Autoencoder(encoder_18, decoder_18)\n",
    "autoencoder_18.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_18.load_weights(\"autoencoder_18/ckp/\")\n",
    "print(\"val loss: \", autoencoder_18.evaluate(X_val, X_val))\n",
    "print(\"test loss: \", autoencoder_18.evaluate(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80b89bcc-2569-485b-bf39-eba5d9cbae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 7ms/step - loss: 0.1667\n",
      "val loss:  0.16668541729450226\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.1688\n",
      "test loss:  0.16877079010009766\n"
     ]
    }
   ],
   "source": [
    "encoder_36 = encoder(36)\n",
    "decoder_36 = decoder(36)\n",
    "autoencoder_36 = Autoencoder(encoder_36, decoder_36)\n",
    "autoencoder_36.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_36.load_weights(\"autoencoder_36/ckp/\")\n",
    "print(\"val loss: \", autoencoder_36.evaluate(X_val, X_val))\n",
    "print(\"test loss: \", autoencoder_36.evaluate(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec30d698-2204-49a2-99f0-402872b7a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 7ms/step - loss: 0.1427\n",
      "val loss:  0.14272011816501617\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1447\n",
      "test loss:  0.14470382034778595\n"
     ]
    }
   ],
   "source": [
    "encoder_72 = encoder(72)\n",
    "decoder_72 = decoder(72)\n",
    "autoencoder_72 = Autoencoder(encoder_72, decoder_72)\n",
    "autoencoder_72.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_72.load_weights(\"autoencoder_72/ckp/\")\n",
    "print(\"val loss: \", autoencoder_72.evaluate(X_val, X_val))\n",
    "print(\"test loss: \", autoencoder_72.evaluate(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35fdd959-5d46-437c-b998-ea19fe5118a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 7ms/step - loss: 0.1120\n",
      "val loss:  0.11195648461580276\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1144\n",
      "test loss:  0.11435379087924957\n"
     ]
    }
   ],
   "source": [
    "encoder_144 = encoder(144)\n",
    "decoder_144 = decoder(144)\n",
    "autoencoder_144 = Autoencoder(encoder_144, decoder_144)\n",
    "autoencoder_144.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_144.load_weights(\"autoencoder_144/ckp/\")\n",
    "print(\"val loss: \", autoencoder_144.evaluate(X_val, X_val))\n",
    "print(\"test loss: \", autoencoder_144.evaluate(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bac0676-6d44-466f-8d39-f97f8dd5c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_hier_2(Model):\n",
    "    def __init__(self, encoder, decoder, ae):\n",
    "        super(Autoencoder_hier_2, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.ae = ae\n",
    "        self.ae.trainable = False\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        latent_1 = self.ae.encoder(x)\n",
    "        latent_all = Concatenate(axis=-1)([encoded, latent_1])\n",
    "        decoded = self.decoder(latent_all)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c65e422-9753-4674-8da5-de48102ba5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 8ms/step - loss: 0.1828\n",
      "val loss:  0.18277886509895325\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.1847\n",
      "test loss:  0.18471655249595642\n"
     ]
    }
   ],
   "source": [
    "encoder_18_sub2 = encoder(18)\n",
    "decoder_18_sub2 = decoder(36)\n",
    "autoencoder_18_hier_2 = Autoencoder_hier_2(encoder_18_sub2, decoder_18_sub2, autoencoder_18)\n",
    "autoencoder_18_hier_2.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_18_hier_2.load_weights(\"autoencoder_18_hier_2/ckp/\")\n",
    "print(\"val loss: \", autoencoder_18_hier_2.evaluate(X_val, X_val))\n",
    "print(\"test loss: \", autoencoder_18_hier_2.evaluate(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "286a4d44-621a-4edc-acdf-b100f556624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_hier_3(Model):\n",
    "    def __init__(self, encoder, decoder, ae):\n",
    "        super(Autoencoder_hier_3, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.ae = ae\n",
    "        self.ae.trainable = False\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        latent_1 = self.ae.ae.encoder(x)\n",
    "        latent_2 = self.ae.encoder(x)\n",
    "        latent_all = Concatenate(axis=-1)([encoded, latent_1, latent_2])\n",
    "        decoded = self.decoder(latent_all)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3541ee17-ca11-4eab-8cfd-7e48db0c2915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 10ms/step - loss: 0.1666\n",
      "val loss:  0.16658547520637512\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 0.1686\n",
      "test loss:  0.16864728927612305\n"
     ]
    }
   ],
   "source": [
    "encoder_18_sub3 = encoder(18)\n",
    "decoder_18_sub3 = decoder(54)\n",
    "autoencoder_18_hier_3 = Autoencoder_hier_3(encoder_18_sub3, decoder_18_sub3, autoencoder_18_hier_2)\n",
    "autoencoder_18_hier_3.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_18_hier_3.load_weights(\"autoencoder_18_hier_3/ckp/\")\n",
    "print(\"val loss: \", autoencoder_18_hier_3.evaluate(X_val, X_val))\n",
    "print(\"test loss: \", autoencoder_18_hier_3.evaluate(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7753ba1d-3c9d-4161-b990-4a375a1d33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_hier_4(Model):\n",
    "    def __init__(self, encoder, decoder, ae):\n",
    "        super(Autoencoder_hier_4, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.ae = ae\n",
    "        self.ae.trainable = False\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        latent_1 = self.ae.ae.ae.encoder(x)\n",
    "        latent_2 = self.ae.ae.encoder(x)\n",
    "        latent_3 = self.ae.encoder(x)\n",
    "        latent_all = Concatenate(axis=-1)([encoded, latent_1, latent_2, latent_3])\n",
    "        decoded = self.decoder(latent_all)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c589ae63-ca19-4d1c-b252-a4634e0a2f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 11ms/step - loss: 0.1572\n",
      "val loss:  0.15718421339988708\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 0.1589\n",
      "test loss:  0.15886655449867249\n"
     ]
    }
   ],
   "source": [
    "encoder_18_sub4 = encoder(18)\n",
    "decoder_18_sub4 = decoder(72)\n",
    "autoencoder_18_hier_4 = Autoencoder_hier_4(encoder_18_sub4, decoder_18_sub4, autoencoder_18_hier_3)\n",
    "autoencoder_18_hier_4.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder_18_hier_4.load_weights(\"autoencoder_18_hier_4/ckp/\")\n",
    "print(\"val loss: \", autoencoder_18_hier_4.evaluate(X_val, X_val))\n",
    "print(\"test loss: \", autoencoder_18_hier_4.evaluate(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824366d-4696-44e0-b4f2-c397fb71d0fc",
   "metadata": {},
   "source": [
    "# Look at the performance of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4bd1105-8fa8-4897-add5-9dbcf8224f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71f2f84a-977d-428d-8367-9c17e3856d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_util(data_train, data_val, data_test, n_components):\n",
    "    N_train = data_train.shape[0]\n",
    "    N_val   = data_val.shape[0]\n",
    "    N_test  = data_test.shape[0]\n",
    "    \n",
    "    data_train_in = data_train.reshape(N_train,-1)\n",
    "    data_val_in   = data_val.reshape(N_val,-1)\n",
    "    data_test_in  = data_test.reshape(N_test,-1)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(data_train_in)\n",
    "    \n",
    "    data_train_pca = pca.transform(data_train_in)\n",
    "    data_val_pca   = pca.transform(data_val_in)\n",
    "    data_test_pca  = pca.transform(data_test_in)\n",
    "    \n",
    "    data_train_out = pca.inverse_transform(data_train_pca)\n",
    "    data_val_out   = pca.inverse_transform(data_val_pca)\n",
    "    data_test_out  = pca.inverse_transform(data_test_pca)\n",
    "    \n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    train_loss = mse(data_train_in, data_train_out).numpy()\n",
    "    val_loss   = mse(data_val_in, data_val_out).numpy()\n",
    "    test_loss  = mse(data_test_in, data_test_out).numpy()\n",
    "    \n",
    "    return pca, train_loss, val_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9464dd4-3b91-40c0-823e-af39bd26e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_all_together(n_components):\n",
    "    pca, train_loss, val_loss, test_loss = pca_util(X_train, X_val, X_test, n_components)\n",
    "    print(\"Latent_dim = {}, train_loss = {}, val_loss = {}, test_loss = {}\".format(n_components, train_loss, val_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e9e3ac3-2101-4948-9949-22eda9de016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent_dim = 4, train_loss = 0.6355684995651245, val_loss = 0.6420252919197083, test_loss = 0.6480738520622253\n",
      "Latent_dim = 18, train_loss = 0.28684109449386597, val_loss = 0.2909291386604309, test_loss = 0.29305580258369446\n",
      "Latent_dim = 36, train_loss = 0.18933157622814178, val_loss = 0.19471284747123718, test_loss = 0.19567106664180756\n",
      "Latent_dim = 72, train_loss = 0.11902341991662979, val_loss = 0.1252274513244629, test_loss = 0.1261128932237625\n",
      "Latent_dim = 144, train_loss = 0.0722622275352478, val_loss = 0.07911373674869537, test_loss = 0.07992440462112427\n",
      "Latent_dim = 2304, train_loss = 0.005508307367563248, val_loss = 0.019411679357290268, test_loss = 0.019859718158841133\n"
     ]
    }
   ],
   "source": [
    "pca_all_together(4)\n",
    "pca_all_together(18)\n",
    "pca_all_together(36)\n",
    "pca_all_together(72)\n",
    "pca_all_together(144)\n",
    "pca_all_together(144*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a3311-3c66-40a8-9bcf-274709108956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.0",
   "language": "python",
   "name": "tensorflow-2.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
