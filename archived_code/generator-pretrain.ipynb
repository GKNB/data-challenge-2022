{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7429742",
   "metadata": {},
   "source": [
    "# Test if GPU works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daff0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01dcffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 02:22:44.353326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:44.358459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:44.359624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:44.361191: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-13 02:22:44.362978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:44.364129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:44.365260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:44.704432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:44.705581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:44.706677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:44.707761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 46627 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:4b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02afac88",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512546d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.layers import Input, Lambda, LeakyReLU, Add, Dense, Activation, Flatten, Conv2D, Conv2DTranspose, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform, constant, TruncatedNormal\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1933e215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-9.77540612e-01  4.69394028e-01]\n",
      "   [-9.88514364e-01  4.83604759e-01]\n",
      "   [-9.88690615e-01  4.81826991e-01]\n",
      "   ...\n",
      "   [-8.59407544e-01 -1.84500366e-02]\n",
      "   [-1.00063288e+00 -3.86129171e-02]\n",
      "   [-1.01613510e+00 -2.85768192e-02]]\n",
      "\n",
      "  [[-9.58535731e-01  4.69260812e-01]\n",
      "   [-9.51555133e-01  5.05433917e-01]\n",
      "   [-9.31993306e-01  4.93905038e-01]\n",
      "   ...\n",
      "   [-9.05861676e-01 -2.17433125e-02]\n",
      "   [-1.00226498e+00 -1.75988879e-02]\n",
      "   [-1.02119088e+00 -1.55116441e-02]]\n",
      "\n",
      "  [[-8.74235630e-01  4.92436051e-01]\n",
      "   [-8.93230438e-01  5.30668914e-01]\n",
      "   [-8.97067606e-01  4.91552979e-01]\n",
      "   ...\n",
      "   [-8.94055903e-01 -8.23918544e-03]\n",
      "   [-9.50336456e-01 -9.58461594e-03]\n",
      "   [-9.82353866e-01 -3.23360483e-03]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-5.32296360e-01 -9.07888860e-02]\n",
      "   [-6.16073012e-01 -1.89724326e-01]\n",
      "   [-6.43677831e-01 -2.43733719e-01]\n",
      "   ...\n",
      "   [-3.77181351e-01  3.39569092e-01]\n",
      "   [-4.33952421e-01  2.34625027e-01]\n",
      "   [-4.43929911e-01  1.32707179e-01]]\n",
      "\n",
      "  [[-7.47961998e-01 -2.87435383e-01]\n",
      "   [-7.92617738e-01 -3.74220848e-01]\n",
      "   [-8.05779636e-01 -3.66487443e-01]\n",
      "   ...\n",
      "   [-4.29030955e-01  2.08908945e-01]\n",
      "   [-4.53801125e-01  5.48598394e-02]\n",
      "   [-4.24366355e-01 -4.06447873e-02]]\n",
      "\n",
      "  [[-8.10059428e-01  4.83143292e-02]\n",
      "   [-8.10035646e-01 -1.89632154e-03]\n",
      "   [-8.16072822e-01 -1.29768878e-01]\n",
      "   ...\n",
      "   [-5.00356138e-01  2.02375408e-02]\n",
      "   [-4.61575150e-01 -5.11944368e-02]\n",
      "   [-4.56526518e-01 -1.07422285e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.26047921e+00  3.88491601e-01]\n",
      "   [-1.26229858e+00  3.71410012e-01]\n",
      "   [-1.24635947e+00  3.74787450e-01]\n",
      "   ...\n",
      "   [-9.77528334e-01  4.07948568e-02]\n",
      "   [-1.13351703e+00  8.78192931e-02]\n",
      "   [-1.13238299e+00  5.20481495e-03]]\n",
      "\n",
      "  [[-1.22002625e+00  3.56136322e-01]\n",
      "   [-1.19960070e+00  3.76190603e-01]\n",
      "   [-1.16839039e+00  3.68356884e-01]\n",
      "   ...\n",
      "   [-1.04632568e+00  2.95855813e-02]\n",
      "   [-1.16980541e+00  6.36723489e-02]\n",
      "   [-1.16753125e+00  3.08026951e-02]]\n",
      "\n",
      "  [[-1.18083346e+00  3.65006328e-01]\n",
      "   [-1.20569861e+00  3.79645854e-01]\n",
      "   [-1.21447563e+00  3.39645922e-01]\n",
      "   ...\n",
      "   [-1.07902706e+00  3.08375340e-02]\n",
      "   [-1.14368069e+00  5.60167059e-02]\n",
      "   [-1.17513382e+00  5.04857674e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-5.06315708e-01 -4.16750349e-02]\n",
      "   [-5.72971225e-01 -1.08074322e-01]\n",
      "   [-6.12232447e-01 -1.66052684e-01]\n",
      "   ...\n",
      "   [-2.54693907e-02  1.50569320e-01]\n",
      "   [-4.10915464e-02  1.91027910e-01]\n",
      "   [-6.44968823e-02  1.68618172e-01]]\n",
      "\n",
      "  [[-6.66993022e-01 -1.74985886e-01]\n",
      "   [-6.98388219e-01 -2.09262207e-01]\n",
      "   [-7.19723046e-01 -2.44794682e-01]\n",
      "   ...\n",
      "   [-3.06666885e-02  1.05476275e-01]\n",
      "   [-2.50911973e-02  1.38646930e-01]\n",
      "   [-9.36195329e-02  6.37250319e-02]]\n",
      "\n",
      "  [[-6.22664332e-01  6.21278100e-02]\n",
      "   [-6.26019955e-01  1.13940708e-01]\n",
      "   [-6.53815031e-01 -1.16589162e-02]\n",
      "   ...\n",
      "   [-1.35809302e-01  2.92447153e-02]\n",
      "   [-1.70376584e-01  6.84138760e-02]\n",
      "   [-2.20651627e-01  2.09550392e-02]]]\n",
      "\n",
      "\n",
      " [[[-1.38290501e+00  3.43145221e-01]\n",
      "   [-1.38908589e+00  3.81361634e-01]\n",
      "   [-1.36557794e+00  3.27022493e-01]\n",
      "   ...\n",
      "   [-8.44485879e-01  6.56099841e-02]\n",
      "   [-8.44215214e-01  1.29245386e-01]\n",
      "   [-7.95478404e-01  1.42822370e-01]]\n",
      "\n",
      "  [[-1.33917689e+00  3.23864609e-01]\n",
      "   [-1.30435741e+00  3.73013407e-01]\n",
      "   [-1.26849854e+00  3.18452448e-01]\n",
      "   ...\n",
      "   [-9.21694160e-01  8.63110349e-02]\n",
      "   [-9.34634268e-01  9.73182842e-02]\n",
      "   [-9.09379661e-01  1.30078256e-01]]\n",
      "\n",
      "  [[-1.30324268e+00  3.39805692e-01]\n",
      "   [-1.32539356e+00  3.45881104e-01]\n",
      "   [-1.33385623e+00  3.35130870e-01]\n",
      "   ...\n",
      "   [-9.80313599e-01  8.71907845e-02]\n",
      "   [-9.97593582e-01  9.24865678e-02]\n",
      "   [-9.86386418e-01  9.29992944e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-5.98438323e-01 -4.10367660e-02]\n",
      "   [-5.73147893e-01 -6.98508993e-02]\n",
      "   [-5.55911660e-01 -1.05005629e-01]\n",
      "   ...\n",
      "   [-1.64449841e-01  3.93875301e-01]\n",
      "   [-1.79697946e-01  4.41230774e-01]\n",
      "   [-1.94661573e-01  4.24757451e-01]]\n",
      "\n",
      "  [[-6.82861805e-01 -1.74074233e-01]\n",
      "   [-7.22314835e-01 -2.01611802e-01]\n",
      "   [-6.47577405e-01 -8.48870799e-02]\n",
      "   ...\n",
      "   [-1.66753113e-01  3.77813011e-01]\n",
      "   [-1.31541729e-01  4.36277062e-01]\n",
      "   [-8.31313655e-02  3.84667397e-01]]\n",
      "\n",
      "  [[-4.79760349e-01  1.72492832e-01]\n",
      "   [-5.69692016e-01  8.81923884e-02]\n",
      "   [-6.15487099e-01  5.22930175e-02]\n",
      "   ...\n",
      "   [-9.04514417e-02  3.46089393e-01]\n",
      "   [-8.75403285e-02  4.01307553e-01]\n",
      "   [-7.12360665e-02  3.61293942e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 1.82724655e+00 -2.57342398e-01]\n",
      "   [ 1.81573951e+00 -4.58970517e-01]\n",
      "   [ 1.78242409e+00 -2.84535468e-01]\n",
      "   ...\n",
      "   [ 1.75701380e+00 -2.33258799e-01]\n",
      "   [ 1.96638894e+00 -2.67322123e-01]\n",
      "   [ 1.96579897e+00 -2.86968499e-01]]\n",
      "\n",
      "  [[ 1.81541336e+00 -3.12055230e-01]\n",
      "   [ 1.78313470e+00 -4.39453930e-01]\n",
      "   [ 1.69469917e+00 -2.93636441e-01]\n",
      "   ...\n",
      "   [ 2.23102951e+00 -1.21706024e-01]\n",
      "   [ 2.38592720e+00 -2.05794588e-01]\n",
      "   [ 2.36475134e+00 -2.51919180e-01]]\n",
      "\n",
      "  [[ 1.55816853e+00 -2.38853768e-01]\n",
      "   [ 1.55852473e+00 -3.31357062e-01]\n",
      "   [ 1.53113925e+00 -2.60730714e-01]\n",
      "   ...\n",
      "   [ 1.97150731e+00 -9.50898677e-02]\n",
      "   [ 1.98680913e+00 -1.96613237e-01]\n",
      "   [ 1.90590560e+00 -1.33457974e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.08359432e+00 -1.17151129e+00]\n",
      "   [ 2.07471943e+00 -1.22675729e+00]\n",
      "   [ 2.06779814e+00 -1.15509236e+00]\n",
      "   ...\n",
      "   [ 1.64492083e+00 -1.42317224e+00]\n",
      "   [ 1.83432913e+00 -1.48552883e+00]\n",
      "   [ 1.94628036e+00 -1.83102202e+00]]\n",
      "\n",
      "  [[ 1.97289336e+00 -1.45835912e+00]\n",
      "   [ 1.88255358e+00 -1.46877360e+00]\n",
      "   [ 1.76159298e+00 -1.50988936e+00]\n",
      "   ...\n",
      "   [ 1.85763812e+00 -1.65238392e+00]\n",
      "   [ 1.87709415e+00 -1.73724723e+00]\n",
      "   [ 1.45412338e+00 -1.40651667e+00]]\n",
      "\n",
      "  [[ 2.14898825e+00 -1.56710541e+00]\n",
      "   [ 2.03661704e+00 -1.51996708e+00]\n",
      "   [ 1.99574280e+00 -1.49943030e+00]\n",
      "   ...\n",
      "   [ 1.89102280e+00 -1.75246835e+00]\n",
      "   [ 1.52266371e+00 -1.26120603e+00]\n",
      "   [ 1.39658284e+00 -1.19041264e+00]]]\n",
      "\n",
      "\n",
      " [[[ 1.73511720e+00 -1.58225492e-01]\n",
      "   [ 1.77581573e+00 -1.59722432e-01]\n",
      "   [ 1.85958278e+00 -1.69630870e-01]\n",
      "   ...\n",
      "   [ 1.95399368e+00  7.62259588e-02]\n",
      "   [ 2.13303304e+00 -3.52499187e-02]\n",
      "   [ 2.13372183e+00  2.41567623e-02]]\n",
      "\n",
      "  [[ 1.88895035e+00 -1.78873390e-01]\n",
      "   [ 1.89943528e+00 -1.76032871e-01]\n",
      "   [ 1.92625523e+00 -1.72781736e-01]\n",
      "   ...\n",
      "   [ 2.41120529e+00  1.11434154e-01]\n",
      "   [ 2.52411294e+00  4.17990275e-02]\n",
      "   [ 2.46408272e+00 -1.45707605e-02]]\n",
      "\n",
      "  [[ 1.86551785e+00 -1.54106706e-01]\n",
      "   [ 1.92573464e+00 -1.68044195e-01]\n",
      "   [ 1.97654784e+00 -2.26345524e-01]\n",
      "   ...\n",
      "   [ 2.13873768e+00  8.92556161e-02]\n",
      "   [ 2.13405514e+00  7.60787427e-02]\n",
      "   [ 2.06658912e+00  7.13595524e-02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.12645769e+00 -1.05286729e+00]\n",
      "   [ 2.25314283e+00 -1.18491018e+00]\n",
      "   [ 2.32309794e+00 -1.06362081e+00]\n",
      "   ...\n",
      "   [ 1.90253246e+00 -1.23426867e+00]\n",
      "   [ 1.86374474e+00 -1.24167240e+00]\n",
      "   [ 1.94077229e+00 -1.33679187e+00]]\n",
      "\n",
      "  [[ 2.07552290e+00 -1.26585495e+00]\n",
      "   [ 2.03525448e+00 -1.19478142e+00]\n",
      "   [ 1.98951924e+00 -1.23194492e+00]\n",
      "   ...\n",
      "   [ 1.78370476e+00 -1.24249637e+00]\n",
      "   [ 1.86179543e+00 -1.37364709e+00]\n",
      "   [ 1.71686685e+00 -1.28711259e+00]]\n",
      "\n",
      "  [[ 2.21124482e+00 -1.30764735e+00]\n",
      "   [ 2.23378801e+00 -1.21438706e+00]\n",
      "   [ 2.23344874e+00 -1.25646842e+00]\n",
      "   ...\n",
      "   [ 1.96416152e+00 -1.32308447e+00]\n",
      "   [ 1.85505593e+00 -1.21557760e+00]\n",
      "   [ 1.52320504e+00 -9.82996702e-01]]]\n",
      "\n",
      "\n",
      " [[[ 2.25224638e+00  1.88680023e-01]\n",
      "   [ 2.30077815e+00  1.08295809e-02]\n",
      "   [ 2.31017470e+00  1.75898790e-01]\n",
      "   ...\n",
      "   [ 2.26332140e+00  9.03437883e-02]\n",
      "   [ 2.46146274e+00 -5.55469468e-02]\n",
      "   [ 2.41147280e+00  2.81953253e-02]]\n",
      "\n",
      "  [[ 2.22859478e+00  1.55336618e-01]\n",
      "   [ 2.24404144e+00  1.87685098e-02]\n",
      "   [ 2.25474668e+00  2.27152884e-01]\n",
      "   ...\n",
      "   [ 2.74622250e+00  1.73121527e-01]\n",
      "   [ 2.87377167e+00  3.58640179e-02]\n",
      "   [ 2.80690384e+00  4.59626205e-02]]\n",
      "\n",
      "  [[ 2.20554519e+00  1.16983309e-01]\n",
      "   [ 2.26604843e+00  4.29743752e-02]\n",
      "   [ 2.35769439e+00  9.40330103e-02]\n",
      "   ...\n",
      "   [ 2.44051170e+00  1.51982948e-01]\n",
      "   [ 2.42145109e+00  9.07190219e-02]\n",
      "   [ 2.30946589e+00  1.39351264e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.37364793e+00 -6.88030481e-01]\n",
      "   [ 2.22958779e+00 -1.00717688e+00]\n",
      "   [ 2.10662603e+00 -5.43206573e-01]\n",
      "   ...\n",
      "   [ 2.11013460e+00 -1.13091600e+00]\n",
      "   [ 2.05068874e+00 -1.22129762e+00]\n",
      "   [ 1.96258783e+00 -1.14413488e+00]]\n",
      "\n",
      "  [[ 1.95191646e+00 -9.79391038e-01]\n",
      "   [ 1.82115519e+00 -9.52033222e-01]\n",
      "   [ 1.82624435e+00 -8.32196116e-01]\n",
      "   ...\n",
      "   [ 1.91478002e+00 -1.20049429e+00]\n",
      "   [ 1.90742218e+00 -1.22540736e+00]\n",
      "   [ 1.85962701e+00 -1.11194634e+00]]\n",
      "\n",
      "  [[ 2.16503692e+00 -1.03187895e+00]\n",
      "   [ 2.20002031e+00 -9.67551827e-01]\n",
      "   [ 2.25209618e+00 -9.37134922e-01]\n",
      "   ...\n",
      "   [ 2.02236867e+00 -1.24229360e+00]\n",
      "   [ 1.97888768e+00 -1.08468330e+00]\n",
      "   [ 1.70130289e+00 -9.01416242e-01]]]]\n",
      "\n",
      "\n",
      "\n",
      "[[[[-1.0082967   0.5063803 ]\n",
      "   [-1.0076408   0.496684  ]\n",
      "   [-1.0153483   0.48742002]\n",
      "   ...\n",
      "   [-1.0138876  -0.05675393]\n",
      "   [-1.0312574  -0.05658764]\n",
      "   [-1.038773   -0.06110113]]\n",
      "\n",
      "  [[-1.0228026   0.5072131 ]\n",
      "   [-1.0273845   0.48786467]\n",
      "   [-1.0318519   0.48887888]\n",
      "   ...\n",
      "   [-1.0429276  -0.04109716]\n",
      "   [-1.0486563  -0.04828837]\n",
      "   [-1.050016   -0.05197321]]\n",
      "\n",
      "  [[-1.0220301   0.5092414 ]\n",
      "   [-1.0245405   0.49638793]\n",
      "   [-1.019841    0.5100402 ]\n",
      "   ...\n",
      "   [-1.0460666  -0.03366394]\n",
      "   [-1.0501559  -0.04044536]\n",
      "   [-1.0490255  -0.04171631]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.72664803  0.02507771]\n",
      "   [-0.7213037  -0.13348989]\n",
      "   [-0.71492577 -0.04641213]\n",
      "   ...\n",
      "   [-0.38210365 -0.02343687]\n",
      "   [-0.38267955 -0.06319581]\n",
      "   [-0.4127331  -0.12002462]]\n",
      "\n",
      "  [[-0.7480877   0.12088082]\n",
      "   [-0.7381432   0.07600805]\n",
      "   [-0.7379145   0.04180754]\n",
      "   ...\n",
      "   [-0.42519674 -0.07977248]\n",
      "   [-0.44552174 -0.09159866]\n",
      "   [-0.49482    -0.11845989]]\n",
      "\n",
      "  [[-0.75101143  0.15115401]\n",
      "   [-0.7465775   0.19980961]\n",
      "   [-0.76130414  0.17666225]\n",
      "   ...\n",
      "   [-0.4166157  -0.10224824]\n",
      "   [-0.40480304 -0.11197067]\n",
      "   [-0.46661612 -0.12462112]]]\n",
      "\n",
      "\n",
      " [[[-1.2892811   0.40368253]\n",
      "   [-1.2891214   0.38976392]\n",
      "   [-1.2949558   0.38004327]\n",
      "   ...\n",
      "   [-1.107566    0.05559232]\n",
      "   [-1.0986403   0.06440026]\n",
      "   [-1.0864314   0.065083  ]]\n",
      "\n",
      "  [[-1.2891886   0.3897721 ]\n",
      "   [-1.2919662   0.37891254]\n",
      "   [-1.2916192   0.37075835]\n",
      "   ...\n",
      "   [-1.1597724   0.04250245]\n",
      "   [-1.1469434   0.04859149]\n",
      "   [-1.1336607   0.05349895]]\n",
      "\n",
      "  [[-1.3021758   0.38391942]\n",
      "   [-1.3019704   0.3794954 ]\n",
      "   [-1.2955699   0.38384083]\n",
      "   ...\n",
      "   [-1.1921283   0.03026785]\n",
      "   [-1.1834875   0.04182007]\n",
      "   [-1.1749014   0.04529378]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.6040268   0.11406798]\n",
      "   [-0.65279365 -0.0677899 ]\n",
      "   [-0.6300962   0.05849169]\n",
      "   ...\n",
      "   [-0.04454392  0.13686006]\n",
      "   [-0.062627    0.10846151]\n",
      "   [-0.09827382  0.08859722]]\n",
      "\n",
      "  [[-0.6294337   0.18179834]\n",
      "   [-0.6470031   0.09184599]\n",
      "   [-0.64076924  0.13773893]\n",
      "   ...\n",
      "   [-0.05233197  0.12665303]\n",
      "   [-0.0832551   0.07867835]\n",
      "   [-0.19012569  0.05191666]]\n",
      "\n",
      "  [[-0.69425726  0.19782685]\n",
      "   [-0.67537844  0.19193293]\n",
      "   [-0.6743803   0.23300359]\n",
      "   ...\n",
      "   [-0.08717135  0.09555063]\n",
      "   [-0.12272143  0.03346464]\n",
      "   [-0.22941156  0.0504833 ]]]\n",
      "\n",
      "\n",
      " [[[-1.3844249   0.35700405]\n",
      "   [-1.3812684   0.35130054]\n",
      "   [-1.3860825   0.34290406]\n",
      "   ...\n",
      "   [-0.6224672   0.26646653]\n",
      "   [-0.5940387   0.23894592]\n",
      "   [-0.5827387   0.306243  ]]\n",
      "\n",
      "  [[-1.36855     0.35250574]\n",
      "   [-1.3708781   0.35870484]\n",
      "   [-1.371206    0.3332127 ]\n",
      "   ...\n",
      "   [-0.6392918   0.2689369 ]\n",
      "   [-0.6191335   0.23604532]\n",
      "   [-0.60676956  0.29666522]]\n",
      "\n",
      "  [[-1.3736316   0.35064957]\n",
      "   [-1.3729696   0.35001174]\n",
      "   [-1.35804     0.34343958]\n",
      "   ...\n",
      "   [-0.6703837   0.26654497]\n",
      "   [-0.6513633   0.21882348]\n",
      "   [-0.6410835   0.28030077]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.40062925  0.26535422]\n",
      "   [-0.5034919   0.03656741]\n",
      "   [-0.4769608   0.17346837]\n",
      "   ...\n",
      "   [-0.00422237  0.399575  ]\n",
      "   [ 0.03560884  0.4095432 ]\n",
      "   [ 0.05109404  0.4349035 ]]\n",
      "\n",
      "  [[-0.37348068  0.29988378]\n",
      "   [-0.4431188   0.1284983 ]\n",
      "   [-0.43734664  0.23857166]\n",
      "   ...\n",
      "   [ 0.03043903  0.41851196]\n",
      "   [ 0.05025249  0.41844374]\n",
      "   [ 0.04961702  0.4475729 ]]\n",
      "\n",
      "  [[-0.41687223  0.28470218]\n",
      "   [-0.37214494  0.20088358]\n",
      "   [-0.3498782   0.340293  ]\n",
      "   ...\n",
      "   [ 0.05289508  0.44049472]\n",
      "   [ 0.06320071  0.43663073]\n",
      "   [ 0.02912462  0.44234186]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 1.6118251  -0.3510431 ]\n",
      "   [ 1.6172984  -0.38273397]\n",
      "   [ 1.6157022  -0.3822666 ]\n",
      "   ...\n",
      "   [ 2.0703104  -0.432852  ]\n",
      "   [ 2.0617769  -0.42303112]\n",
      "   [ 2.0404644  -0.4475144 ]]\n",
      "\n",
      "  [[ 1.6290165  -0.37594432]\n",
      "   [ 1.6441128  -0.42868608]\n",
      "   [ 1.6464448  -0.41117585]\n",
      "   ...\n",
      "   [ 2.025701   -0.45040944]\n",
      "   [ 2.033764   -0.4248618 ]\n",
      "   [ 2.0290055  -0.46934122]]\n",
      "\n",
      "  [[ 1.6264659  -0.38728127]\n",
      "   [ 1.6241444  -0.42129362]\n",
      "   [ 1.6112304  -0.39137205]\n",
      "   ...\n",
      "   [ 2.1833837  -0.4864116 ]\n",
      "   [ 2.1817796  -0.45253775]\n",
      "   [ 2.1700711  -0.5089549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.2728446  -1.6631153 ]\n",
      "   [ 2.2095766  -1.6143771 ]\n",
      "   [ 2.153145   -1.5574763 ]\n",
      "   ...\n",
      "   [ 1.5279207  -1.4467655 ]\n",
      "   [ 1.5762146  -1.2747449 ]\n",
      "   [ 1.602256   -1.3632054 ]]\n",
      "\n",
      "  [[ 2.290453   -1.7515279 ]\n",
      "   [ 2.2313466  -1.6491107 ]\n",
      "   [ 2.150186   -1.5757182 ]\n",
      "   ...\n",
      "   [ 1.7302613  -1.3605126 ]\n",
      "   [ 1.6218493  -1.2979552 ]\n",
      "   [ 1.2292981  -1.1619412 ]]\n",
      "\n",
      "  [[ 2.2391417  -1.7802733 ]\n",
      "   [ 2.2028897  -1.6222174 ]\n",
      "   [ 2.14453    -1.6290861 ]\n",
      "   ...\n",
      "   [ 1.4867536  -1.2660291 ]\n",
      "   [ 1.3517698  -1.2017785 ]\n",
      "   [ 1.3355573  -1.1142331 ]]]\n",
      "\n",
      "\n",
      " [[[ 1.7986711  -0.21753211]\n",
      "   [ 1.8197306  -0.2048977 ]\n",
      "   [ 1.8394618  -0.2171273 ]\n",
      "   ...\n",
      "   [ 2.1543906  -0.15982457]\n",
      "   [ 2.1254356  -0.15427227]\n",
      "   [ 2.0764434  -0.2020454 ]]\n",
      "\n",
      "  [[ 1.8538082  -0.21220376]\n",
      "   [ 1.8692069  -0.22176267]\n",
      "   [ 1.8761955  -0.23005557]\n",
      "   ...\n",
      "   [ 2.0515077  -0.1835623 ]\n",
      "   [ 2.0401084  -0.1581975 ]\n",
      "   [ 2.0214405  -0.20050257]]\n",
      "\n",
      "  [[ 1.8349282  -0.17695807]\n",
      "   [ 1.8483248  -0.21658744]\n",
      "   [ 1.8582258  -0.18203525]\n",
      "   ...\n",
      "   [ 2.182015   -0.2004311 ]\n",
      "   [ 2.168592   -0.16631296]\n",
      "   [ 2.1464949  -0.19006133]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.2437453  -1.3584248 ]\n",
      "   [ 2.2221427  -1.2848526 ]\n",
      "   [ 2.2373765  -1.3077632 ]\n",
      "   ...\n",
      "   [ 1.6683111  -1.2175322 ]\n",
      "   [ 1.5492477  -0.9756265 ]\n",
      "   [ 1.5251932  -1.0340569 ]]\n",
      "\n",
      "  [[ 2.2634203  -1.3310966 ]\n",
      "   [ 2.2697992  -1.3173013 ]\n",
      "   [ 2.235593   -1.2630925 ]\n",
      "   ...\n",
      "   [ 1.6753708  -1.153531  ]\n",
      "   [ 1.671302   -0.9802382 ]\n",
      "   [ 1.56143    -1.1161023 ]]\n",
      "\n",
      "  [[ 2.2634075  -1.3004785 ]\n",
      "   [ 2.267444   -1.3305619 ]\n",
      "   [ 2.234444   -1.2581351 ]\n",
      "   ...\n",
      "   [ 1.799704   -1.0634311 ]\n",
      "   [ 1.7421201  -1.0393131 ]\n",
      "   [ 1.4986144  -1.0015949 ]]]\n",
      "\n",
      "\n",
      " [[[ 2.1255748   0.06167396]\n",
      "   [ 2.1668868   0.03769146]\n",
      "   [ 2.1904805   0.07510129]\n",
      "   ...\n",
      "   [ 2.2931776  -0.0085788 ]\n",
      "   [ 2.2752628  -0.02560635]\n",
      "   [ 2.2111566  -0.03404461]]\n",
      "\n",
      "  [[ 2.2112024   0.06113553]\n",
      "   [ 2.2461777   0.02573694]\n",
      "   [ 2.2696285   0.07030665]\n",
      "   ...\n",
      "   [ 2.2808497  -0.04453995]\n",
      "   [ 2.2300158  -0.04197278]\n",
      "   [ 2.1486797  -0.02897823]]\n",
      "\n",
      "  [[ 2.2320244   0.07707987]\n",
      "   [ 2.2636      0.04527485]\n",
      "   [ 2.2781453   0.10408486]\n",
      "   ...\n",
      "   [ 2.3485103  -0.06433522]\n",
      "   [ 2.2919996  -0.05407219]\n",
      "   [ 2.22698    -0.02604985]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 2.2680936  -1.2770014 ]\n",
      "   [ 2.163581   -1.0435963 ]\n",
      "   [ 2.1790216  -1.1241612 ]\n",
      "   ...\n",
      "   [ 1.8715143  -1.0225384 ]\n",
      "   [ 1.7993445  -1.0888295 ]\n",
      "   [ 1.6090195  -0.74032134]]\n",
      "\n",
      "  [[ 2.269666   -1.2380298 ]\n",
      "   [ 2.2480187  -1.1645919 ]\n",
      "   [ 2.2441769  -1.1489435 ]\n",
      "   ...\n",
      "   [ 1.7656429  -1.03299   ]\n",
      "   [ 1.64417    -0.913152  ]\n",
      "   [ 1.4465028  -0.74316657]]\n",
      "\n",
      "  [[ 2.2644122  -1.2195163 ]\n",
      "   [ 2.3542106  -1.2516861 ]\n",
      "   [ 2.387779   -1.236983  ]\n",
      "   ...\n",
      "   [ 1.8274528  -1.0090129 ]\n",
      "   [ 1.8086157  -0.7851691 ]\n",
      "   [ 1.7620988  -0.914797  ]]]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('processed_data/np_gan_standard.h5', 'r') as hf:\n",
    "    data_lr = hf['np_lr'][:]\n",
    "    data_lr_mean = hf['np_lr_mean'][:]\n",
    "    data_lr_stddev = hf['np_lr_stddev'][:]\n",
    "    data_hr = hf['np_hr'][:]\n",
    "    data_hr_mean = hf['np_hr_mean'][:]\n",
    "    data_hr_stddev = hf['np_hr_stddev'][:]\n",
    "    \n",
    "print(data_lr)\n",
    "print('\\n\\n')\n",
    "print(data_hr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3a714fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8520, 96, 96, 2)\n",
      "[ 0.7051484 -1.0147774]\n",
      "[3.1869051 2.8827915]\n",
      "\n",
      "\n",
      "\n",
      "(8520, 192, 192, 2)\n",
      "[ 0.701198  -1.0068085]\n",
      "[3.149407  2.8781955]\n"
     ]
    }
   ],
   "source": [
    "print(data_lr.shape)\n",
    "print(data_lr_mean)\n",
    "print(data_lr_stddev)\n",
    "print('\\n\\n')\n",
    "print(data_hr.shape)\n",
    "print(data_hr_mean)\n",
    "print(data_hr_stddev)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92084b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5112, 96, 96, 2)\n",
      "(1704, 96, 96, 2)\n",
      "(1704, 96, 96, 2)\n",
      "(5112, 192, 192, 2)\n",
      "(1704, 192, 192, 2)\n",
      "(1704, 192, 192, 2)\n",
      "6.890482 5.989441 5.7024393 -5.4777956 -5.0524907 -5.5057893\n",
      "7.035497 5.8600492 6.0442076 -5.315754 -5.2317853 -5.2996855\n"
     ]
    }
   ],
   "source": [
    "#First split data into train+validation and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_lr, data_hr, test_size=0.2, random_state=42)\n",
    "\n",
    "#Next split training again into train and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(np.max(x_train), np.max(x_val), np.max(x_test), np.min(x_train), np.min(x_val), np.min(x_test))\n",
    "print(np.max(y_train), np.max(y_val), np.max(y_test), np.min(y_train), np.min(y_val), np.min(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab5dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(input_shape = (96, 96, 2), nf = 64, r = 2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset, H*W*C\n",
    "    nf -- integer, the number of filters in all convT layer before super-resolution step\n",
    "    r -- integer, resolution ratio between output and input\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    C0 = input_shape[2]\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Define kernel size and stride used\n",
    "    k, stride = 3, 1\n",
    "    \n",
    "    # Shall we use a mirror padding and finally cutoff the edge, like the paper does? FIXME\n",
    "    X = Conv2DTranspose(filters=nf, kernel_size=(k, k), strides=(stride, stride), padding='same')(X_input)\n",
    "    # Shall we use relu, or leaky_relu? FIXME\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    skip_connection = X\n",
    "    \n",
    "    for i in range(16):\n",
    "        X_shortcut = X\n",
    "        \n",
    "        X = Conv2DTranspose(filters=nf, kernel_size=(k, k), strides=(stride, stride), padding='same')(X)\n",
    "        X = Activation('relu')(X)\n",
    "        X = Conv2DTranspose(filters=nf, kernel_size=(k, k), strides=(stride, stride), padding='same')(X)\n",
    "        X = Add()([X, X_shortcut])\n",
    "        # Are we missing a relu activation here, if we follow the resnet paper? FIXME\n",
    "    \n",
    "    X = Conv2DTranspose(filters=nf, kernel_size=(k, k), strides=(stride, stride), padding='same')(X)\n",
    "    X = Add()([X, skip_connection])\n",
    "    \n",
    "    # Start to perform sr\n",
    "    nf_sr = (r**2) * nf\n",
    "    X = Conv2DTranspose(filters=nf_sr, kernel_size=(k, k), strides=(stride, stride), padding='same')(X)\n",
    "    \n",
    "    sub_layer = Lambda(lambda x:tf.nn.depth_to_space(x,r))\n",
    "    X = sub_layer(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=C0, kernel_size=(k, k), strides=(stride, stride), padding='same')(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8620a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 02:22:46.895639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:46.897432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:46.898997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:46.900121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:46.901222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 02:22:46.902299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46627 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:4b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 96, 96, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 96, 96, 64)  1216        ['input_1[0][0]']                \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 96, 96, 64)   0           ['conv2d_transpose[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 96, 96, 64)  36928       ['activation[0][0]']             \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 96, 96, 64)   0           ['conv2d_transpose_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 96, 96, 64)  36928       ['activation_1[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 96, 96, 64)   0           ['conv2d_transpose_2[0][0]',     \n",
      "                                                                  'activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 96, 96, 64)  36928       ['add[0][0]']                    \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 96, 96, 64)   0           ['conv2d_transpose_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 96, 96, 64)  36928       ['activation_2[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 96, 96, 64)   0           ['conv2d_transpose_4[0][0]',     \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 96, 96, 64)  36928       ['add_1[0][0]']                  \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 96, 96, 64)   0           ['conv2d_transpose_5[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 96, 96, 64)  36928       ['activation_3[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 96, 96, 64)   0           ['conv2d_transpose_6[0][0]',     \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 96, 96, 64)  36928       ['add_2[0][0]']                  \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 96, 96, 64)   0           ['conv2d_transpose_7[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2DTran  (None, 96, 96, 64)  36928       ['activation_4[0][0]']           \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 96, 96, 64)   0           ['conv2d_transpose_8[0][0]',     \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_9 (Conv2DTran  (None, 96, 96, 64)  36928       ['add_3[0][0]']                  \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 96, 96, 64)   0           ['conv2d_transpose_9[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_transpose_10 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_5[0][0]']           \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 96, 96, 64)   0           ['conv2d_transpose_10[0][0]',    \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_11 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_4[0][0]']                  \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 96, 96, 64)   0           ['conv2d_transpose_11[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_12 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_6[0][0]']           \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 96, 96, 64)   0           ['conv2d_transpose_12[0][0]',    \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_13 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_5[0][0]']                  \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 96, 96, 64)   0           ['conv2d_transpose_13[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_14 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_7[0][0]']           \n",
      " nspose)                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 96, 96, 64)   0           ['conv2d_transpose_14[0][0]',    \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_15 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_6[0][0]']                  \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 96, 96, 64)   0           ['conv2d_transpose_15[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_16 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_8[0][0]']           \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 96, 96, 64)   0           ['conv2d_transpose_16[0][0]',    \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_17 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_7[0][0]']                  \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 96, 96, 64)   0           ['conv2d_transpose_17[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_18 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_9[0][0]']           \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 96, 96, 64)   0           ['conv2d_transpose_18[0][0]',    \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_19 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_8[0][0]']                  \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 96, 96, 64)   0           ['conv2d_transpose_19[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_20 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_10[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 96, 96, 64)   0           ['conv2d_transpose_20[0][0]',    \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_21 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_9[0][0]']                  \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 96, 96, 64)   0           ['conv2d_transpose_21[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_22 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_11[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 96, 96, 64)   0           ['conv2d_transpose_22[0][0]',    \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_transpose_23 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_10[0][0]']                 \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 96, 96, 64)   0           ['conv2d_transpose_23[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_24 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_12[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 96, 96, 64)   0           ['conv2d_transpose_24[0][0]',    \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_25 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_11[0][0]']                 \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 96, 96, 64)   0           ['conv2d_transpose_25[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_26 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_13[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 96, 96, 64)   0           ['conv2d_transpose_26[0][0]',    \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_27 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_12[0][0]']                 \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 96, 96, 64)   0           ['conv2d_transpose_27[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_28 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_14[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 96, 96, 64)   0           ['conv2d_transpose_28[0][0]',    \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_29 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_13[0][0]']                 \n",
      " nspose)                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 96, 96, 64)   0           ['conv2d_transpose_29[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_30 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_15[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 96, 96, 64)   0           ['conv2d_transpose_30[0][0]',    \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_31 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_14[0][0]']                 \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 96, 96, 64)   0           ['conv2d_transpose_31[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_32 (Conv2DTra  (None, 96, 96, 64)  36928       ['activation_16[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 96, 96, 64)   0           ['conv2d_transpose_32[0][0]',    \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_33 (Conv2DTra  (None, 96, 96, 64)  36928       ['add_15[0][0]']                 \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 96, 96, 64)   0           ['conv2d_transpose_33[0][0]',    \n",
      "                                                                  'activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_34 (Conv2DTra  (None, 96, 96, 256)  147712     ['add_16[0][0]']                 \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 192, 192, 64  0           ['conv2d_transpose_34[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 192, 192, 64  0           ['lambda[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_35 (Conv2DTra  (None, 192, 192, 2)  1154       ['activation_17[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,368,706\n",
      "Trainable params: 1,368,706\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "generator_model = generator(input_shape = (96, 96, 2))\n",
    "\n",
    "print(generator_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf537351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First do pretrain generator for better convergence\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "generator_model.compile(optimizer=adam, loss=losses.MeanSquaredError())\n",
    "\n",
    "logdir = \"gan_v1/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "checkpoint_filepath = 'gan_v1/ckp/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                               save_weights_only=True,\n",
    "                                                               save_freq=10*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b97f6415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 02:22:50.879818: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 35s 757ms/step - loss: 0.4259 - val_loss: 0.0936\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 28s 707ms/step - loss: 0.0737 - val_loss: 0.0598\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 29s 715ms/step - loss: 0.0526 - val_loss: 0.0470\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 29s 716ms/step - loss: 0.0432 - val_loss: 0.0401\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 29s 716ms/step - loss: 0.0376 - val_loss: 0.0358\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0339 - val_loss: 0.0326\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0314 - val_loss: 0.0307\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0295 - val_loss: 0.0290\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0281 - val_loss: 0.0281\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0271 - val_loss: 0.0269\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0263 - val_loss: 0.0259\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0253 - val_loss: 0.0255\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0248 - val_loss: 0.0248\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0243 - val_loss: 0.0240\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0237 - val_loss: 0.0242\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0233 - val_loss: 0.0236\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0230 - val_loss: 0.0229\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0224 - val_loss: 0.0230\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0222 - val_loss: 0.0222\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0217 - val_loss: 0.0219\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0214 - val_loss: 0.0217\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0211 - val_loss: 0.0213\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0210 - val_loss: 0.0215\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0207 - val_loss: 0.0211\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0208 - val_loss: 0.0207\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0203 - val_loss: 0.0204\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0202 - val_loss: 0.0203\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0201 - val_loss: 0.0202\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0203 - val_loss: 0.0209\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0199 - val_loss: 0.0199\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0195 - val_loss: 0.0197\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0194 - val_loss: 0.0197\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0194 - val_loss: 0.0197\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0194 - val_loss: 0.0195\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0191 - val_loss: 0.0193\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0191 - val_loss: 0.0193\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0190 - val_loss: 0.0194\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0197 - val_loss: 0.0194\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0192 - val_loss: 0.0193\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0189 - val_loss: 0.0191\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0187 - val_loss: 0.0190\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0185 - val_loss: 0.0187\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0186 - val_loss: 0.0187\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0186 - val_loss: 0.0189\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0185 - val_loss: 0.0187\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0187 - val_loss: 0.0192\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0185 - val_loss: 0.0184\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0181 - val_loss: 0.0185\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0183 - val_loss: 0.0184\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0184 - val_loss: 0.0183\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0181 - val_loss: 0.0184\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0180 - val_loss: 0.0184\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0182 - val_loss: 0.0181\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0180 - val_loss: 0.0182\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0180 - val_loss: 0.0182\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0180 - val_loss: 0.0181\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0177 - val_loss: 0.0181\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0178 - val_loss: 0.0181\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0178 - val_loss: 0.0180\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0178 - val_loss: 0.0180\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0178 - val_loss: 0.0180\n",
      "Epoch 64/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0176 - val_loss: 0.0179\n",
      "Epoch 65/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0176 - val_loss: 0.0178\n",
      "Epoch 66/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0178 - val_loss: 0.0178\n",
      "Epoch 67/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0173 - val_loss: 0.0176\n",
      "Epoch 68/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0175 - val_loss: 0.0177\n",
      "Epoch 69/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0176 - val_loss: 0.0181\n",
      "Epoch 70/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0175 - val_loss: 0.0175\n",
      "Epoch 71/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0174 - val_loss: 0.0185\n",
      "Epoch 72/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0174 - val_loss: 0.0178\n",
      "Epoch 73/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0172 - val_loss: 0.0174\n",
      "Epoch 74/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0174 - val_loss: 0.0177\n",
      "Epoch 75/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0172 - val_loss: 0.0173\n",
      "Epoch 76/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0172 - val_loss: 0.0180\n",
      "Epoch 77/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0175 - val_loss: 0.0175\n",
      "Epoch 78/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0170 - val_loss: 0.0171\n",
      "Epoch 79/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0170 - val_loss: 0.0175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0170 - val_loss: 0.0172\n",
      "Epoch 81/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0172 - val_loss: 0.0172\n",
      "Epoch 82/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0169 - val_loss: 0.0171\n",
      "Epoch 83/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0171 - val_loss: 0.0172\n",
      "Epoch 84/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 85/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0169 - val_loss: 0.0171\n",
      "Epoch 86/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0170 - val_loss: 0.0177\n",
      "Epoch 87/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0172 - val_loss: 0.0177\n",
      "Epoch 88/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 89/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0168 - val_loss: 0.0173\n",
      "Epoch 90/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 91/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 92/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 93/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 94/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0168 - val_loss: 0.0176\n",
      "Epoch 95/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 96/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 97/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 98/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 99/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 100/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0165 - val_loss: 0.0168\n",
      "Epoch 101/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 102/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0165 - val_loss: 0.0170\n",
      "Epoch 103/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 104/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 105/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0163 - val_loss: 0.0164\n",
      "Epoch 106/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0165 - val_loss: 0.0168\n",
      "Epoch 107/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0165 - val_loss: 0.0167\n",
      "Epoch 108/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0167 - val_loss: 0.0172\n",
      "Epoch 109/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0164 - val_loss: 0.0168\n",
      "Epoch 110/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0163 - val_loss: 0.0164\n",
      "Epoch 111/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0163 - val_loss: 0.0167\n",
      "Epoch 112/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0163 - val_loss: 0.0169\n",
      "Epoch 113/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0164 - val_loss: 0.0164\n",
      "Epoch 114/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0161 - val_loss: 0.0169\n",
      "Epoch 115/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0161 - val_loss: 0.0164\n",
      "Epoch 116/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0161 - val_loss: 0.0163\n",
      "Epoch 117/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0160 - val_loss: 0.0163\n",
      "Epoch 118/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0161 - val_loss: 0.0167\n",
      "Epoch 119/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0162 - val_loss: 0.0164\n",
      "Epoch 120/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0163 - val_loss: 0.0164\n",
      "Epoch 121/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0163 - val_loss: 0.0165\n",
      "Epoch 122/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0165 - val_loss: 0.0170\n",
      "Epoch 123/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0164 - val_loss: 0.0163\n",
      "Epoch 124/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0161 - val_loss: 0.0178\n",
      "Epoch 125/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0163 - val_loss: 0.0166\n",
      "Epoch 126/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0162 - val_loss: 0.0172\n",
      "Epoch 127/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0161 - val_loss: 0.0162\n",
      "Epoch 128/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0159 - val_loss: 0.0163\n",
      "Epoch 129/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0158 - val_loss: 0.0162\n",
      "Epoch 130/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0158 - val_loss: 0.0161\n",
      "Epoch 131/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0159 - val_loss: 0.0161\n",
      "Epoch 132/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0159 - val_loss: 0.0168\n",
      "Epoch 133/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0159 - val_loss: 0.0165\n",
      "Epoch 134/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0158 - val_loss: 0.0163\n",
      "Epoch 135/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0159 - val_loss: 0.0162\n",
      "Epoch 136/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0158 - val_loss: 0.0160\n",
      "Epoch 137/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0157 - val_loss: 0.0159\n",
      "Epoch 138/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0156 - val_loss: 0.0159\n",
      "Epoch 139/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0157 - val_loss: 0.0159\n",
      "Epoch 140/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0158 - val_loss: 0.0164\n",
      "Epoch 141/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 142/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0158 - val_loss: 0.0164\n",
      "Epoch 143/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0159 - val_loss: 0.0160\n",
      "Epoch 144/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0157 - val_loss: 0.0163\n",
      "Epoch 145/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0158 - val_loss: 0.0160\n",
      "Epoch 146/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0156 - val_loss: 0.0157\n",
      "Epoch 147/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0156 - val_loss: 0.0163\n",
      "Epoch 148/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0161 - val_loss: 0.0159\n",
      "Epoch 149/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0160 - val_loss: 0.0163\n",
      "Epoch 150/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 151/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0154 - val_loss: 0.0157\n",
      "Epoch 152/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0154 - val_loss: 0.0157\n",
      "Epoch 153/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0154 - val_loss: 0.0157\n",
      "Epoch 154/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0156 - val_loss: 0.0160\n",
      "Epoch 155/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0156 - val_loss: 0.0159\n",
      "Epoch 156/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0153 - val_loss: 0.0157\n",
      "Epoch 157/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 158/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0154 - val_loss: 0.0155\n",
      "Epoch 159/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0153 - val_loss: 0.0156\n",
      "Epoch 160/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0154 - val_loss: 0.0162\n",
      "Epoch 161/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0157 - val_loss: 0.0160\n",
      "Epoch 162/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 163/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0153 - val_loss: 0.0158\n",
      "Epoch 164/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0159 - val_loss: 0.0158\n",
      "Epoch 165/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0152 - val_loss: 0.0154\n",
      "Epoch 166/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0154 - val_loss: 0.0158\n",
      "Epoch 167/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0157 - val_loss: 0.0163\n",
      "Epoch 168/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0155 - val_loss: 0.0156\n",
      "Epoch 169/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0153 - val_loss: 0.0158\n",
      "Epoch 170/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0152 - val_loss: 0.0154\n",
      "Epoch 171/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0152 - val_loss: 0.0154\n",
      "Epoch 172/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0150 - val_loss: 0.0154\n",
      "Epoch 173/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0152 - val_loss: 0.0157\n",
      "Epoch 174/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0152 - val_loss: 0.0154\n",
      "Epoch 175/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0151 - val_loss: 0.0156\n",
      "Epoch 176/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0152 - val_loss: 0.0154\n",
      "Epoch 177/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0151 - val_loss: 0.0155\n",
      "Epoch 178/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0155 - val_loss: 0.0158\n",
      "Epoch 179/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 180/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0149 - val_loss: 0.0153\n",
      "Epoch 181/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0151 - val_loss: 0.0153\n",
      "Epoch 182/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0152 - val_loss: 0.0155\n",
      "Epoch 183/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0152 - val_loss: 0.0160\n",
      "Epoch 184/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0154 - val_loss: 0.0154\n",
      "Epoch 185/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0150 - val_loss: 0.0152\n",
      "Epoch 186/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0150 - val_loss: 0.0153\n",
      "Epoch 187/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0150 - val_loss: 0.0151\n",
      "Epoch 188/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0151 - val_loss: 0.0154\n",
      "Epoch 189/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0150 - val_loss: 0.0154\n",
      "Epoch 190/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0152 - val_loss: 0.0158\n",
      "Epoch 191/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0151 - val_loss: 0.0155\n",
      "Epoch 192/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0153 - val_loss: 0.0159\n",
      "Epoch 193/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0150 - val_loss: 0.0154\n",
      "Epoch 194/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0152 - val_loss: 0.0156\n",
      "Epoch 195/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0149 - val_loss: 0.0153\n",
      "Epoch 196/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0150 - val_loss: 0.0152\n",
      "Epoch 197/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0149 - val_loss: 0.0156\n",
      "Epoch 198/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0150 - val_loss: 0.0152\n",
      "Epoch 199/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0147 - val_loss: 0.0152\n",
      "Epoch 200/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0148 - val_loss: 0.0152\n",
      "Epoch 201/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0148 - val_loss: 0.0150\n",
      "Epoch 202/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0147 - val_loss: 0.0150\n",
      "Epoch 203/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0147 - val_loss: 0.0149\n",
      "Epoch 204/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0147 - val_loss: 0.0151\n",
      "Epoch 205/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0148 - val_loss: 0.0155\n",
      "Epoch 206/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0149 - val_loss: 0.0155\n",
      "Epoch 207/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0148 - val_loss: 0.0153\n",
      "Epoch 208/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0148 - val_loss: 0.0157\n",
      "Epoch 209/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0148 - val_loss: 0.0152\n",
      "Epoch 210/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0148 - val_loss: 0.0154\n",
      "Epoch 211/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0147 - val_loss: 0.0151\n",
      "Epoch 212/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0147 - val_loss: 0.0153\n",
      "Epoch 213/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0146 - val_loss: 0.0150\n",
      "Epoch 214/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0146 - val_loss: 0.0149\n",
      "Epoch 215/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0145 - val_loss: 0.0152\n",
      "Epoch 216/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0146 - val_loss: 0.0149\n",
      "Epoch 217/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0145 - val_loss: 0.0150\n",
      "Epoch 218/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0146 - val_loss: 0.0149\n",
      "Epoch 219/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0145 - val_loss: 0.0151\n",
      "Epoch 220/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0151 - val_loss: 0.0149\n",
      "Epoch 221/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0144 - val_loss: 0.0148\n",
      "Epoch 222/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0144 - val_loss: 0.0149\n",
      "Epoch 223/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0145 - val_loss: 0.0146\n",
      "Epoch 224/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0145 - val_loss: 0.0152\n",
      "Epoch 225/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0145 - val_loss: 0.0147\n",
      "Epoch 226/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0144 - val_loss: 0.0154\n",
      "Epoch 227/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 228/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0144 - val_loss: 0.0150\n",
      "Epoch 229/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0143 - val_loss: 0.0146\n",
      "Epoch 230/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0143 - val_loss: 0.0147\n",
      "Epoch 231/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0144 - val_loss: 0.0146\n",
      "Epoch 232/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0142 - val_loss: 0.0146\n",
      "Epoch 233/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0143 - val_loss: 0.0149\n",
      "Epoch 234/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0142 - val_loss: 0.0146\n",
      "Epoch 235/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0143 - val_loss: 0.0149\n",
      "Epoch 236/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0143 - val_loss: 0.0145\n",
      "Epoch 237/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0144 - val_loss: 0.0146\n",
      "Epoch 238/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0144 - val_loss: 0.0148\n",
      "Epoch 239/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0149 - val_loss: 0.0152\n",
      "Epoch 240/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0143 - val_loss: 0.0147\n",
      "Epoch 241/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0142 - val_loss: 0.0145\n",
      "Epoch 242/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0141 - val_loss: 0.0146\n",
      "Epoch 243/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0141 - val_loss: 0.0147\n",
      "Epoch 244/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0142 - val_loss: 0.0146\n",
      "Epoch 245/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 246/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0143 - val_loss: 0.0152\n",
      "Epoch 247/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0142 - val_loss: 0.0146\n",
      "Epoch 248/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0141 - val_loss: 0.0145\n",
      "Epoch 249/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0140 - val_loss: 0.0148\n",
      "Epoch 250/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0142 - val_loss: 0.0146\n",
      "Epoch 251/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0140 - val_loss: 0.0144\n",
      "Epoch 252/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0140 - val_loss: 0.0145\n",
      "Epoch 253/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 254/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0141 - val_loss: 0.0145\n",
      "Epoch 255/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 256/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0142 - val_loss: 0.0149\n",
      "Epoch 257/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0144 - val_loss: 0.0153\n",
      "Epoch 258/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0145 - val_loss: 0.0149\n",
      "Epoch 259/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0143 - val_loss: 0.0146\n",
      "Epoch 260/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0141 - val_loss: 0.0145\n",
      "Epoch 261/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0140 - val_loss: 0.0145\n",
      "Epoch 262/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0140 - val_loss: 0.0144\n",
      "Epoch 263/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0140 - val_loss: 0.0147\n",
      "Epoch 264/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0142 - val_loss: 0.0146\n",
      "Epoch 265/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0140 - val_loss: 0.0148\n",
      "Epoch 266/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 267/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0139 - val_loss: 0.0142\n",
      "Epoch 268/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0138 - val_loss: 0.0145\n",
      "Epoch 269/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0140 - val_loss: 0.0142\n",
      "Epoch 270/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0138 - val_loss: 0.0144\n",
      "Epoch 271/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0140 - val_loss: 0.0143\n",
      "Epoch 272/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 273/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0138 - val_loss: 0.0143\n",
      "Epoch 274/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 275/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0138 - val_loss: 0.0143\n",
      "Epoch 276/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0138 - val_loss: 0.0142\n",
      "Epoch 277/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 278/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0139 - val_loss: 0.0142\n",
      "Epoch 279/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0138 - val_loss: 0.0143\n",
      "Epoch 280/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0137 - val_loss: 0.0142\n",
      "Epoch 281/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0137 - val_loss: 0.0143\n",
      "Epoch 282/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0136 - val_loss: 0.0142\n",
      "Epoch 283/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0137 - val_loss: 0.0142\n",
      "Epoch 284/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0137 - val_loss: 0.0141\n",
      "Epoch 285/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0138 - val_loss: 0.0146\n",
      "Epoch 286/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0138 - val_loss: 0.0143\n",
      "Epoch 287/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0139 - val_loss: 0.0145\n",
      "Epoch 288/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0136 - val_loss: 0.0140\n",
      "Epoch 289/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0137 - val_loss: 0.0141\n",
      "Epoch 290/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0136 - val_loss: 0.0142\n",
      "Epoch 291/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0137 - val_loss: 0.0143\n",
      "Epoch 292/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 293/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0137 - val_loss: 0.0140\n",
      "Epoch 294/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 295/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0136 - val_loss: 0.0140\n",
      "Epoch 296/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0135 - val_loss: 0.0139\n",
      "Epoch 297/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0135 - val_loss: 0.0140\n",
      "Epoch 298/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0137 - val_loss: 0.0142\n",
      "Epoch 299/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 300/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0136 - val_loss: 0.0141\n",
      "Epoch 301/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 302/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0135 - val_loss: 0.0139\n",
      "Epoch 303/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0135 - val_loss: 0.0139\n",
      "Epoch 304/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0134 - val_loss: 0.0139\n",
      "Epoch 305/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0134 - val_loss: 0.0138\n",
      "Epoch 306/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0137 - val_loss: 0.0143\n",
      "Epoch 307/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 308/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 309/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0135 - val_loss: 0.0139\n",
      "Epoch 310/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0135 - val_loss: 0.0140\n",
      "Epoch 311/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0135 - val_loss: 0.0139\n",
      "Epoch 312/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0134 - val_loss: 0.0140\n",
      "Epoch 313/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0134 - val_loss: 0.0138\n",
      "Epoch 314/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0134 - val_loss: 0.0140\n",
      "Epoch 315/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 316/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0134 - val_loss: 0.0138\n",
      "Epoch 317/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0134 - val_loss: 0.0138\n",
      "Epoch 318/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0134 - val_loss: 0.0136\n",
      "Epoch 319/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0134 - val_loss: 0.0138\n",
      "Epoch 320/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 321/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0133 - val_loss: 0.0138\n",
      "Epoch 322/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0132 - val_loss: 0.0138\n",
      "Epoch 323/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0133 - val_loss: 0.0140\n",
      "Epoch 324/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0137 - val_loss: 0.0140\n",
      "Epoch 325/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0132 - val_loss: 0.0136\n",
      "Epoch 326/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0132 - val_loss: 0.0138\n",
      "Epoch 327/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0134 - val_loss: 0.0140\n",
      "Epoch 328/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0134 - val_loss: 0.0141\n",
      "Epoch 329/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0132 - val_loss: 0.0137\n",
      "Epoch 330/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0132 - val_loss: 0.0137\n",
      "Epoch 331/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0132 - val_loss: 0.0136\n",
      "Epoch 332/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 333/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0132 - val_loss: 0.0136\n",
      "Epoch 334/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0133 - val_loss: 0.0139\n",
      "Epoch 335/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0132 - val_loss: 0.0136\n",
      "Epoch 336/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0133 - val_loss: 0.0136\n",
      "Epoch 337/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0131 - val_loss: 0.0136\n",
      "Epoch 338/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0131 - val_loss: 0.0137\n",
      "Epoch 339/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0132 - val_loss: 0.0137\n",
      "Epoch 340/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0132 - val_loss: 0.0136\n",
      "Epoch 341/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0130 - val_loss: 0.0136\n",
      "Epoch 342/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0131 - val_loss: 0.0136\n",
      "Epoch 343/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0132 - val_loss: 0.0139\n",
      "Epoch 344/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0132 - val_loss: 0.0138\n",
      "Epoch 345/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 346/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0130 - val_loss: 0.0135\n",
      "Epoch 347/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0130 - val_loss: 0.0136\n",
      "Epoch 348/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0131 - val_loss: 0.0141\n",
      "Epoch 349/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0136 - val_loss: 0.0137\n",
      "Epoch 350/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 351/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0133 - val_loss: 0.0136\n",
      "Epoch 352/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0130 - val_loss: 0.0135\n",
      "Epoch 353/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 354/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0130 - val_loss: 0.0135\n",
      "Epoch 355/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0129 - val_loss: 0.0135\n",
      "Epoch 356/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 357/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0129 - val_loss: 0.0136\n",
      "Epoch 358/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0130 - val_loss: 0.0134\n",
      "Epoch 359/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 360/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 361/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0128 - val_loss: 0.0135\n",
      "Epoch 362/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0130 - val_loss: 0.0137\n",
      "Epoch 363/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 364/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 365/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0130 - val_loss: 0.0138\n",
      "Epoch 366/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0129 - val_loss: 0.0135\n",
      "Epoch 367/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0129 - val_loss: 0.0136\n",
      "Epoch 368/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0128 - val_loss: 0.0134\n",
      "Epoch 369/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0130 - val_loss: 0.0135\n",
      "Epoch 370/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 371/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0129 - val_loss: 0.0135\n",
      "Epoch 372/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 373/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 374/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 375/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0128 - val_loss: 0.0134\n",
      "Epoch 376/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 377/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 378/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 379/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0128 - val_loss: 0.0135\n",
      "Epoch 380/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 381/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 382/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0127 - val_loss: 0.0133\n",
      "Epoch 383/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 384/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 385/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0129 - val_loss: 0.0133\n",
      "Epoch 386/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 387/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 388/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 389/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0129 - val_loss: 0.0136\n",
      "Epoch 390/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0128 - val_loss: 0.0134\n",
      "Epoch 391/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0127 - val_loss: 0.0134\n",
      "Epoch 392/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 393/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0127 - val_loss: 0.0133\n",
      "Epoch 394/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 395/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0127 - val_loss: 0.0133\n",
      "Epoch 396/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 397/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 398/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 399/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0126 - val_loss: 0.0133\n",
      "Epoch 400/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 401/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 402/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0126 - val_loss: 0.0133\n",
      "Epoch 403/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 404/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 405/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0124 - val_loss: 0.0131\n",
      "Epoch 406/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 407/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 408/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0126 - val_loss: 0.0133\n",
      "Epoch 409/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0126 - val_loss: 0.0131\n",
      "Epoch 410/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0128 - val_loss: 0.0134\n",
      "Epoch 411/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0130\n",
      "Epoch 412/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 413/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0132\n",
      "Epoch 414/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 415/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0132\n",
      "Epoch 416/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 417/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 418/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 419/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 420/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0123 - val_loss: 0.0130\n",
      "Epoch 421/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0124 - val_loss: 0.0131\n",
      "Epoch 422/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 423/1000\n",
      "40/40 [==============================] - 28s 708ms/step - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 424/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 425/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 426/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 427/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0130\n",
      "Epoch 428/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0124 - val_loss: 0.0136\n",
      "Epoch 429/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 430/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 431/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 432/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 433/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 434/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 435/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0123 - val_loss: 0.0130\n",
      "Epoch 436/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0128\n",
      "Epoch 437/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0122 - val_loss: 0.0130\n",
      "Epoch 438/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 439/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0122 - val_loss: 0.0131\n",
      "Epoch 440/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 441/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 442/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 443/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0122 - val_loss: 0.0128\n",
      "Epoch 444/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 445/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0122 - val_loss: 0.0131\n",
      "Epoch 446/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 447/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0122 - val_loss: 0.0128\n",
      "Epoch 448/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0122 - val_loss: 0.0129\n",
      "Epoch 449/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 450/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 451/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0129\n",
      "Epoch 452/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0122 - val_loss: 0.0130\n",
      "Epoch 453/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0122 - val_loss: 0.0129\n",
      "Epoch 454/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0122 - val_loss: 0.0127\n",
      "Epoch 455/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0129\n",
      "Epoch 456/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 457/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0120 - val_loss: 0.0128\n",
      "Epoch 458/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0129\n",
      "Epoch 459/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0120 - val_loss: 0.0127\n",
      "Epoch 460/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0120 - val_loss: 0.0128\n",
      "Epoch 461/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0130\n",
      "Epoch 462/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 463/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0129\n",
      "Epoch 464/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 465/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 466/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 467/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0120 - val_loss: 0.0127\n",
      "Epoch 468/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 469/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 470/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 471/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0129\n",
      "Epoch 472/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0120 - val_loss: 0.0126\n",
      "Epoch 473/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0120 - val_loss: 0.0126\n",
      "Epoch 474/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 475/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 476/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0120 - val_loss: 0.0132\n",
      "Epoch 477/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0131\n",
      "Epoch 478/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 479/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 480/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0120 - val_loss: 0.0128\n",
      "Epoch 481/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 482/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 483/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 484/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 485/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0119 - val_loss: 0.0125\n",
      "Epoch 486/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 487/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0120 - val_loss: 0.0132\n",
      "Epoch 488/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0121 - val_loss: 0.0126\n",
      "Epoch 489/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0119 - val_loss: 0.0127\n",
      "Epoch 490/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 491/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 492/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0125\n",
      "Epoch 493/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 494/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0120 - val_loss: 0.0127\n",
      "Epoch 495/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0119 - val_loss: 0.0127\n",
      "Epoch 496/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0125\n",
      "Epoch 497/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 498/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 499/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0125\n",
      "Epoch 500/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 501/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 502/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 503/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0119 - val_loss: 0.0127\n",
      "Epoch 504/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0121 - val_loss: 0.0126\n",
      "Epoch 505/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 506/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0119 - val_loss: 0.0127\n",
      "Epoch 507/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 508/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 509/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 510/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 511/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 512/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 513/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 514/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0117 - val_loss: 0.0124\n",
      "Epoch 515/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0126\n",
      "Epoch 516/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0127\n",
      "Epoch 517/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0117 - val_loss: 0.0128\n",
      "Epoch 518/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 519/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0117 - val_loss: 0.0127\n",
      "Epoch 520/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 521/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 522/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 523/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 524/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0117 - val_loss: 0.0126\n",
      "Epoch 525/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 526/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0117 - val_loss: 0.0124\n",
      "Epoch 527/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 528/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 529/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 530/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0116 - val_loss: 0.0127\n",
      "Epoch 531/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 532/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 533/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0126\n",
      "Epoch 534/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 535/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 536/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 537/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0117 - val_loss: 0.0124\n",
      "Epoch 538/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0115 - val_loss: 0.0125\n",
      "Epoch 539/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 540/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 541/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 542/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 543/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 544/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0115 - val_loss: 0.0125\n",
      "Epoch 545/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 546/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0115 - val_loss: 0.0125\n",
      "Epoch 547/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0115 - val_loss: 0.0127\n",
      "Epoch 548/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0118 - val_loss: 0.0129\n",
      "Epoch 549/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0117 - val_loss: 0.0124\n",
      "Epoch 550/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0115 - val_loss: 0.0126\n",
      "Epoch 551/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 552/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 553/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0124\n",
      "Epoch 554/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 555/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 556/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0115 - val_loss: 0.0126\n",
      "Epoch 557/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0117 - val_loss: 0.0123\n",
      "Epoch 558/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0125\n",
      "Epoch 559/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0114 - val_loss: 0.0124\n",
      "Epoch 560/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0114 - val_loss: 0.0125\n",
      "Epoch 561/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 562/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 563/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 564/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 565/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0124\n",
      "Epoch 566/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 567/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 568/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 569/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 570/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 571/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 572/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 573/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0114 - val_loss: 0.0122\n",
      "Epoch 574/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 575/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 576/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 577/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 578/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 579/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0114 - val_loss: 0.0126\n",
      "Epoch 580/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 581/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 582/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 583/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 584/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 585/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 586/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 587/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0124\n",
      "Epoch 588/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 589/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 590/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 591/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 592/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 593/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 594/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 595/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0124\n",
      "Epoch 596/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 597/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 598/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 599/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 600/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 601/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 602/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 603/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 604/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 605/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0124\n",
      "Epoch 606/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 607/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 608/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0124\n",
      "Epoch 609/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 610/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 611/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 612/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 613/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 614/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 615/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 616/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 617/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 618/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 619/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 620/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 621/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 622/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0124\n",
      "Epoch 623/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 624/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 625/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 626/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 627/1000\n",
      "40/40 [==============================] - 28s 709ms/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 628/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 629/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 630/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 631/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 632/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 633/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 634/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 635/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 636/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 637/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 638/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 639/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 640/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 641/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 642/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 643/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0109 - val_loss: 0.0123\n",
      "Epoch 644/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 645/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 646/1000\n",
      "40/40 [==============================] - 28s 710ms/step - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 647/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 648/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 649/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 650/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 651/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 652/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 653/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 654/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 655/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 656/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 657/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 658/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0110 - val_loss: 0.0123\n",
      "Epoch 659/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 660/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 661/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 662/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 663/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 664/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 665/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 666/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0123\n",
      "Epoch 667/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 668/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 669/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 670/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 671/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0123\n",
      "Epoch 672/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 673/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 674/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 675/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 676/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 677/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 678/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 679/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 680/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 681/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 682/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 683/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 684/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 685/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 686/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0108 - val_loss: 0.0121\n",
      "Epoch 687/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 688/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 689/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 690/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 691/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 692/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0121\n",
      "Epoch 693/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 694/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0125\n",
      "Epoch 695/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 696/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 697/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 698/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 699/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 700/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 701/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 702/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 703/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 704/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 705/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 706/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 707/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 708/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 709/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 710/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0106 - val_loss: 0.0120\n",
      "Epoch 711/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 712/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 713/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 714/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 715/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 716/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 717/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 718/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 719/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 720/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 721/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 722/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 723/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 724/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 725/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 726/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 727/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0105 - val_loss: 0.0122\n",
      "Epoch 728/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 729/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 730/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 731/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 732/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 733/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 734/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 735/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 736/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 737/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 738/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 739/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0105 - val_loss: 0.0121\n",
      "Epoch 740/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 741/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0123\n",
      "Epoch 742/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0105 - val_loss: 0.0120\n",
      "Epoch 743/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 744/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 745/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0122\n",
      "Epoch 746/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0122\n",
      "Epoch 747/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 748/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 749/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 750/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 751/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 752/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 753/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 754/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0104 - val_loss: 0.0120\n",
      "Epoch 755/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 756/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0123\n",
      "Epoch 757/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 758/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 759/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 760/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 761/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 762/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 763/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 764/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 765/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 766/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 767/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 768/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 769/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 770/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 771/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0104 - val_loss: 0.0121\n",
      "Epoch 772/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0122\n",
      "Epoch 773/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 774/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 775/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0122\n",
      "Epoch 776/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 777/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 778/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0121\n",
      "Epoch 779/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 780/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0103 - val_loss: 0.0122\n",
      "Epoch 781/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 782/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0121\n",
      "Epoch 783/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 784/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0121\n",
      "Epoch 785/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 786/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 787/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 788/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0103 - val_loss: 0.0120\n",
      "Epoch 789/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 790/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 791/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 792/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 793/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 794/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 795/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 796/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 797/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 798/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0101 - val_loss: 0.0121\n",
      "Epoch 799/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 800/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0101 - val_loss: 0.0121\n",
      "Epoch 801/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0122\n",
      "Epoch 802/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 803/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 804/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 805/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 806/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0100 - val_loss: 0.0121\n",
      "Epoch 807/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 808/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 809/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 810/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 811/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 812/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0123\n",
      "Epoch 813/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 814/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0101 - val_loss: 0.0121\n",
      "Epoch 815/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 816/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 817/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 818/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 819/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 820/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 821/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 822/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 823/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 824/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 825/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 826/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0102 - val_loss: 0.0120\n",
      "Epoch 827/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0102 - val_loss: 0.0121\n",
      "Epoch 828/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 829/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 830/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 831/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 832/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 833/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 834/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0121\n",
      "Epoch 835/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 836/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 837/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 838/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 839/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 840/1000\n",
      "40/40 [==============================] - 29s 715ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 841/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 842/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 843/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 844/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0123\n",
      "Epoch 845/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0120\n",
      "Epoch 846/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0099 - val_loss: 0.0122\n",
      "Epoch 847/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 848/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0122\n",
      "Epoch 849/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 850/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 851/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 852/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 853/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0121\n",
      "Epoch 854/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0101 - val_loss: 0.0121\n",
      "Epoch 855/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 856/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 857/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 858/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0123\n",
      "Epoch 859/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0119\n",
      "Epoch 860/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 861/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0119\n",
      "Epoch 862/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 863/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 864/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 865/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0119\n",
      "Epoch 866/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0119\n",
      "Epoch 867/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 868/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0121\n",
      "Epoch 869/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 870/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 871/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 872/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 873/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 874/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0097 - val_loss: 0.0119\n",
      "Epoch 875/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0119\n",
      "Epoch 876/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0121\n",
      "Epoch 877/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0121\n",
      "Epoch 878/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0119\n",
      "Epoch 879/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 880/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 881/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 882/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0121\n",
      "Epoch 883/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0126\n",
      "Epoch 884/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0100 - val_loss: 0.0122\n",
      "Epoch 885/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0120\n",
      "Epoch 886/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 887/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 888/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 889/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 890/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 891/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 892/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0121\n",
      "Epoch 893/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 894/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 895/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0119\n",
      "Epoch 896/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0119\n",
      "Epoch 897/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0097 - val_loss: 0.0122\n",
      "Epoch 898/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 899/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 900/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 901/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 902/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0098 - val_loss: 0.0121\n",
      "Epoch 903/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0121\n",
      "Epoch 904/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0119\n",
      "Epoch 905/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 906/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 907/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0098 - val_loss: 0.0120\n",
      "Epoch 908/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0121\n",
      "Epoch 909/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0097 - val_loss: 0.0121\n",
      "Epoch 910/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0096 - val_loss: 0.0119\n",
      "Epoch 911/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0119\n",
      "Epoch 912/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 913/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 914/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0119\n",
      "Epoch 915/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0121\n",
      "Epoch 916/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 917/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0119\n",
      "Epoch 918/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 919/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 920/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0096 - val_loss: 0.0121\n",
      "Epoch 921/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 922/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 923/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 924/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 925/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 926/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 927/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 928/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 929/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 930/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 931/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 932/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 933/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 934/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0121\n",
      "Epoch 935/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 936/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 937/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0119\n",
      "Epoch 938/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 939/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0122\n",
      "Epoch 940/1000\n",
      "40/40 [==============================] - 29s 716ms/step - loss: 0.0096 - val_loss: 0.0121\n",
      "Epoch 941/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 942/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 943/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 944/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 945/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0096 - val_loss: 0.0122\n",
      "Epoch 946/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 947/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 948/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 949/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 950/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0094 - val_loss: 0.0120\n",
      "Epoch 951/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0122\n",
      "Epoch 952/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0120\n",
      "Epoch 953/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0120\n",
      "Epoch 954/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 955/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0120\n",
      "Epoch 956/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0094 - val_loss: 0.0120\n",
      "Epoch 957/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0128\n",
      "Epoch 958/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0097 - val_loss: 0.0121\n",
      "Epoch 959/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0123\n",
      "Epoch 960/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 961/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 962/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 963/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0122\n",
      "Epoch 964/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0122\n",
      "Epoch 965/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 966/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0094 - val_loss: 0.0120\n",
      "Epoch 967/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 968/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0094 - val_loss: 0.0120\n",
      "Epoch 969/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0121\n",
      "Epoch 970/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 971/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 972/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0096 - val_loss: 0.0120\n",
      "Epoch 973/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 974/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0120\n",
      "Epoch 975/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0120\n",
      "Epoch 976/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 977/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0093 - val_loss: 0.0120\n",
      "Epoch 978/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 979/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 980/1000\n",
      "40/40 [==============================] - 28s 713ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 981/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0093 - val_loss: 0.0121\n",
      "Epoch 982/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0120\n",
      "Epoch 983/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0122\n",
      "Epoch 984/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 985/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 986/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0093 - val_loss: 0.0121\n",
      "Epoch 987/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0121\n",
      "Epoch 988/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0122\n",
      "Epoch 989/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 990/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 991/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0093 - val_loss: 0.0121\n",
      "Epoch 992/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0120\n",
      "Epoch 993/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0123\n",
      "Epoch 994/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0122\n",
      "Epoch 995/1000\n",
      "40/40 [==============================] - 28s 711ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 996/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0122\n",
      "Epoch 997/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 998/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0093 - val_loss: 0.0121\n",
      "Epoch 999/1000\n",
      "40/40 [==============================] - 28s 712ms/step - loss: 0.0094 - val_loss: 0.0121\n",
      "Epoch 1000/1000\n",
      "40/40 [==============================] - 28s 714ms/step - loss: 0.0093 - val_loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "training_history = generator_model.fit(x_train, y_train,\n",
    "                                       batch_size=128,\n",
    "                                       epochs=1000,\n",
    "                                       shuffle=True,\n",
    "                                       validation_data=(x_val, y_val),\n",
    "                                       callbacks=[tensorboard_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce6302a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 3s 52ms/step - loss: 0.0120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.012037176638841629"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6faa9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89a340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89f8145e",
   "metadata": {},
   "source": [
    "# Operations after that should no longer be adopted!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f37b7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next do pretrain discriminator for better convergence\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "disc_model.compile(optimizer=adam, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "\n",
    "logdir = \"gan_v2/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "checkpoint_filepath = 'gan_v2/ckp/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                               save_weights_only=True,\n",
    "                                                               save_freq=10*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e73ff42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 8s 51ms/step\n",
      "(5112, 192, 192, 2)\n",
      "54/54 [==============================] - 3s 51ms/step\n",
      "(1704, 192, 192, 2)\n"
     ]
    }
   ],
   "source": [
    "sr_train_temp = generator_model.predict(x_train)\n",
    "print(sr_train_temp.shape)\n",
    "\n",
    "sr_val_temp = generator_model.predict(x_val)\n",
    "print(sr_val_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a1a4041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5112,) [1. 1. 1. ... 1. 1. 1.]\n",
      "(5112,) [0. 0. 0. ... 0. 0. 0.]\n",
      "(1704,) [1. 1. 1. ... 1. 1. 1.]\n",
      "(1704,) [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_true_label_train = np.ones(y_train.shape[0])\n",
    "print(y_true_label_train.shape, y_true_label_train)\n",
    "\n",
    "y_false_label_train = np.zeros(sr_train_temp.shape[0])\n",
    "print(y_false_label_train.shape, y_false_label_train)\n",
    "\n",
    "y_true_label_val = np.ones(y_val.shape[0])\n",
    "print(y_true_label_val.shape, y_true_label_val)\n",
    "\n",
    "y_false_label_val = np.zeros(sr_val_temp.shape[0])\n",
    "print(y_false_label_val.shape, y_false_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d91362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10224, 192, 192, 2)\n",
      "(10224,)\n",
      "(3408, 192, 192, 2)\n",
      "(3408,)\n"
     ]
    }
   ],
   "source": [
    "disc_y_train = np.concatenate((y_train, sr_train_temp), axis=0)\n",
    "print(disc_y_train.shape)\n",
    "\n",
    "disc_y_label_train = np.concatenate((y_true_label_train, y_false_label_train), axis=0)\n",
    "print(disc_y_label_train.shape)\n",
    "\n",
    "disc_y_val = np.concatenate((y_val, sr_val_temp), axis=0)\n",
    "print(disc_y_val.shape)\n",
    "\n",
    "disc_y_label_val = np.concatenate((y_true_label_val, y_false_label_val), axis=0)\n",
    "print(disc_y_label_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "048e5b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "80/80 [==============================] - 10s 114ms/step - loss: 0.6903 - val_loss: 0.6861\n",
      "Epoch 2/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 0.5332 - val_loss: 0.2979\n",
      "Epoch 3/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.1340 - val_loss: 0.0639\n",
      "Epoch 4/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0300 - val_loss: 0.0172\n",
      "Epoch 5/1000\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 0.0161 - val_loss: 0.0115\n",
      "Epoch 6/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0063 - val_loss: 0.0061\n",
      "Epoch 7/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0049 - val_loss: 0.0045\n",
      "Epoch 8/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0022 - val_loss: 0.0061\n",
      "Epoch 9/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0996 - val_loss: 0.0115\n",
      "Epoch 10/1000\n",
      "80/80 [==============================] - 8s 103ms/step - loss: 0.0313 - val_loss: 0.2413\n",
      "Epoch 11/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0253 - val_loss: 0.0059\n",
      "Epoch 12/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0037 - val_loss: 0.0067\n",
      "Epoch 13/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0026 - val_loss: 0.0031\n",
      "Epoch 14/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 15/1000\n",
      "80/80 [==============================] - 8s 103ms/step - loss: 0.0016 - val_loss: 0.0046\n",
      "Epoch 16/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 17/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 18/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.6885e-04 - val_loss: 0.0019\n",
      "Epoch 19/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.1575e-04 - val_loss: 0.0017\n",
      "Epoch 20/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 5.7590e-04 - val_loss: 0.0014\n",
      "Epoch 21/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.2852e-04 - val_loss: 0.0015\n",
      "Epoch 22/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.9346e-04 - val_loss: 0.0016\n",
      "Epoch 23/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.2808e-04 - val_loss: 0.0027\n",
      "Epoch 24/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.1552e-04 - val_loss: 0.0013\n",
      "Epoch 25/1000\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 3.0206e-04 - val_loss: 0.0012\n",
      "Epoch 26/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.7806e-04 - val_loss: 0.0012\n",
      "Epoch 27/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.6729e-04 - val_loss: 0.0011\n",
      "Epoch 28/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.6059e-04 - val_loss: 0.0010\n",
      "Epoch 29/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.1903e-04 - val_loss: 0.0010\n",
      "Epoch 30/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 2.7694e-04 - val_loss: 9.4217e-04\n",
      "Epoch 31/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.2357e-04 - val_loss: 9.4068e-04\n",
      "Epoch 32/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.4095e-04 - val_loss: 8.7974e-04\n",
      "Epoch 33/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3130e-04 - val_loss: 8.6146e-04\n",
      "Epoch 34/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.3675e-04 - val_loss: 0.0015\n",
      "Epoch 35/1000\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 5.0202e-04 - val_loss: 9.0781e-04\n",
      "Epoch 36/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.5727e-04 - val_loss: 8.0285e-04\n",
      "Epoch 37/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.0176e-04 - val_loss: 0.0011\n",
      "Epoch 38/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.8543e-05 - val_loss: 7.1440e-04\n",
      "Epoch 39/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 8.6317e-05 - val_loss: 8.0108e-04\n",
      "Epoch 40/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 7.4150e-05 - val_loss: 7.1485e-04\n",
      "Epoch 41/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0272e-04 - val_loss: 0.0011\n",
      "Epoch 42/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.1419e-05 - val_loss: 7.8606e-04\n",
      "Epoch 43/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.7292e-05 - val_loss: 6.2111e-04\n",
      "Epoch 44/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.7211e-05 - val_loss: 6.9772e-04\n",
      "Epoch 45/1000\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 8.7580e-05 - val_loss: 8.0550e-04\n",
      "Epoch 46/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.3602e-05 - val_loss: 0.0012\n",
      "Epoch 47/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.3302e-05 - val_loss: 0.0013\n",
      "Epoch 48/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.2931e-05 - val_loss: 6.2192e-04\n",
      "Epoch 49/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 4.2843e-05 - val_loss: 6.2781e-04\n",
      "Epoch 50/1000\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 3.2895e-05 - val_loss: 6.0216e-04\n",
      "Epoch 51/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0447e-05 - val_loss: 6.0491e-04\n",
      "Epoch 52/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.8836e-05 - val_loss: 6.4559e-04\n",
      "Epoch 53/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.3848e-05 - val_loss: 6.2483e-04\n",
      "Epoch 54/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.3662e-05 - val_loss: 5.7666e-04\n",
      "Epoch 55/1000\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 2.8728e-05 - val_loss: 5.7468e-04\n",
      "Epoch 56/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.4815e-05 - val_loss: 6.2800e-04\n",
      "Epoch 57/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0487e-05 - val_loss: 5.9055e-04\n",
      "Epoch 58/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.5018e-05 - val_loss: 6.0282e-04\n",
      "Epoch 59/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5635e-05 - val_loss: 6.8009e-04\n",
      "Epoch 60/1000\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 3.1267e-05 - val_loss: 6.3590e-04\n",
      "Epoch 61/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.4823e-05 - val_loss: 7.0902e-04\n",
      "Epoch 62/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6357e-05 - val_loss: 5.3999e-04\n",
      "Epoch 63/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0749e-05 - val_loss: 7.2687e-04\n",
      "Epoch 64/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.4960e-05 - val_loss: 5.2195e-04\n",
      "Epoch 65/1000\n",
      "80/80 [==============================] - 8s 102ms/step - loss: 1.7249e-05 - val_loss: 7.8765e-04\n",
      "Epoch 66/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7521e-05 - val_loss: 5.4256e-04\n",
      "Epoch 67/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.1444e-05 - val_loss: 5.2248e-04\n",
      "Epoch 68/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0856e-05 - val_loss: 0.0011\n",
      "Epoch 69/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0383e-05 - val_loss: 6.8294e-04\n",
      "Epoch 70/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 9.2608e-06 - val_loss: 5.0130e-04\n",
      "Epoch 71/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.0191e-05 - val_loss: 4.9005e-04\n",
      "Epoch 72/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.6597e-06 - val_loss: 5.0641e-04\n",
      "Epoch 73/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.5304e-06 - val_loss: 5.2017e-04\n",
      "Epoch 74/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.2321e-06 - val_loss: 4.7239e-04\n",
      "Epoch 75/1000\n",
      "80/80 [==============================] - 8s 104ms/step - loss: 7.3668e-06 - val_loss: 4.5719e-04\n",
      "Epoch 76/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.9048e-05 - val_loss: 0.0012\n",
      "Epoch 77/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.1289e-05 - val_loss: 6.9278e-04\n",
      "Epoch 78/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 6.3273e-06 - val_loss: 4.8355e-04\n",
      "Epoch 79/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 5.3845e-06 - val_loss: 5.1889e-04\n",
      "Epoch 80/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 5.5046e-06 - val_loss: 4.9071e-04\n",
      "Epoch 81/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.6899e-06 - val_loss: 4.7545e-04\n",
      "Epoch 82/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.3276e-06 - val_loss: 4.3072e-04\n",
      "Epoch 83/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.6218e-06 - val_loss: 4.7438e-04\n",
      "Epoch 84/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.7068e-06 - val_loss: 4.4555e-04\n",
      "Epoch 85/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 4.4142e-06 - val_loss: 4.3756e-04\n",
      "Epoch 86/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.2332e-06 - val_loss: 4.5365e-04\n",
      "Epoch 87/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.6759e-06 - val_loss: 6.5050e-04\n",
      "Epoch 88/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.1278e-06 - val_loss: 4.4521e-04\n",
      "Epoch 89/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.7023e-06 - val_loss: 7.3388e-04\n",
      "Epoch 90/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 3.2178e-06 - val_loss: 4.7300e-04\n",
      "Epoch 91/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0779e-06 - val_loss: 0.0010\n",
      "Epoch 92/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.2850e-06 - val_loss: 5.4462e-04\n",
      "Epoch 93/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.7146e-06 - val_loss: 5.9692e-04\n",
      "Epoch 94/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.7027e-06 - val_loss: 4.3934e-04\n",
      "Epoch 95/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.8286e-06 - val_loss: 4.4886e-04\n",
      "Epoch 96/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5005e-06 - val_loss: 4.7062e-04\n",
      "Epoch 97/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5689e-06 - val_loss: 4.5233e-04\n",
      "Epoch 98/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.1626e-06 - val_loss: 4.7822e-04\n",
      "Epoch 99/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.3075e-06 - val_loss: 0.0010\n",
      "Epoch 100/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 9.8385e-06 - val_loss: 6.7635e-04\n",
      "Epoch 101/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.9462e-06 - val_loss: 4.6742e-04\n",
      "Epoch 102/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.2124e-06 - val_loss: 4.2500e-04\n",
      "Epoch 103/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.1013 - val_loss: 18.5838\n",
      "Epoch 104/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.6269 - val_loss: 0.0756\n",
      "Epoch 105/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0308 - val_loss: 0.0167\n",
      "Epoch 106/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0131 - val_loss: 0.0117\n",
      "Epoch 107/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0071 - val_loss: 0.0078\n",
      "Epoch 108/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0045 - val_loss: 0.0068\n",
      "Epoch 109/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0069 - val_loss: 0.0054\n",
      "Epoch 110/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0029 - val_loss: 0.0045\n",
      "Epoch 111/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0022 - val_loss: 0.0038\n",
      "Epoch 112/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 113/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 114/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 115/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0013 - val_loss: 0.0039\n",
      "Epoch 116/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 117/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.9776e-04 - val_loss: 0.0027\n",
      "Epoch 118/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 119/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 120/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 9.1827e-04 - val_loss: 0.0045\n",
      "Epoch 121/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.2805e-04 - val_loss: 0.0022\n",
      "Epoch 122/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.4963e-04 - val_loss: 0.0024\n",
      "Epoch 123/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.4652e-04 - val_loss: 0.0024\n",
      "Epoch 124/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.4977e-04 - val_loss: 0.0028\n",
      "Epoch 125/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 5.3741e-04 - val_loss: 0.0020\n",
      "Epoch 126/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.8865e-04 - val_loss: 0.0033\n",
      "Epoch 127/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.1356e-04 - val_loss: 0.0020\n",
      "Epoch 128/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0039 - val_loss: 0.0021\n",
      "Epoch 129/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.9074e-04 - val_loss: 0.0032\n",
      "Epoch 130/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 3.9066e-04 - val_loss: 0.0015\n",
      "Epoch 131/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.4142e-04 - val_loss: 0.0015\n",
      "Epoch 132/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.9880e-04 - val_loss: 0.0013\n",
      "Epoch 133/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.4778e-04 - val_loss: 0.0015\n",
      "Epoch 134/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.9901e-04 - val_loss: 0.0015\n",
      "Epoch 135/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 8.0740e-04 - val_loss: 0.0018\n",
      "Epoch 136/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5737e-04 - val_loss: 0.0012\n",
      "Epoch 137/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.5371e-04 - val_loss: 0.0013\n",
      "Epoch 138/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6423e-04 - val_loss: 0.0013\n",
      "Epoch 139/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2873e-04 - val_loss: 0.0014\n",
      "Epoch 140/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.3435e-04 - val_loss: 0.0012\n",
      "Epoch 141/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3071e-04 - val_loss: 0.0026\n",
      "Epoch 142/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2901e-04 - val_loss: 0.0011\n",
      "Epoch 143/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.0146e-05 - val_loss: 0.0012\n",
      "Epoch 144/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.8876e-05 - val_loss: 0.0014\n",
      "Epoch 145/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 8.5412e-05 - val_loss: 0.0016\n",
      "Epoch 146/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.7624e-05 - val_loss: 0.0011\n",
      "Epoch 147/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.3194e-05 - val_loss: 0.0010\n",
      "Epoch 148/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.0216e-05 - val_loss: 9.8644e-04\n",
      "Epoch 149/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.2708e-05 - val_loss: 9.7388e-04\n",
      "Epoch 150/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 8.4079e-05 - val_loss: 0.0017\n",
      "Epoch 151/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.0884e-05 - val_loss: 9.9105e-04\n",
      "Epoch 152/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.8392e-05 - val_loss: 0.0014\n",
      "Epoch 153/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 8s 94ms/step - loss: 6.1789e-05 - val_loss: 9.6167e-04\n",
      "Epoch 154/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 5.0259e-05 - val_loss: 0.0012\n",
      "Epoch 155/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 4.6608e-05 - val_loss: 9.9245e-04\n",
      "Epoch 156/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.1611e-05 - val_loss: 0.0010\n",
      "Epoch 157/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.1819e-05 - val_loss: 9.2923e-04\n",
      "Epoch 158/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.8634e-05 - val_loss: 9.0677e-04\n",
      "Epoch 159/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.8864e-05 - val_loss: 9.4714e-04\n",
      "Epoch 160/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.4281e-05 - val_loss: 0.0010\n",
      "Epoch 161/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 3.9509e-05 - val_loss: 9.0674e-04\n",
      "Epoch 162/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 6.2614e-05 - val_loss: 0.0010\n",
      "Epoch 163/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 3.1446e-05 - val_loss: 0.0012\n",
      "Epoch 164/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.8696e-05 - val_loss: 9.5217e-04\n",
      "Epoch 165/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.7192e-05 - val_loss: 0.0012\n",
      "Epoch 166/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.6459e-05 - val_loss: 0.0010\n",
      "Epoch 167/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.4937e-05 - val_loss: 8.7702e-04\n",
      "Epoch 168/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.4770 - val_loss: 0.0969\n",
      "Epoch 169/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0270 - val_loss: 0.0066\n",
      "Epoch 170/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0028 - val_loss: 0.0035\n",
      "Epoch 171/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0014 - val_loss: 0.0030\n",
      "Epoch 172/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.4947e-04 - val_loss: 0.0027\n",
      "Epoch 173/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.1968e-04 - val_loss: 0.0024\n",
      "Epoch 174/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.1442e-04 - val_loss: 0.0024\n",
      "Epoch 175/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 4.0617e-04 - val_loss: 0.0021\n",
      "Epoch 176/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.9203e-04 - val_loss: 0.0020\n",
      "Epoch 177/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.6928e-04 - val_loss: 0.0020\n",
      "Epoch 178/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.1223e-04 - val_loss: 0.0018\n",
      "Epoch 179/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.8598e-04 - val_loss: 0.0017\n",
      "Epoch 180/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.6853e-04 - val_loss: 0.0018\n",
      "Epoch 181/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7086e-04 - val_loss: 0.0016\n",
      "Epoch 182/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6750e-04 - val_loss: 0.0016\n",
      "Epoch 183/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1113e-04 - val_loss: 0.0017\n",
      "Epoch 184/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0208e-04 - val_loss: 0.0017\n",
      "Epoch 185/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 9.3403e-05 - val_loss: 0.0015\n",
      "Epoch 186/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.6812e-05 - val_loss: 0.0014\n",
      "Epoch 187/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.3625e-05 - val_loss: 0.0017\n",
      "Epoch 188/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.9790e-05 - val_loss: 0.0014\n",
      "Epoch 189/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 6.1631e-05 - val_loss: 0.0014\n",
      "Epoch 190/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 5.7766e-05 - val_loss: 0.0014\n",
      "Epoch 191/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.8012e-05 - val_loss: 0.0013\n",
      "Epoch 192/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.7622e-05 - val_loss: 0.0013\n",
      "Epoch 193/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.1454e-05 - val_loss: 0.0013\n",
      "Epoch 194/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.9335e-05 - val_loss: 0.0013\n",
      "Epoch 195/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 3.5613e-05 - val_loss: 0.0013\n",
      "Epoch 196/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.2142e-05 - val_loss: 0.0013\n",
      "Epoch 197/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.1613e-05 - val_loss: 0.0013\n",
      "Epoch 198/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0634e-05 - val_loss: 0.0013\n",
      "Epoch 199/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0150e-05 - val_loss: 0.0013\n",
      "Epoch 200/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 2.7293e-05 - val_loss: 0.0011\n",
      "Epoch 201/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.4969e-05 - val_loss: 0.0011\n",
      "Epoch 202/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.3786e-05 - val_loss: 0.0012\n",
      "Epoch 203/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.4098e-05 - val_loss: 0.0011\n",
      "Epoch 204/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0438e-05 - val_loss: 0.0011\n",
      "Epoch 205/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.7931e-05 - val_loss: 0.0011\n",
      "Epoch 206/1000\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 1.7283e-05 - val_loss: 0.0011\n",
      "Epoch 207/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6708e-05 - val_loss: 0.0014\n",
      "Epoch 208/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6423e-05 - val_loss: 0.0011\n",
      "Epoch 209/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.5944e-05 - val_loss: 0.0010\n",
      "Epoch 210/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.4128e-05 - val_loss: 0.0010\n",
      "Epoch 211/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3072e-05 - val_loss: 0.0010\n",
      "Epoch 212/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.2428e-05 - val_loss: 0.0010\n",
      "Epoch 213/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1273e-05 - val_loss: 0.0010\n",
      "Epoch 214/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0809e-05 - val_loss: 0.0013\n",
      "Epoch 215/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.2164e-05 - val_loss: 0.0011\n",
      "Epoch 216/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.6867e-06 - val_loss: 0.0011\n",
      "Epoch 217/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 9.1284e-06 - val_loss: 9.9660e-04\n",
      "Epoch 218/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 8.4546e-06 - val_loss: 9.6583e-04\n",
      "Epoch 219/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.9089e-06 - val_loss: 9.9038e-04\n",
      "Epoch 220/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 8.3559e-06 - val_loss: 9.7507e-04\n",
      "Epoch 221/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 7.5404e-06 - val_loss: 9.8115e-04\n",
      "Epoch 222/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.9939e-06 - val_loss: 9.5735e-04\n",
      "Epoch 223/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.7907e-06 - val_loss: 9.4208e-04\n",
      "Epoch 224/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.6336e-06 - val_loss: 9.0758e-04\n",
      "Epoch 225/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 6.1715e-06 - val_loss: 0.0010\n",
      "Epoch 226/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.6150e-06 - val_loss: 8.5222e-04\n",
      "Epoch 227/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.8871e-06 - val_loss: 8.9904e-04\n",
      "Epoch 228/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.9586e-06 - val_loss: 8.9454e-04\n",
      "Epoch 229/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 8s 94ms/step - loss: 4.7498e-06 - val_loss: 0.0014\n",
      "Epoch 230/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 5.6386e-06 - val_loss: 0.0011\n",
      "Epoch 231/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.5538e-06 - val_loss: 8.8109e-04\n",
      "Epoch 232/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.0110e-06 - val_loss: 9.4529e-04\n",
      "Epoch 233/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.7596e-06 - val_loss: 9.5718e-04\n",
      "Epoch 234/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.6571e-06 - val_loss: 8.9089e-04\n",
      "Epoch 235/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.5127e-06 - val_loss: 8.6937e-04\n",
      "Epoch 236/1000\n",
      "80/80 [==============================] - 7s 93ms/step - loss: 3.3000e-06 - val_loss: 7.8627e-04\n",
      "Epoch 237/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 3.1408e-06 - val_loss: 8.2114e-04\n",
      "Epoch 238/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.2785e-06 - val_loss: 9.2162e-04\n",
      "Epoch 239/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0571e-06 - val_loss: 8.2893e-04\n",
      "Epoch 240/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.8251e-06 - val_loss: 8.5083e-04\n",
      "Epoch 241/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.6647e-06 - val_loss: 8.6666e-04\n",
      "Epoch 242/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.3782e-06 - val_loss: 7.8922e-04\n",
      "Epoch 243/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5042e-06 - val_loss: 8.2471e-04\n",
      "Epoch 244/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.5097e-06 - val_loss: 7.7488e-04\n",
      "Epoch 245/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 2.1325e-06 - val_loss: 8.2192e-04\n",
      "Epoch 246/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.9463e-06 - val_loss: 8.3745e-04\n",
      "Epoch 247/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.9040e-06 - val_loss: 7.0878e-04\n",
      "Epoch 248/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7770e-06 - val_loss: 7.9464e-04\n",
      "Epoch 249/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7923e-06 - val_loss: 7.8645e-04\n",
      "Epoch 250/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.5900e-06 - val_loss: 7.2527e-04\n",
      "Epoch 251/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.4911e-06 - val_loss: 7.6756e-04\n",
      "Epoch 252/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6273e-06 - val_loss: 8.0643e-04\n",
      "Epoch 253/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3969e-06 - val_loss: 8.0538e-04\n",
      "Epoch 254/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3092e-06 - val_loss: 8.1157e-04\n",
      "Epoch 255/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.4259e-06 - val_loss: 7.4611e-04\n",
      "Epoch 256/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1591e-06 - val_loss: 6.8468e-04\n",
      "Epoch 257/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.1960e-06 - val_loss: 8.3886e-04\n",
      "Epoch 258/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1381e-06 - val_loss: 7.6409e-04\n",
      "Epoch 259/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0292e-06 - val_loss: 7.7988e-04\n",
      "Epoch 260/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 9.9478e-07 - val_loss: 7.5031e-04\n",
      "Epoch 261/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.8787e-07 - val_loss: 7.4833e-04\n",
      "Epoch 262/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.5547e-07 - val_loss: 7.7057e-04\n",
      "Epoch 263/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.5286e-07 - val_loss: 7.4721e-04\n",
      "Epoch 264/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.1886e-07 - val_loss: 7.4107e-04\n",
      "Epoch 265/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 7.3962e-07 - val_loss: 7.6693e-04\n",
      "Epoch 266/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.9411e-07 - val_loss: 7.7423e-04\n",
      "Epoch 267/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.6448e-07 - val_loss: 7.7845e-04\n",
      "Epoch 268/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.2541e-07 - val_loss: 6.4238e-04\n",
      "Epoch 269/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 6.5646e-07 - val_loss: 6.4778e-04\n",
      "Epoch 270/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 6.1140e-07 - val_loss: 9.8844e-04\n",
      "Epoch 271/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.5190e-07 - val_loss: 7.9421e-04\n",
      "Epoch 272/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 6.3721e-07 - val_loss: 8.0859e-04\n",
      "Epoch 273/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.2269e-07 - val_loss: 6.9033e-04\n",
      "Epoch 274/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 4.9830e-07 - val_loss: 9.9185e-04\n",
      "Epoch 275/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 5.9589e-07 - val_loss: 7.1164e-04\n",
      "Epoch 276/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.8091e-07 - val_loss: 8.2955e-04\n",
      "Epoch 277/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.4041e-07 - val_loss: 7.2298e-04\n",
      "Epoch 278/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 4.1738e-07 - val_loss: 6.4493e-04\n",
      "Epoch 279/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.6706e-07 - val_loss: 8.0122e-04\n",
      "Epoch 280/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.6697e-07 - val_loss: 7.2960e-04\n",
      "Epoch 281/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.4582e-07 - val_loss: 7.0565e-04\n",
      "Epoch 282/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.5368e-07 - val_loss: 7.4778e-04\n",
      "Epoch 283/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.6629 - val_loss: 0.0593\n",
      "Epoch 284/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0189 - val_loss: 0.0131\n",
      "Epoch 285/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 286/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0028 - val_loss: 0.0052\n",
      "Epoch 287/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0022 - val_loss: 0.0039\n",
      "Epoch 288/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0017 - val_loss: 0.0043\n",
      "Epoch 289/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0015 - val_loss: 0.0095\n",
      "Epoch 290/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.0014 - val_loss: 0.0035\n",
      "Epoch 291/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.4348e-04 - val_loss: 0.0029\n",
      "Epoch 292/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.0435e-04 - val_loss: 0.0038\n",
      "Epoch 293/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.8242e-04 - val_loss: 0.0031\n",
      "Epoch 294/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.2920e-04 - val_loss: 0.0028\n",
      "Epoch 295/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 4.1298e-04 - val_loss: 0.0024\n",
      "Epoch 296/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.5854e-04 - val_loss: 0.0024\n",
      "Epoch 297/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.3353e-04 - val_loss: 0.0023\n",
      "Epoch 298/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0095e-04 - val_loss: 0.0023\n",
      "Epoch 299/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.7492e-04 - val_loss: 0.0031\n",
      "Epoch 300/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 2.5315e-04 - val_loss: 0.0021\n",
      "Epoch 301/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.3827e-04 - val_loss: 0.0022\n",
      "Epoch 302/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.9295e-04 - val_loss: 0.0021\n",
      "Epoch 303/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0122e-04 - val_loss: 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 304/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6684e-04 - val_loss: 0.0020\n",
      "Epoch 305/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.5933e-04 - val_loss: 0.0021\n",
      "Epoch 306/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3954e-04 - val_loss: 0.0020\n",
      "Epoch 307/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3274e-04 - val_loss: 0.0020\n",
      "Epoch 308/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3559e-04 - val_loss: 0.0019\n",
      "Epoch 309/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6708e-04 - val_loss: 0.0024\n",
      "Epoch 310/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.1162 - val_loss: 0.0194\n",
      "Epoch 311/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0084 - val_loss: 0.0078\n",
      "Epoch 312/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0034 - val_loss: 0.0052\n",
      "Epoch 313/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0019 - val_loss: 0.0040\n",
      "Epoch 314/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 315/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 9.0907e-04 - val_loss: 0.0029\n",
      "Epoch 316/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 7.0713e-04 - val_loss: 0.0026\n",
      "Epoch 317/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.5940e-04 - val_loss: 0.0025\n",
      "Epoch 318/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.6566e-04 - val_loss: 0.0023\n",
      "Epoch 319/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 3.9676e-04 - val_loss: 0.0023\n",
      "Epoch 320/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.3232e-04 - val_loss: 0.0021\n",
      "Epoch 321/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 2.8676e-04 - val_loss: 0.0021\n",
      "Epoch 322/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.4827e-04 - val_loss: 0.0020\n",
      "Epoch 323/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.2146e-04 - val_loss: 0.0019\n",
      "Epoch 324/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0077e-04 - val_loss: 0.0019\n",
      "Epoch 325/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.7125e-04 - val_loss: 0.0018\n",
      "Epoch 326/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.5916e-04 - val_loss: 0.0019\n",
      "Epoch 327/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3780e-04 - val_loss: 0.0017\n",
      "Epoch 328/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.2767e-04 - val_loss: 0.0018\n",
      "Epoch 329/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1350e-04 - val_loss: 0.0019\n",
      "Epoch 330/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.0591e-04 - val_loss: 0.0016\n",
      "Epoch 331/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.8297e-05 - val_loss: 0.0017\n",
      "Epoch 332/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.9813e-05 - val_loss: 0.0017\n",
      "Epoch 333/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.1533e-05 - val_loss: 0.0017\n",
      "Epoch 334/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 7.7424e-05 - val_loss: 0.0019\n",
      "Epoch 335/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 6.9278e-05 - val_loss: 0.0016\n",
      "Epoch 336/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.2931e-05 - val_loss: 0.0016\n",
      "Epoch 337/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.9165e-05 - val_loss: 0.0015\n",
      "Epoch 338/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.5059e-05 - val_loss: 0.0016\n",
      "Epoch 339/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.1005e-05 - val_loss: 0.0015\n",
      "Epoch 340/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 4.7565e-05 - val_loss: 0.0016\n",
      "Epoch 341/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 4.4961e-05 - val_loss: 0.0015\n",
      "Epoch 342/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.1475e-05 - val_loss: 0.0016\n",
      "Epoch 343/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.8434e-05 - val_loss: 0.0016\n",
      "Epoch 344/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.6461e-05 - val_loss: 0.0016\n",
      "Epoch 345/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 3.3766e-05 - val_loss: 0.0014\n",
      "Epoch 346/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.2550e-05 - val_loss: 0.0016\n",
      "Epoch 347/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0419e-05 - val_loss: 0.0014\n",
      "Epoch 348/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.8159e-05 - val_loss: 0.0014\n",
      "Epoch 349/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5785e-05 - val_loss: 0.0013\n",
      "Epoch 350/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 2.4886e-05 - val_loss: 0.0013\n",
      "Epoch 351/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.2837e-05 - val_loss: 0.0015\n",
      "Epoch 352/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.1573e-05 - val_loss: 0.0013\n",
      "Epoch 353/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0752e-05 - val_loss: 0.0012\n",
      "Epoch 354/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.9842e-05 - val_loss: 0.0013\n",
      "Epoch 355/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.8278e-05 - val_loss: 0.0014\n",
      "Epoch 356/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7371e-05 - val_loss: 0.0013\n",
      "Epoch 357/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6290e-05 - val_loss: 0.0014\n",
      "Epoch 358/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.5144e-05 - val_loss: 0.0012\n",
      "Epoch 359/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.4498e-05 - val_loss: 0.0014\n",
      "Epoch 360/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.4265e-05 - val_loss: 0.0014\n",
      "Epoch 361/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2750e-05 - val_loss: 0.0012\n",
      "Epoch 362/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2242e-05 - val_loss: 0.0012\n",
      "Epoch 363/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.1597e-05 - val_loss: 0.0014\n",
      "Epoch 364/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0948e-05 - val_loss: 0.0011\n",
      "Epoch 365/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.0659e-05 - val_loss: 0.0013\n",
      "Epoch 366/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0031e-05 - val_loss: 0.0012\n",
      "Epoch 367/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0474e-05 - val_loss: 0.0011\n",
      "Epoch 368/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.9599e-06 - val_loss: 0.0012\n",
      "Epoch 369/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.4425e-06 - val_loss: 0.0012\n",
      "Epoch 370/1000\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 7.8830e-06 - val_loss: 0.0011\n",
      "Epoch 371/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.3751e-06 - val_loss: 0.0013\n",
      "Epoch 372/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.3379e-06 - val_loss: 0.0012\n",
      "Epoch 373/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.6566e-06 - val_loss: 0.0011\n",
      "Epoch 374/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.6851e-06 - val_loss: 0.0013\n",
      "Epoch 375/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 6.0857e-06 - val_loss: 0.0011\n",
      "Epoch 376/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.9053e-06 - val_loss: 0.0012\n",
      "Epoch 377/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 5.4368e-06 - val_loss: 0.0011\n",
      "Epoch 378/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.2701e-06 - val_loss: 0.0012\n",
      "Epoch 379/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.9582e-06 - val_loss: 0.0013\n",
      "Epoch 380/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 4.7515e-06 - val_loss: 0.0012\n",
      "Epoch 381/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.4785e-06 - val_loss: 0.0011\n",
      "Epoch 382/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.2784e-06 - val_loss: 0.0011\n",
      "Epoch 383/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.1981e-06 - val_loss: 0.0011\n",
      "Epoch 384/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.9428e-06 - val_loss: 0.0013\n",
      "Epoch 385/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 3.6863e-06 - val_loss: 0.0012\n",
      "Epoch 386/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.5007e-06 - val_loss: 0.0011\n",
      "Epoch 387/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.3206e-06 - val_loss: 0.0010\n",
      "Epoch 388/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 3.0648e-06 - val_loss: 0.0011\n",
      "Epoch 389/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.9296e-06 - val_loss: 9.4664e-04\n",
      "Epoch 390/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.8802e-06 - val_loss: 0.0010\n",
      "Epoch 391/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.7645e-06 - val_loss: 0.0011\n",
      "Epoch 392/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5539e-06 - val_loss: 0.0011\n",
      "Epoch 393/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.4323e-06 - val_loss: 0.0012\n",
      "Epoch 394/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.3310e-06 - val_loss: 0.0011\n",
      "Epoch 395/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.2006e-06 - val_loss: 0.0010\n",
      "Epoch 396/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0862e-06 - val_loss: 0.0011\n",
      "Epoch 397/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0117e-06 - val_loss: 0.0011\n",
      "Epoch 398/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.9893e-06 - val_loss: 0.0010\n",
      "Epoch 399/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7719e-06 - val_loss: 0.0013\n",
      "Epoch 400/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.6990e-06 - val_loss: 9.8564e-04\n",
      "Epoch 401/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6862e-06 - val_loss: 9.9173e-04\n",
      "Epoch 402/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.5500e-06 - val_loss: 9.9671e-04\n",
      "Epoch 403/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.5035e-06 - val_loss: 0.0010\n",
      "Epoch 404/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.3914e-06 - val_loss: 9.2421e-04\n",
      "Epoch 405/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.3445e-06 - val_loss: 0.0012\n",
      "Epoch 406/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3032e-06 - val_loss: 9.9662e-04\n",
      "Epoch 407/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2654e-06 - val_loss: 0.0012\n",
      "Epoch 408/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2217e-06 - val_loss: 9.3652e-04\n",
      "Epoch 409/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0976e-06 - val_loss: 0.0011\n",
      "Epoch 410/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.0299e-06 - val_loss: 0.0011\n",
      "Epoch 411/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.7577e-07 - val_loss: 8.6524e-04\n",
      "Epoch 412/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 9.6912e-07 - val_loss: 0.0010\n",
      "Epoch 413/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.9205e-07 - val_loss: 9.8706e-04\n",
      "Epoch 414/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.5679e-07 - val_loss: 0.0012\n",
      "Epoch 415/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 8.3907e-07 - val_loss: 9.9922e-04\n",
      "Epoch 416/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.8785e-07 - val_loss: 0.0011\n",
      "Epoch 417/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.3538e-07 - val_loss: 9.2706e-04\n",
      "Epoch 418/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.3831e-07 - val_loss: 0.0011\n",
      "Epoch 419/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.7458e-07 - val_loss: 0.0010\n",
      "Epoch 420/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 6.7787e-07 - val_loss: 9.4779e-04\n",
      "Epoch 421/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.1492e-07 - val_loss: 0.0010\n",
      "Epoch 422/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.0048e-07 - val_loss: 9.8939e-04\n",
      "Epoch 423/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.5532e-07 - val_loss: 9.3757e-04\n",
      "Epoch 424/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.4615e-07 - val_loss: 9.7323e-04\n",
      "Epoch 425/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 5.1269e-07 - val_loss: 9.8176e-04\n",
      "Epoch 426/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 4.8870e-07 - val_loss: 9.8491e-04\n",
      "Epoch 427/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.8172e-07 - val_loss: 9.7473e-04\n",
      "Epoch 428/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.5388e-07 - val_loss: 8.9827e-04\n",
      "Epoch 429/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.5066e-07 - val_loss: 8.3307e-04\n",
      "Epoch 430/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 4.0425e-07 - val_loss: 9.6107e-04\n",
      "Epoch 431/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.8878e-07 - val_loss: 0.0011\n",
      "Epoch 432/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.8459e-07 - val_loss: 8.5980e-04\n",
      "Epoch 433/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.4765e-07 - val_loss: 9.4853e-04\n",
      "Epoch 434/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.4385e-07 - val_loss: 9.2779e-04\n",
      "Epoch 435/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.3006e-07 - val_loss: 9.7760e-04\n",
      "Epoch 436/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.9882e-07 - val_loss: 0.0010\n",
      "Epoch 437/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.8384e-07 - val_loss: 9.6685e-04\n",
      "Epoch 438/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.7367e-07 - val_loss: 0.0011\n",
      "Epoch 439/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5884e-07 - val_loss: 9.7356e-04\n",
      "Epoch 440/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.5057e-07 - val_loss: 0.0010\n",
      "Epoch 441/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.4001e-07 - val_loss: 0.0010\n",
      "Epoch 442/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.2721e-07 - val_loss: 0.0012\n",
      "Epoch 443/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.2402e-07 - val_loss: 9.8887e-04\n",
      "Epoch 444/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.1166e-07 - val_loss: 9.4754e-04\n",
      "Epoch 445/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 2.0342e-07 - val_loss: 8.6925e-04\n",
      "Epoch 446/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.9833e-07 - val_loss: 8.9044e-04\n",
      "Epoch 447/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.8432e-07 - val_loss: 0.0010\n",
      "Epoch 448/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7227e-07 - val_loss: 9.5960e-04\n",
      "Epoch 449/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6481e-07 - val_loss: 9.0296e-04\n",
      "Epoch 450/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.6250e-07 - val_loss: 8.8133e-04\n",
      "Epoch 451/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.4906e-07 - val_loss: 9.6728e-04\n",
      "Epoch 452/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.4427e-07 - val_loss: 9.2159e-04\n",
      "Epoch 453/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3707e-07 - val_loss: 8.9187e-04\n",
      "Epoch 454/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3198e-07 - val_loss: 8.8397e-04\n",
      "Epoch 455/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 8s 101ms/step - loss: 1.2393e-07 - val_loss: 9.2800e-04\n",
      "Epoch 456/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1812e-07 - val_loss: 9.3652e-04\n",
      "Epoch 457/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1820e-07 - val_loss: 9.0199e-04\n",
      "Epoch 458/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1202e-07 - val_loss: 8.9497e-04\n",
      "Epoch 459/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0323e-07 - val_loss: 8.3276e-04\n",
      "Epoch 460/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.0012e-07 - val_loss: 8.5021e-04\n",
      "Epoch 461/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0002e-07 - val_loss: 8.8457e-04\n",
      "Epoch 462/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.8846e-08 - val_loss: 0.0010\n",
      "Epoch 463/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.0782e-08 - val_loss: 9.2517e-04\n",
      "Epoch 464/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.3908e-08 - val_loss: 9.9861e-04\n",
      "Epoch 465/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 7.9339e-08 - val_loss: 0.0011\n",
      "Epoch 466/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.5576e-08 - val_loss: 8.2844e-04\n",
      "Epoch 467/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.3055e-08 - val_loss: 8.5845e-04\n",
      "Epoch 468/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.8475e-08 - val_loss: 8.9932e-04\n",
      "Epoch 469/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.7201e-08 - val_loss: 0.0010\n",
      "Epoch 470/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 6.4477e-08 - val_loss: 8.4276e-04\n",
      "Epoch 471/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.0240e-08 - val_loss: 9.8837e-04\n",
      "Epoch 472/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 5.7817e-08 - val_loss: 8.6896e-04\n",
      "Epoch 473/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 5.7333e-08 - val_loss: 9.0297e-04\n",
      "Epoch 474/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.2961e-08 - val_loss: 0.0010\n",
      "Epoch 475/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 5.0881e-08 - val_loss: 8.5466e-04\n",
      "Epoch 476/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.1047e-08 - val_loss: 9.2227e-04\n",
      "Epoch 477/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.5546e-08 - val_loss: 8.5179e-04\n",
      "Epoch 478/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.6518e-08 - val_loss: 8.9253e-04\n",
      "Epoch 479/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.2285e-08 - val_loss: 9.1712e-04\n",
      "Epoch 480/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.9940e-08 - val_loss: 0.0010\n",
      "Epoch 481/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.8375e-08 - val_loss: 9.9270e-04\n",
      "Epoch 482/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.8224e-08 - val_loss: 0.0010\n",
      "Epoch 483/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.5767e-08 - val_loss: 9.6131e-04\n",
      "Epoch 484/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.4627e-08 - val_loss: 9.4347e-04\n",
      "Epoch 485/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 3.2950e-08 - val_loss: 8.4195e-04\n",
      "Epoch 486/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.1237e-08 - val_loss: 9.1306e-04\n",
      "Epoch 487/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.9973e-08 - val_loss: 8.3493e-04\n",
      "Epoch 488/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.8170e-08 - val_loss: 8.2903e-04\n",
      "Epoch 489/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.6695e-08 - val_loss: 8.6057e-04\n",
      "Epoch 490/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.5535e-08 - val_loss: 9.1130e-04\n",
      "Epoch 491/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.4403e-08 - val_loss: 8.6925e-04\n",
      "Epoch 492/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.3494e-08 - val_loss: 9.2108e-04\n",
      "Epoch 493/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 2.2308e-08 - val_loss: 8.9570e-04\n",
      "Epoch 494/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.2651e-08 - val_loss: 9.8112e-04\n",
      "Epoch 495/1000\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 2.0169e-08 - val_loss: 8.7679e-04\n",
      "Epoch 496/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.9903e-08 - val_loss: 9.4534e-04\n",
      "Epoch 497/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.9057e-08 - val_loss: 9.1502e-04\n",
      "Epoch 498/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.8308e-08 - val_loss: 9.3314e-04\n",
      "Epoch 499/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7182e-08 - val_loss: 9.6900e-04\n",
      "Epoch 500/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.6918e-08 - val_loss: 8.8432e-04\n",
      "Epoch 501/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.6176e-08 - val_loss: 9.4006e-04\n",
      "Epoch 502/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.5474e-08 - val_loss: 8.5613e-04\n",
      "Epoch 503/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.4591e-08 - val_loss: 0.0010\n",
      "Epoch 504/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3685e-08 - val_loss: 8.1144e-04\n",
      "Epoch 505/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.4660e-08 - val_loss: 8.7323e-04\n",
      "Epoch 506/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2599e-08 - val_loss: 9.5205e-04\n",
      "Epoch 507/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2433e-08 - val_loss: 8.2976e-04\n",
      "Epoch 508/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1960e-08 - val_loss: 9.1277e-04\n",
      "Epoch 509/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0954e-08 - val_loss: 9.0461e-04\n",
      "Epoch 510/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.0574e-08 - val_loss: 8.5544e-04\n",
      "Epoch 511/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.0286e-08 - val_loss: 9.8932e-04\n",
      "Epoch 512/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0222e-08 - val_loss: 9.0315e-04\n",
      "Epoch 513/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.3477e-09 - val_loss: 8.1904e-04\n",
      "Epoch 514/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.2173e-09 - val_loss: 8.3976e-04\n",
      "Epoch 515/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 8.5014e-09 - val_loss: 8.2513e-04\n",
      "Epoch 516/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.6789e-09 - val_loss: 9.3131e-04\n",
      "Epoch 517/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 7.8470e-09 - val_loss: 9.0934e-04\n",
      "Epoch 518/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.4571e-09 - val_loss: 8.8861e-04\n",
      "Epoch 519/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.2715e-09 - val_loss: 9.1056e-04\n",
      "Epoch 520/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 6.7015e-09 - val_loss: 8.3618e-04\n",
      "Epoch 521/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.9836e-09 - val_loss: 8.7572e-04\n",
      "Epoch 522/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 6.3393e-09 - val_loss: 9.3991e-04\n",
      "Epoch 523/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.1297e-09 - val_loss: 9.3981e-04\n",
      "Epoch 524/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.0514e-09 - val_loss: 9.6822e-04\n",
      "Epoch 525/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 5.4924e-09 - val_loss: 9.2417e-04\n",
      "Epoch 526/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.4075e-09 - val_loss: 9.3069e-04\n",
      "Epoch 527/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.2047e-09 - val_loss: 8.6600e-04\n",
      "Epoch 528/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.1138e-09 - val_loss: 8.9259e-04\n",
      "Epoch 529/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 8s 94ms/step - loss: 4.9872e-09 - val_loss: 9.8023e-04\n",
      "Epoch 530/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 4.5929e-09 - val_loss: 9.0481e-04\n",
      "Epoch 531/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.3558e-09 - val_loss: 8.7717e-04\n",
      "Epoch 532/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 4.4080e-09 - val_loss: 8.9084e-04\n",
      "Epoch 533/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 4.0379e-09 - val_loss: 8.6475e-04\n",
      "Epoch 534/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.8291e-09 - val_loss: 9.4557e-04\n",
      "Epoch 535/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.6229e-09 - val_loss: 0.0010\n",
      "Epoch 536/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.4046e-09 - val_loss: 8.8276e-04\n",
      "Epoch 537/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.2604e-09 - val_loss: 8.4797e-04\n",
      "Epoch 538/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 3.2908e-09 - val_loss: 9.7275e-04\n",
      "Epoch 539/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.1443e-09 - val_loss: 9.3357e-04\n",
      "Epoch 540/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.9523e-09 - val_loss: 9.4622e-04\n",
      "Epoch 541/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.8460e-09 - val_loss: 8.7244e-04\n",
      "Epoch 542/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.8608e-09 - val_loss: 9.6425e-04\n",
      "Epoch 543/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.6445e-09 - val_loss: 9.2527e-04\n",
      "Epoch 544/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.4528e-09 - val_loss: 8.7006e-04\n",
      "Epoch 545/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.5728e-09 - val_loss: 8.7507e-04\n",
      "Epoch 546/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.3328e-09 - val_loss: 9.5097e-04\n",
      "Epoch 547/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.2719e-09 - val_loss: 9.6631e-04\n",
      "Epoch 548/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.1729e-09 - val_loss: 9.0042e-04\n",
      "Epoch 549/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.2049e-09 - val_loss: 8.9218e-04\n",
      "Epoch 550/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.9621e-09 - val_loss: 8.7742e-04\n",
      "Epoch 551/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.8712e-09 - val_loss: 9.1495e-04\n",
      "Epoch 552/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.8067e-09 - val_loss: 9.5037e-04\n",
      "Epoch 553/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6519e-09 - val_loss: 8.8821e-04\n",
      "Epoch 554/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7597e-09 - val_loss: 9.1895e-04\n",
      "Epoch 555/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.6226e-09 - val_loss: 9.3861e-04\n",
      "Epoch 556/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.4869e-09 - val_loss: 8.8315e-04\n",
      "Epoch 557/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.5572e-09 - val_loss: 9.1025e-04\n",
      "Epoch 558/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.4617e-09 - val_loss: 9.1495e-04\n",
      "Epoch 559/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3438e-09 - val_loss: 9.2464e-04\n",
      "Epoch 560/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.2788e-09 - val_loss: 9.1547e-04\n",
      "Epoch 561/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2397e-09 - val_loss: 8.8588e-04\n",
      "Epoch 562/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.2212e-09 - val_loss: 9.9184e-04\n",
      "Epoch 563/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2239e-09 - val_loss: 9.1996e-04\n",
      "Epoch 564/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1195e-09 - val_loss: 9.9017e-04\n",
      "Epoch 565/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.1170e-09 - val_loss: 9.8027e-04\n",
      "Epoch 566/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1074e-09 - val_loss: 9.5829e-04\n",
      "Epoch 567/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.9871e-10 - val_loss: 9.3704e-04\n",
      "Epoch 568/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.8240e-10 - val_loss: 9.4246e-04\n",
      "Epoch 569/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.6026e-10 - val_loss: 9.2953e-04\n",
      "Epoch 570/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 9.2432e-10 - val_loss: 9.2769e-04\n",
      "Epoch 571/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.4011e-10 - val_loss: 9.2042e-04\n",
      "Epoch 572/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.4318e-10 - val_loss: 9.2937e-04\n",
      "Epoch 573/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.3010e-10 - val_loss: 9.2157e-04\n",
      "Epoch 574/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.1906e-10 - val_loss: 9.2093e-04\n",
      "Epoch 575/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 7.5475e-10 - val_loss: 9.4745e-04\n",
      "Epoch 576/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.2047e-10 - val_loss: 9.5508e-04\n",
      "Epoch 577/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.6136e-10 - val_loss: 9.3624e-04\n",
      "Epoch 578/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.4876e-10 - val_loss: 9.2069e-04\n",
      "Epoch 579/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.8834e-10 - val_loss: 9.0334e-04\n",
      "Epoch 580/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 5.8735e-10 - val_loss: 9.5066e-04\n",
      "Epoch 581/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.0034e-10 - val_loss: 9.3100e-04\n",
      "Epoch 582/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.6222e-10 - val_loss: 9.6609e-04\n",
      "Epoch 583/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 5.7225e-10 - val_loss: 9.3859e-04\n",
      "Epoch 584/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.2920e-10 - val_loss: 9.2391e-04\n",
      "Epoch 585/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 5.8571e-10 - val_loss: 9.7819e-04\n",
      "Epoch 586/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.9974e-10 - val_loss: 9.5616e-04\n",
      "Epoch 587/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.8976e-10 - val_loss: 9.3697e-04\n",
      "Epoch 588/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.5008e-10 - val_loss: 9.5992e-04\n",
      "Epoch 589/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.3720e-10 - val_loss: 0.0010\n",
      "Epoch 590/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 4.3846e-10 - val_loss: 0.0010\n",
      "Epoch 591/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.2039e-10 - val_loss: 9.8026e-04\n",
      "Epoch 592/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.3444e-10 - val_loss: 9.7682e-04\n",
      "Epoch 593/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.8444e-10 - val_loss: 9.7074e-04\n",
      "Epoch 594/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.9130e-10 - val_loss: 9.9360e-04\n",
      "Epoch 595/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.6703e-10 - val_loss: 9.8588e-04\n",
      "Epoch 596/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.9910e-10 - val_loss: 0.0010\n",
      "Epoch 597/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.3947e-10 - val_loss: 0.0010\n",
      "Epoch 598/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.1967e-10 - val_loss: 0.0010\n",
      "Epoch 599/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 3.1400e-10 - val_loss: 0.0010\n",
      "Epoch 600/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 2.9805e-10 - val_loss: 0.0011\n",
      "Epoch 601/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0260e-10 - val_loss: 0.0011\n",
      "Epoch 602/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.9110e-10 - val_loss: 0.0011\n",
      "Epoch 603/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.9584e-10 - val_loss: 0.0010\n",
      "Epoch 604/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.7399e-10 - val_loss: 0.0011\n",
      "Epoch 605/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.6095e-10 - val_loss: 0.0011\n",
      "Epoch 606/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5499e-10 - val_loss: 0.0011\n",
      "Epoch 607/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.4966e-10 - val_loss: 0.0011\n",
      "Epoch 608/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.5086e-10 - val_loss: 0.0012\n",
      "Epoch 609/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.4009e-10 - val_loss: 0.0011\n",
      "Epoch 610/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 2.4814e-10 - val_loss: 0.0011\n",
      "Epoch 611/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0735e-10 - val_loss: 0.0011\n",
      "Epoch 612/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.3402e-10 - val_loss: 0.0011\n",
      "Epoch 613/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0946e-10 - val_loss: 0.0011\n",
      "Epoch 614/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.1052e-10 - val_loss: 0.0012\n",
      "Epoch 615/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 2.0103e-10 - val_loss: 0.0012\n",
      "Epoch 616/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.8930e-10 - val_loss: 0.0012\n",
      "Epoch 617/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.9780e-10 - val_loss: 0.0012\n",
      "Epoch 618/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.0251e-10 - val_loss: 0.0013\n",
      "Epoch 619/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.9142e-10 - val_loss: 0.0012\n",
      "Epoch 620/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.7900e-10 - val_loss: 0.0012\n",
      "Epoch 621/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.8493e-10 - val_loss: 0.0012\n",
      "Epoch 622/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.6841e-10 - val_loss: 0.0012\n",
      "Epoch 623/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.5188e-10 - val_loss: 0.0012\n",
      "Epoch 624/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.5256e-10 - val_loss: 0.0012\n",
      "Epoch 625/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.4620e-10 - val_loss: 0.0012\n",
      "Epoch 626/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.7287e-10 - val_loss: 0.0012\n",
      "Epoch 627/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.4916e-10 - val_loss: 0.0012\n",
      "Epoch 628/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.4464e-10 - val_loss: 0.0012\n",
      "Epoch 629/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6496e-10 - val_loss: 0.0011\n",
      "Epoch 630/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.3305e-10 - val_loss: 0.0012\n",
      "Epoch 631/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2735e-10 - val_loss: 0.0012\n",
      "Epoch 632/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3794e-10 - val_loss: 0.0012\n",
      "Epoch 633/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3558e-10 - val_loss: 0.0012\n",
      "Epoch 634/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.3116e-10 - val_loss: 0.0012\n",
      "Epoch 635/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.3939e-10 - val_loss: 0.0012\n",
      "Epoch 636/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.0782e-10 - val_loss: 0.0012\n",
      "Epoch 637/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1696e-10 - val_loss: 0.0013\n",
      "Epoch 638/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.1032e-10 - val_loss: 0.0012\n",
      "Epoch 639/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1199e-10 - val_loss: 0.0012\n",
      "Epoch 640/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.1552e-10 - val_loss: 0.0012\n",
      "Epoch 641/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.2268e-10 - val_loss: 0.0012\n",
      "Epoch 642/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1295e-10 - val_loss: 0.0013\n",
      "Epoch 643/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1679e-10 - val_loss: 0.0012\n",
      "Epoch 644/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.0030e-10 - val_loss: 0.0013\n",
      "Epoch 645/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 9.5592e-11 - val_loss: 0.0013\n",
      "Epoch 646/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0635e-10 - val_loss: 0.0014\n",
      "Epoch 647/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.0958e-10 - val_loss: 0.0013\n",
      "Epoch 648/1000\n",
      "80/80 [==============================] - 8s 95ms/step - loss: 8.2850e-11 - val_loss: 0.0014\n",
      "Epoch 649/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.5160e-11 - val_loss: 0.0013\n",
      "Epoch 650/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 8.8882e-11 - val_loss: 0.0012\n",
      "Epoch 651/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0931e-10 - val_loss: 0.0015\n",
      "Epoch 652/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0276e-10 - val_loss: 0.0014\n",
      "Epoch 653/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0659e-10 - val_loss: 0.0013\n",
      "Epoch 654/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.7059e-11 - val_loss: 0.0013\n",
      "Epoch 655/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 9.0892e-11 - val_loss: 0.0013\n",
      "Epoch 656/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 9.5748e-11 - val_loss: 0.0013\n",
      "Epoch 657/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.2016e-11 - val_loss: 0.0012\n",
      "Epoch 658/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.0678e-10 - val_loss: 0.0015\n",
      "Epoch 659/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.6542e-11 - val_loss: 0.0012\n",
      "Epoch 660/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 0.6287 - val_loss: 0.0326\n",
      "Epoch 661/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0197 - val_loss: 0.0104\n",
      "Epoch 662/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0060 - val_loss: 0.0077\n",
      "Epoch 663/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0031 - val_loss: 0.0076\n",
      "Epoch 664/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 0.0023 - val_loss: 0.0047\n",
      "Epoch 665/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 666/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.9298e-04 - val_loss: 0.0030\n",
      "Epoch 667/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.1265e-04 - val_loss: 0.0026\n",
      "Epoch 668/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.3315e-04 - val_loss: 0.0025\n",
      "Epoch 669/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.4279e-04 - val_loss: 0.0024\n",
      "Epoch 670/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.8107e-04 - val_loss: 0.0026\n",
      "Epoch 671/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.5686e-04 - val_loss: 0.0023\n",
      "Epoch 672/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.1993e-04 - val_loss: 0.0022\n",
      "Epoch 673/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.9370e-04 - val_loss: 0.0020\n",
      "Epoch 674/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.7458e-04 - val_loss: 0.0024\n",
      "Epoch 675/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 1.2270e-04 - val_loss: 0.0021\n",
      "Epoch 676/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.2010e-04 - val_loss: 0.0021\n",
      "Epoch 677/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.0944e-05 - val_loss: 0.0019\n",
      "Epoch 678/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.1920e-05 - val_loss: 0.0020\n",
      "Epoch 679/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.4974e-05 - val_loss: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 680/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 8.2393e-05 - val_loss: 0.0020\n",
      "Epoch 681/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.5062e-05 - val_loss: 0.0020\n",
      "Epoch 682/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 6.4713e-05 - val_loss: 0.0018\n",
      "Epoch 683/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 5.8613e-05 - val_loss: 0.0019\n",
      "Epoch 684/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.1034e-05 - val_loss: 0.0019\n",
      "Epoch 685/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 4.9558e-05 - val_loss: 0.0017\n",
      "Epoch 686/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.6471e-05 - val_loss: 0.0017\n",
      "Epoch 687/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.2664e-05 - val_loss: 0.0018\n",
      "Epoch 688/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.0701e-05 - val_loss: 0.0017\n",
      "Epoch 689/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 4.4058e-05 - val_loss: 0.0018\n",
      "Epoch 690/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 3.8070e-05 - val_loss: 0.0018\n",
      "Epoch 691/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.1381e-05 - val_loss: 0.0018\n",
      "Epoch 692/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.2855e-05 - val_loss: 0.0017\n",
      "Epoch 693/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.8736e-05 - val_loss: 0.0016\n",
      "Epoch 694/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.8787e-05 - val_loss: 0.0018\n",
      "Epoch 695/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 3.0575e-05 - val_loss: 0.0017\n",
      "Epoch 696/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 3.0653e-05 - val_loss: 0.0017\n",
      "Epoch 697/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.0623e-05 - val_loss: 0.0017\n",
      "Epoch 698/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 2.1765e-05 - val_loss: 0.0017\n",
      "Epoch 699/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 2.3116e-05 - val_loss: 0.0019\n",
      "Epoch 700/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 2.0118e-05 - val_loss: 0.0016\n",
      "Epoch 701/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.8720e-05 - val_loss: 0.0015\n",
      "Epoch 702/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.6732e-05 - val_loss: 0.0015\n",
      "Epoch 703/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.5673e-05 - val_loss: 0.0016\n",
      "Epoch 704/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.5063e-05 - val_loss: 0.0016\n",
      "Epoch 705/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.4436e-05 - val_loss: 0.0016\n",
      "Epoch 706/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.4570e-05 - val_loss: 0.0015\n",
      "Epoch 707/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.2447e-05 - val_loss: 0.0018\n",
      "Epoch 708/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.4057e-05 - val_loss: 0.0016\n",
      "Epoch 709/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 1.1061e-05 - val_loss: 0.0015\n",
      "Epoch 710/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 1.0172e-05 - val_loss: 0.0016\n",
      "Epoch 711/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 1.0350e-05 - val_loss: 0.0015\n",
      "Epoch 712/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 9.2950e-06 - val_loss: 0.0015\n",
      "Epoch 713/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.3083e-06 - val_loss: 0.0015\n",
      "Epoch 714/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.1629e-06 - val_loss: 0.0014\n",
      "Epoch 715/1000\n",
      "80/80 [==============================] - 8s 101ms/step - loss: 8.5615e-06 - val_loss: 0.0016\n",
      "Epoch 716/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.4697e-06 - val_loss: 0.0015\n",
      "Epoch 717/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 8.0210e-06 - val_loss: 0.0015\n",
      "Epoch 718/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.3709e-06 - val_loss: 0.0015\n",
      "Epoch 719/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 7.2344e-06 - val_loss: 0.0014\n",
      "Epoch 720/1000\n",
      "80/80 [==============================] - 8s 100ms/step - loss: 6.2127e-06 - val_loss: 0.0016\n",
      "Epoch 721/1000\n",
      "80/80 [==============================] - 8s 94ms/step - loss: 6.4315e-06 - val_loss: 0.0014\n",
      "Epoch 722/1000\n",
      "80/80 [==============================] - 7s 94ms/step - loss: 6.1578e-06 - val_loss: 0.0014\n",
      "Epoch 723/1000\n",
      "59/80 [=====================>........] - ETA: 1s - loss: 5.7561e-06"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_history_disc \u001b[38;5;241m=\u001b[39m \u001b[43mdisc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisc_y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_y_label_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdisc_y_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_y_label_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_history_disc = disc_model.fit(disc_y_train, disc_y_label_train,\n",
    "                                       batch_size=128,\n",
    "                                       epochs=1000,\n",
    "                                       shuffle=True,\n",
    "                                       validation_data=(disc_y_val, disc_y_label_val),\n",
    "                                       callbacks=[tensorboard_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4e37b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
