{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60fb7440-14b9-4c2b-adbb-94d44e433e3f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This script is an example to run the code for the data challenge with explanations. Details of each step can be found in each section. Functions used are from the ``` ./lib ``` folder. \n",
    "\n",
    "The structure of the code is:\n",
    "1. Data preprocessing:\n",
    "    - Removing NANs in the data\n",
    "    - Standardrization\n",
    "2. Training: \n",
    "    - Dimension reduction with autoencoder\n",
    "    - GAN/CNN for super-resolution (SR)\n",
    "    \n",
    "## Code structure:\n",
    "- Data preprocessing:\n",
    "    * ``` preprocess.py ``` provides the functions needed for data preprocessing\n",
    "- GAN for super-resolution\n",
    "    * ``` models.py ``` provides the models needed in GAN\n",
    "    * ``` GAN_class.py ``` contains the class defined for full GAN training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8dbda-c13e-4617-8d20-e4dc3b187e34",
   "metadata": {},
   "source": [
    "# Code for running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edadce7-68a0-4e10-9e44-e9cfb9c2a81e",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"./lib\")\n",
    "from preprocess import *\n",
    "from models import *\n",
    "from GAN_class import *\n",
    "from AE_class import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf4540-2b00-409b-90e8-c12353cb8735",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f14ee-af01-474f-b135-6548ecf5a60d",
   "metadata": {},
   "source": [
    "The function ``` data_preprocess ``` will take the list of data sets and parameters required in the preprocessing.\n",
    "\n",
    "The parameters are explained below:\n",
    "\n",
    "1. For NAN removal:\n",
    "    - ``` \"nan_dim_along\" ```: the dimension along which we remove NAN data\n",
    "    - ``` \"nan_data_irrelevant\" ```: the irrelevant data to detect NAN\n",
    "    - ``` \"output_folder\" ```: folder for data output\n",
    "    - ``` \"file_format\" ```: the file name format used for saving NAN-removed data\n",
    "2. For Standardrization:\n",
    "    - ``` \"stat_dim\" ```: the statistical dimension of the data, used for mean, stddev, etc.\n",
    "    - ``` \"std_file_format\" ```: the file name format used for standardrized data file h5\n",
    "    - ``` \"std_data_format\" ```: the data name format used for h5 standardrized data \n",
    "    - ``` \"std_data_list\" ```: list of data to be standardrized\n",
    "    - ``` \"std_dataset_list\" ```: list of data sets to be standardrized\n",
    "    - ``` \"num_error_tolerance\" ```: numerical tolerance for passing the standardrization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecd2eda-f516-41e4-abb0-787909629a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up parameters\n",
    "output_folder = \"../data/preprocessed/\"\n",
    "file_format= \"%s\"\n",
    "std_file_format = \"np_gan_standard\"\n",
    "std_data_format = \"np_%s\"\n",
    "parameters = {\"nan_dim_along\":\"time\", \"nan_data_irrelevant\":\"absolute_height\", \\\n",
    "              \"output_folder\":output_folder,\"file_format\":file_format, \\\n",
    "              \"stat_dim\" : \"time\",\\\n",
    "              \"std_file_format\":std_file_format, \"std_data_format\":std_data_format,\\\n",
    "              \"std_data_list\":[\"u\",\"v\"], \\\n",
    "              \"std_dataset_list\":[\"perdigao_low_res_1H_2020\",\"perdigao_high_res_1H_2020\"],\\\n",
    "              \"num_error_tolerance\":1e-5}\n",
    "list_of_data_set_path=['../data/perdigao_era5_2020.nc', '../data/perdigao_low_res_1H_2020.nc', '../data/perdigao_high_res_1H_2020.nc' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start data_preprocess\n",
    "data_preprocess(list_of_data_set_path, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02a5bd-9d06-4570-a1bc-c4241ff7311b",
   "metadata": {},
   "source": [
    "### Example for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5f1ed1-5dfc-426a-a186-3bb6d1718d97",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data in file ../data/preprocessed/np_gan_standard.h5 are: \n",
      " ['np_perdigao_high_res_1H_2020_mean', 'np_perdigao_high_res_1H_2020_raw', 'np_perdigao_high_res_1H_2020_std', 'np_perdigao_high_res_1H_2020_stddev', 'np_perdigao_low_res_1H_2020_mean', 'np_perdigao_low_res_1H_2020_raw', 'np_perdigao_low_res_1H_2020_std', 'np_perdigao_low_res_1H_2020_stddev']\n",
      "Examining data np_perdigao_high_res_1H_2020_mean\n",
      "Examining data np_perdigao_high_res_1H_2020_raw\n",
      "Examining data np_perdigao_high_res_1H_2020_std\n",
      "Examining data np_perdigao_high_res_1H_2020_stddev\n",
      "Examining data np_perdigao_low_res_1H_2020_mean\n",
      "Examining data np_perdigao_low_res_1H_2020_raw\n",
      "Examining data np_perdigao_low_res_1H_2020_std\n",
      "Loading data np_perdigao_low_res_1H_2020_std from file ../data/preprocessed/np_gan_standard.h5\n",
      "Data in file ../data/preprocessed/np_gan_standard.h5 are: \n",
      " ['np_perdigao_high_res_1H_2020_mean', 'np_perdigao_high_res_1H_2020_raw', 'np_perdigao_high_res_1H_2020_std', 'np_perdigao_high_res_1H_2020_stddev', 'np_perdigao_low_res_1H_2020_mean', 'np_perdigao_low_res_1H_2020_raw', 'np_perdigao_low_res_1H_2020_std', 'np_perdigao_low_res_1H_2020_stddev']\n",
      "Examining data np_perdigao_high_res_1H_2020_mean\n",
      "Examining data np_perdigao_high_res_1H_2020_raw\n",
      "Examining data np_perdigao_high_res_1H_2020_std\n",
      "Loading data np_perdigao_high_res_1H_2020_std from file ../data/preprocessed/np_gan_standard.h5\n",
      "Got data_x with shape:(8520, 96, 96, 2), data_y with shape:(8520, 192, 192, 2)\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"../data/preprocessed/\"\n",
    "file_format = \"np_gan_standard\"\n",
    "xy_keyword_dict = {\"x\":\"low\", \"y\":\"high\"}\n",
    "data_xy = get_data_xy_from_h5(output_folder, file_format, xy_keyword_dict, exclude_list = [\"stddev\", \"mean\",\"raw\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b14520-0c82-4b03-a2a6-c88fe8a86e5f",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36f670-4e9b-49b8-b46a-afc4f3a6d1de",
   "metadata": {},
   "source": [
    "### Key parameters\n",
    "The parameters for HIER-AE are explained below:\n",
    "1. For training, ``` parameters[\"train\"] ```:\n",
    "    - ``` \"batch_size\", \"shuffle\"```: the generic parameters for improvements\n",
    "    - ``` \"n_sub_net\" ```: number of sub nets in the hierarchi\n",
    "    - ``` \"latent_dim_en\" ```: latent dimension size of the encoder \n",
    "    - ``` \"latent_dim_de_origin\" ```: latent dimension size of the decoder in the lowest level, later ones will be multiplied by the number of level.\n",
    "    - ``` \"log_path_file_format\" ```: file_format for string formatting for log output\n",
    "    - ``` \"sub_net_epochs\" ```: number of epochs for each level of sub net\n",
    "2. For data IO and manipulation, ``` parameters[\"data\"] ```:\n",
    "    - ``` \"output_folder\" ```: the output folder of the data preprocess, used as input for loading data\n",
    "    - ``` \"file_format\" ```: the file name format used for standardrized data file h5\n",
    "    - ``` \"xy_keyword_dict\" ```: the data name key used for detecting and catogorizing x and y from h5 standardrized datasets. Here we only load high res data and use y the same as x\n",
    "    - ``` \"xy_exclude_list\" ```: list of dataset type to be skipped if appeared in the data set name, like stddev, mean and std if we only need raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "021735a6-557d-46aa-a742-377f9287dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_AE = dict()\n",
    "\n",
    "parameters_AE[\"train\"] = {\"batch_size\": 128,\n",
    "                       \"shuffle\": True,\n",
    "                       \"n_sub_net\":4,\n",
    "                       \"latent_dim_en\":18,\n",
    "                       \"latent_dim_de_origin\":18,\n",
    "                       \"log_path_file_format\":\"../data/log/%s\",\n",
    "                       \"sub_net_epochs\":[5,4,3,2]}\n",
    "parameters_AE[\"data\"] = {'output_folder': \"../data/preprocessed/\",\n",
    "                      'file_format': \"np_gan_standard\",\n",
    "                      'xy_keyword_dict': {\"x\":\"high\", \"y\":\"high\"}, #only load high res data\n",
    "                      'xy_exclude_list': [\"stddev\", \"mean\",\"std\"]} #here we only need to load \"raw\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c12a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step by step running example\n",
    "#initialize model with parameters_AE\n",
    "model_AE = AWWSM4_HIER_AE(parameters_AE)\n",
    "#load data\n",
    "model_AE.load_data()\n",
    "#split data\n",
    "model_AE.split_data()\n",
    "#perform the training for each sub net one by one\n",
    "model_AE.generate_AE_one_by_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fad0a1-d45e-4ed6-a79e-90d11c9e55d2",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f9960-a475-4c09-b9e2-323f4fad76d9",
   "metadata": {},
   "source": [
    "### Key parameters\n",
    "The parameters for GAN are explained below:\n",
    "1. For training, ``` parameters[\"train\"] ```:\n",
    "    - ``` \"learning_rate_g\" ```: the learning rate for the generator\n",
    "    - ``` \"learning_rate_d\" ```: the learning rate for the discriminator\n",
    "    - ``` \"beta_1\", \"beta_2\", \"epsilon\", \"batch_size\"```: the generic parameters for improvements\n",
    "    - ``` \"n_epochs_pretrain\" ```: number of epochs for pretraining\n",
    "    - ``` \"n_epochs_GAN\" ```: number of epochs for full GAN\n",
    "2. For data IO and manipulation, ``` parameters[\"data\"] ```:\n",
    "    - ``` \"output_folder\" ```: the output folder of the data preprocess, used as input for loading data\n",
    "    - ``` \"file_format\" ```: the file name format used for standardrized data file h5\n",
    "    - ``` \"xy_keyword_dict\" ```: the data name key used for detecting and catogorizing x and y from h5 standardrized datasets \n",
    "    - ``` \"xy_exclude_list\" ```: list of dataset type to be skipped if appeared in the data set name, like stddev, mean, raw if we only need standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c9fdddc-2981-4b55-991b-37b72e2cb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_GAN = dict()\n",
    "\n",
    "parameters_GAN[\"train\"] = {\"learning_rate_g\": 1e-4, \n",
    "                       \"learning_rate_d\": 1e-4,\n",
    "                       \"beta_1\": 0.9,\n",
    "                       \"beta_2\": 0.999,\n",
    "                       \"epsilon\": 1e-08,\n",
    "                       \"batch_size\": 128,\n",
    "                       \"alpha_advers\": 1e-3,\n",
    "                       \"n_epochs_pretrain\":1, \n",
    "                       \"n_epochs_GAN\":1}\n",
    "parameters_GAN[\"data\"] = {'output_folder': \"../data/preprocessed/\",\n",
    "                      'file_format': \"np_gan_standard\",\n",
    "                      'xy_keyword_dict': {\"x\":\"low\", \"y\":\"high\"},\n",
    "                      'xy_exclude_list': [\"stddev\", \"mean\",\"raw\"]} #only keep the \"std\"(standardrized) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50afcf80-b144-4a2d-b829-701f3579200a",
   "metadata": {},
   "source": [
    "### Step by step running example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4673fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a GAN model class with doing_pretrain = True \n",
    "model = AWWSM4_SR_GAN(parameters=parameters_GAN, is_GAN=True, doing_pretrain=True) \n",
    "# load and split data based on the parameters[\"data\"]\n",
    "model.load_data()\n",
    "model.split_data()\n",
    "# pretrain and save the model\n",
    "model.pretrain() #use default epoch value = 20\n",
    "model.save_gen_model(\"./temp_v0_gen.h5\")\n",
    "# OPTIONAL: create another model which loaded the pretrained weights and can continue doing pretrain\n",
    "model2 = AWWSM4_SR_GAN(parameters=parameters_GAN, is_GAN=True, doing_pretrain=True)\n",
    "model2.load_gen_model(\"./temp_v0_gen.h5\")\n",
    "model2.load_data()\n",
    "model2.split_data()\n",
    "# continue to work on GAN\n",
    "model.reset_working_mode(doing_pretrain=False)\n",
    "model.train_GAN(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_working_mode(doing_pretrain=False)\n",
    "model.train_GAN(epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.0",
   "language": "python",
   "name": "tensorflow-2.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
