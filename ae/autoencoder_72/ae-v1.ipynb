{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98621cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/twang/.local/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/twang/.local/lib/python3.8/site-packages (from sklearn) (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/twang/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/twang/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/twang/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c225bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datetime in /home/twang/.local/lib/python3.8/site-packages (4.5)\n",
      "Requirement already satisfied: zope.interface in /home/twang/.local/lib/python3.8/site-packages (from datetime) (5.4.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from datetime) (2022.1)\n",
      "Requirement already satisfied: setuptools in /home/twang/.local/lib/python3.8/site-packages (from zope.interface->datetime) (62.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16fb8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "import h5py\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, Dense, Activation, Flatten, Conv2D, Conv2DTranspose, MaxPooling2D, BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fccf082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8530, 192, 192, 2)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('processed_data_ae/np_data.h5', 'r') as hf:\n",
    "    data = hf['np_data'][:]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a474a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-2.47433877e+00  4.50653076e-01]\n",
      "   [-2.47227287e+00  4.22745228e-01]\n",
      "   [-2.49654698e+00  3.96081567e-01]\n",
      "   ...\n",
      "   [-2.49194694e+00 -1.17015743e+00]\n",
      "   [-2.54665089e+00 -1.16967881e+00]\n",
      "   [-2.57032084e+00 -1.18266952e+00]]\n",
      "\n",
      "  [[-2.52002358e+00  4.53049988e-01]\n",
      "   [-2.53445411e+00  3.97361368e-01]\n",
      "   [-2.54852366e+00  4.00280446e-01]\n",
      "   ...\n",
      "   [-2.58340573e+00 -1.12509418e+00]\n",
      "   [-2.60144782e+00 -1.14579189e+00]\n",
      "   [-2.60572982e+00 -1.15639758e+00]]\n",
      "\n",
      "  [[-2.51759076e+00  4.58887845e-01]\n",
      "   [-2.52549720e+00  4.21892971e-01]\n",
      "   [-2.51069617e+00  4.61187005e-01]\n",
      "   ...\n",
      "   [-2.59329128e+00 -1.10369992e+00]\n",
      "   [-2.60617042e+00 -1.12321818e+00]\n",
      "   [-2.60261035e+00 -1.12687624e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.58731222e+00 -9.34629977e-01]\n",
      "   [-1.57048082e+00 -1.39101851e+00]\n",
      "   [-1.55039418e+00 -1.14039171e+00]\n",
      "   ...\n",
      "   [-5.02201855e-01 -1.07426441e+00]\n",
      "   [-5.04015684e-01 -1.18869841e+00]\n",
      "   [-5.98666489e-01 -1.35226285e+00]]\n",
      "\n",
      "  [[-1.65483463e+00 -6.58889890e-01]\n",
      "   [-1.62351513e+00 -7.88042486e-01]\n",
      "   [-1.62279499e+00 -8.86478245e-01]\n",
      "   ...\n",
      "   [-6.37919605e-01 -1.23640931e+00]\n",
      "   [-7.01931238e-01 -1.27044737e+00]\n",
      "   [-8.57191563e-01 -1.34775925e+00]]\n",
      "\n",
      "  [[-1.66404259e+00 -5.71757734e-01]\n",
      "   [-1.65007830e+00 -4.31717396e-01]\n",
      "   [-1.69645834e+00 -4.98340040e-01]\n",
      "   ...\n",
      "   [-6.10894382e-01 -1.30109894e+00]\n",
      "   [-5.73691487e-01 -1.32908201e+00]\n",
      "   [-7.68366098e-01 -1.36549246e+00]]]\n",
      "\n",
      "\n",
      " [[[-3.35927320e+00  1.55068800e-01]\n",
      "   [-3.35877013e+00  1.15008183e-01]\n",
      "   [-3.37714481e+00  8.70303214e-02]\n",
      "   ...\n",
      "   [-2.78697777e+00 -8.46802950e-01]\n",
      "   [-2.75886726e+00 -8.21451962e-01]\n",
      "   [-2.72041678e+00 -8.19486916e-01]]\n",
      "\n",
      "  [[-3.35898185e+00  1.15031756e-01]\n",
      "   [-3.36772919e+00  8.37758556e-02]\n",
      "   [-3.36663651e+00  6.03065006e-02]\n",
      "   ...\n",
      "   [-2.95139718e+00 -8.84478152e-01]\n",
      "   [-2.91099358e+00 -8.66952717e-01]\n",
      "   [-2.86916065e+00 -8.52828085e-01]]\n",
      "\n",
      "  [[-3.39988351e+00  9.81865823e-02]\n",
      "   [-3.39923644e+00  8.54534879e-02]\n",
      "   [-3.37907887e+00  9.79605094e-02]\n",
      "   ...\n",
      "   [-3.05329919e+00 -9.19691741e-01]\n",
      "   [-3.02608585e+00 -8.86442184e-01]\n",
      "   [-2.99904466e+00 -8.76444161e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.20112813e+00 -6.78498566e-01]\n",
      "   [-1.35471475e+00 -1.20192111e+00]\n",
      "   [-1.28323138e+00 -8.38458002e-01]\n",
      "   ...\n",
      "   [ 5.60911059e-01 -6.12898529e-01]\n",
      "   [ 5.03960073e-01 -6.94635093e-01]\n",
      "   [ 3.91693741e-01 -7.51808405e-01]]\n",
      "\n",
      "  [[-1.28114486e+00 -4.83557373e-01]\n",
      "   [-1.33647811e+00 -7.42457807e-01]\n",
      "   [-1.31684494e+00 -6.10368967e-01]\n",
      "   ...\n",
      "   [ 5.36383331e-01 -6.42276347e-01]\n",
      "   [ 4.38993782e-01 -7.80356824e-01]\n",
      "   [ 1.02414854e-01 -8.57382238e-01]]\n",
      "\n",
      "  [[-1.48530066e+00 -4.37424183e-01]\n",
      "   [-1.42584360e+00 -4.54387993e-01]\n",
      "   [-1.42270005e+00 -3.36178690e-01]\n",
      "   ...\n",
      "   [ 4.26659912e-01 -7.31795132e-01]\n",
      "   [ 3.14698249e-01 -9.10490751e-01]\n",
      "   [-2.13123765e-02 -8.61507714e-01]]]\n",
      "\n",
      "\n",
      " [[[-3.65891957e+00  2.07189210e-02]\n",
      "   [-3.64897847e+00  4.30312520e-03]\n",
      "   [-3.66413999e+00 -1.98635198e-02]\n",
      "   ...\n",
      "   [-1.25920451e+00 -2.39865765e-01]\n",
      "   [-1.16967165e+00 -3.19075465e-01]\n",
      "   [-1.13408327e+00 -1.25381306e-01]]\n",
      "\n",
      "  [[-3.60892272e+00  7.77200423e-03]\n",
      "   [-3.61625481e+00  2.56141890e-02]\n",
      "   [-3.61728764e+00 -4.77572195e-02]\n",
      "   ...\n",
      "   [-1.31219208e+00 -2.32755542e-01]\n",
      "   [-1.24870527e+00 -3.27423900e-01]\n",
      "   [-1.20976627e+00 -1.52948037e-01]]\n",
      "\n",
      "  [[-3.62492681e+00  2.42949743e-03]\n",
      "   [-3.62284184e+00  5.93658071e-04]\n",
      "   [-3.57582259e+00 -1.83222294e-02]\n",
      "   ...\n",
      "   [-1.41011286e+00 -2.39639997e-01]\n",
      "   [-1.35021007e+00 -3.76991749e-01]\n",
      "   [-1.31783473e+00 -2.00048134e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-5.60546577e-01 -2.43067220e-01]\n",
      "   [-8.84502828e-01 -9.01560366e-01]\n",
      "   [-8.00945699e-01 -5.07532656e-01]\n",
      "   ...\n",
      "   [ 6.87900007e-01  1.43246353e-01]\n",
      "   [ 8.13344717e-01  1.71936825e-01]\n",
      "   [ 8.62113893e-01  2.44928777e-01]]\n",
      "\n",
      "  [[-4.75044638e-01 -1.43684343e-01]\n",
      "   [-6.94363415e-01 -6.36965275e-01]\n",
      "   [-6.76184475e-01 -3.20152611e-01]\n",
      "   ...\n",
      "   [ 7.97062874e-01  1.97750643e-01]\n",
      "   [ 8.59463513e-01  1.97554335e-01]\n",
      "   [ 8.57462168e-01  2.81393796e-01]]\n",
      "\n",
      "  [[-6.11702383e-01 -1.87379971e-01]\n",
      "   [-4.70837861e-01 -4.28626299e-01]\n",
      "   [-4.00710851e-01 -2.73787454e-02]\n",
      "   ...\n",
      "   [ 8.67786109e-01  2.61021405e-01]\n",
      "   [ 9.00242746e-01  2.49900058e-01]\n",
      "   [ 7.92923272e-01  2.66337842e-01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 5.77749109e+00 -2.01717925e+00]\n",
      "   [ 5.79472876e+00 -2.10839176e+00]\n",
      "   [ 5.78970146e+00 -2.10704660e+00]\n",
      "   ...\n",
      "   [ 7.22144747e+00 -2.25264120e+00]\n",
      "   [ 7.19457245e+00 -2.22437477e+00]\n",
      "   [ 7.12745047e+00 -2.29484248e+00]]\n",
      "\n",
      "  [[ 5.83163404e+00 -2.08884978e+00]\n",
      "   [ 5.87917852e+00 -2.24065089e+00]\n",
      "   [ 5.88652277e+00 -2.19025302e+00]\n",
      "   ...\n",
      "   [ 7.08095503e+00 -2.30317497e+00]\n",
      "   [ 7.10634804e+00 -2.22964382e+00]\n",
      "   [ 7.09136200e+00 -2.35766435e+00]]\n",
      "\n",
      "  [[ 5.82360125e+00 -2.12147975e+00]\n",
      "   [ 5.81628990e+00 -2.21937394e+00]\n",
      "   [ 5.77561808e+00 -2.13325381e+00]\n",
      "   ...\n",
      "   [ 7.57756186e+00 -2.40679622e+00]\n",
      "   [ 7.57251024e+00 -2.30930066e+00]\n",
      "   [ 7.53563499e+00 -2.47168016e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 7.85931015e+00 -5.79357910e+00]\n",
      "   [ 7.66005373e+00 -5.65330124e+00]\n",
      "   [ 7.48232841e+00 -5.48953009e+00]\n",
      "   ...\n",
      "   [ 5.51324224e+00 -5.17088270e+00]\n",
      "   [ 5.66533899e+00 -4.67577362e+00]\n",
      "   [ 5.74735403e+00 -4.93038034e+00]]\n",
      "\n",
      "  [[ 7.91476679e+00 -6.04804802e+00]\n",
      "   [ 7.72861671e+00 -5.75327110e+00]\n",
      "   [ 7.47300863e+00 -5.54203367e+00]\n",
      "   ...\n",
      "   [ 6.15049505e+00 -4.92262983e+00]\n",
      "   [ 5.80906153e+00 -4.74257708e+00]\n",
      "   [ 4.57275820e+00 -4.35110235e+00]]\n",
      "\n",
      "  [[ 7.75316620e+00 -6.13078308e+00]\n",
      "   [ 7.63899422e+00 -5.67586708e+00]\n",
      "   [ 7.45519590e+00 -5.69563675e+00]\n",
      "   ...\n",
      "   [ 5.38359022e+00 -4.65068769e+00]\n",
      "   [ 4.95847130e+00 -4.46576214e+00]\n",
      "   [ 4.90741158e+00 -4.21378946e+00]]]\n",
      "\n",
      "\n",
      " [[[ 6.36594534e+00 -1.63290846e+00]\n",
      "   [ 6.43227053e+00 -1.59654415e+00]\n",
      "   [ 6.49441195e+00 -1.63174331e+00]\n",
      "   ...\n",
      "   [ 7.48625040e+00 -1.46681488e+00]\n",
      "   [ 7.39505959e+00 -1.45083427e+00]\n",
      "   [ 7.24076319e+00 -1.58833468e+00]]\n",
      "\n",
      "  [[ 6.53959417e+00 -1.61757243e+00]\n",
      "   [ 6.58809137e+00 -1.64508486e+00]\n",
      "   [ 6.61010122e+00 -1.66895342e+00]\n",
      "   ...\n",
      "   [ 7.16223097e+00 -1.53513670e+00]\n",
      "   [ 7.12632942e+00 -1.46213186e+00]\n",
      "   [ 7.06753683e+00 -1.58389413e+00]]\n",
      "\n",
      "  [[ 6.48013353e+00 -1.51612842e+00]\n",
      "   [ 6.52232504e+00 -1.63018954e+00]\n",
      "   [ 6.55350733e+00 -1.53074157e+00]\n",
      "   ...\n",
      "   [ 7.57325125e+00 -1.58368838e+00]\n",
      "   [ 7.53097677e+00 -1.48548973e+00]\n",
      "   [ 7.46138382e+00 -1.55384219e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 7.76766539e+00 -4.91662073e+00]\n",
      "   [ 7.69962931e+00 -4.70486546e+00]\n",
      "   [ 7.74760675e+00 -4.77080679e+00]\n",
      "   ...\n",
      "   [ 5.95538855e+00 -4.51110411e+00]\n",
      "   [ 5.58040953e+00 -3.81485248e+00]\n",
      "   [ 5.50465202e+00 -3.98302650e+00]]\n",
      "\n",
      "  [[ 7.82962942e+00 -4.83796501e+00]\n",
      "   [ 7.84971952e+00 -4.79825926e+00]\n",
      "   [ 7.74199057e+00 -4.64223576e+00]\n",
      "   ...\n",
      "   [ 5.97762251e+00 -4.32689619e+00]\n",
      "   [ 5.96480799e+00 -3.82812572e+00]\n",
      "   [ 5.61877632e+00 -4.21916914e+00]]\n",
      "\n",
      "  [[ 7.82958937e+00 -4.74983978e+00]\n",
      "   [ 7.84230185e+00 -4.83642578e+00]\n",
      "   [ 7.73837137e+00 -4.62796736e+00]\n",
      "   ...\n",
      "   [ 6.36919832e+00 -4.06757116e+00]\n",
      "   [ 6.18784332e+00 -3.99815488e+00]\n",
      "   [ 5.42094469e+00 -3.88959432e+00]]]\n",
      "\n",
      "\n",
      " [[[ 7.39549828e+00 -8.29298794e-01]\n",
      "   [ 7.52560663e+00 -8.98325145e-01]\n",
      "   [ 7.59991217e+00 -7.90652335e-01]\n",
      "   ...\n",
      "   [ 7.92334747e+00 -1.03149998e+00]\n",
      "   [ 7.86692667e+00 -1.08050859e+00]\n",
      "   [ 7.66503000e+00 -1.10479558e+00]]\n",
      "\n",
      "  [[ 7.66517401e+00 -8.30848515e-01]\n",
      "   [ 7.77532530e+00 -9.32732582e-01]\n",
      "   [ 7.84918213e+00 -8.04452240e-01]\n",
      "   ...\n",
      "   [ 7.88452196e+00 -1.13500321e+00]\n",
      "   [ 7.72442484e+00 -1.12761438e+00]\n",
      "   [ 7.46826506e+00 -1.09021354e+00]]\n",
      "\n",
      "  [[ 7.73075151e+00 -7.84957588e-01]\n",
      "   [ 7.83019590e+00 -8.76498640e-01]\n",
      "   [ 7.87600470e+00 -7.07231939e-01]\n",
      "   ...\n",
      "   [ 8.09761238e+00 -1.19197786e+00]\n",
      "   [ 7.91963720e+00 -1.16243887e+00]\n",
      "   [ 7.71486425e+00 -1.08178508e+00]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 7.84434748e+00 -4.68226814e+00]\n",
      "   [ 7.51519442e+00 -4.01048279e+00]\n",
      "   [ 7.56382370e+00 -4.24236441e+00]\n",
      "   ...\n",
      "   [ 6.59535837e+00 -3.94987416e+00]\n",
      "   [ 6.36806631e+00 -4.14067268e+00]\n",
      "   [ 5.76865530e+00 -3.13759804e+00]]\n",
      "\n",
      "  [[ 7.84929991e+00 -4.57010031e+00]\n",
      "   [ 7.78112364e+00 -4.35873175e+00]\n",
      "   [ 7.76902390e+00 -4.31369257e+00]\n",
      "   ...\n",
      "   [ 6.26192617e+00 -3.97995567e+00]\n",
      "   [ 5.87935877e+00 -3.63503838e+00]\n",
      "   [ 5.25682402e+00 -3.14578724e+00]]\n",
      "\n",
      "  [[ 7.83275366e+00 -4.51681471e+00]\n",
      "   [ 8.11556530e+00 -4.60940599e+00]\n",
      "   [ 8.22128582e+00 -4.56708717e+00]\n",
      "   ...\n",
      "   [ 6.45659065e+00 -3.91094518e+00]\n",
      "   [ 6.39726496e+00 -3.26667881e+00]\n",
      "   [ 6.25076437e+00 -3.63977313e+00]]]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3a5cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.913855 -16.306587\n"
     ]
    }
   ],
   "source": [
    "print(np.max(data), np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab4d0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5118, 192, 192, 2)\n",
      "(1706, 192, 192, 2)\n",
      "(1706, 192, 192, 2)\n",
      "19.913855 18.198473 19.736868 -16.306587 -16.134562 -16.26034\n"
     ]
    }
   ],
   "source": [
    "#First split data into train+validation and test set\n",
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "#Next split training again into train and validation\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(np.max(X_train), np.max(X_val), np.max(X_test), np.min(X_train), np.min(X_val), np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ad31ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(latent_dim):\n",
    "    '''\n",
    "    return an encoder which encodes the input image into a latent vector with dimension latent_dim\n",
    "    '''\n",
    "    \n",
    "    X_input = Input((192, 192, 2))\n",
    "    \n",
    "    #FIXME Should we add BN layer? I currently add that between conv and relu for the first 4 sets of layers\n",
    "    X = Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    #FIXME Should we add some dropout layer to regularize the model? \n",
    "    #I didn't do that, but need to look at train/val error\n",
    "    \n",
    "    X = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(units=latent_dim)(X)\n",
    "    #FIXME Should we add an activation layer here? I didn't do it\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece76255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(latent_dim):\n",
    "    '''\n",
    "    return an encoder which encodes the input image into a latent vector with dimension latent_dim\n",
    "    '''\n",
    "    \n",
    "    X_input = Input((latent_dim))\n",
    "    \n",
    "    X = Dense(units=3*3*64, input_dim=latent_dim)(X_input)\n",
    "    X = Reshape((3,3,64))(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=64, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=64, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=32, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=32, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=16, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(filters=16, kernel_size=(3,3), strides=(2,2), padding=\"same\")(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2D(filters=2, kernel_size=(3,3), strides=(1,1), padding=\"same\")(X)    \n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0f2af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 192, 192, 2)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 192, 192, 16)      304       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 192, 192, 16)     64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 192, 192, 16)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 96, 96, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 96, 96, 16)        2320      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 96, 96, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 96, 96, 16)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 48, 48, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 48, 48, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 48, 48, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 24, 24, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 6, 6, 64)          36928     \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 3, 3, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 72)                41544     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113,864\n",
      "Trainable params: 113,672\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 11:48:41.589855: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-12 11:48:41.931324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30965 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:01:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "encoder_72_sub1 = encoder(72)\n",
    "print(encoder_72_sub1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48ad088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 576)               42048     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 3, 3, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 6, 6, 64)         36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 6, 6, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 12, 12, 64)       36928     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 12, 12, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 24, 24, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 24, 24, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 48, 48, 32)       9248      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 48, 48, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 96, 96, 16)       4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 96, 96, 16)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 192, 192, 16)     2320      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 192, 192, 16)      0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 192, 192, 2)       290       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151,618\n",
      "Trainable params: 151,234\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "decoder_72_sub1 = decoder(72)\n",
    "print(decoder_72_sub1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb97b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__() \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e454217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_72 = Autoencoder(encoder_72_sub1, decoder_72_sub1)\n",
    "autoencoder_72.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61fbfe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"autoencoder_trivial/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "checkpoint_filepath = 'autoencoder_trivial/ckp/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                               save_weights_only=True,\n",
    "                                                               save_freq=10*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b01a7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc2702f6700>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_72.load_weights(\"autoencoder_trivial/ckp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d182b36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 11:49:28.903805: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 - 2s - loss: 0.1363 - 2s/epoch - 34ms/step\n"
     ]
    }
   ],
   "source": [
    "loss = autoencoder_72.evaluate(X_test, X_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7525175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1706, 72)\n"
     ]
    }
   ],
   "source": [
    "letent_X_test = autoencoder_72.encoder(X_test)\n",
    "print(letent_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a58297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1706, 192, 192, 2)\n"
     ]
    }
   ],
   "source": [
    "X_test_regen = autoencoder_72.decoder(letent_X_test)\n",
    "print(X_test_regen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae73a89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8525925 -5.4978747\n"
     ]
    }
   ],
   "source": [
    "X_test_diff = X_test_regen - X_test\n",
    "print(np.max(X_test_diff), np.min(X_test_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "915a4cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.368899\n"
     ]
    }
   ],
   "source": [
    "print(np.std(X_test_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0774b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "40/40 [==============================] - 8s 151ms/step - loss: 0.1323 - val_loss: 0.1446\n",
      "Epoch 2/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1351 - val_loss: 0.1540\n",
      "Epoch 3/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1316 - val_loss: 0.1417\n",
      "Epoch 4/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1286 - val_loss: 0.1364\n",
      "Epoch 5/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1293 - val_loss: 0.1375\n",
      "Epoch 6/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1310 - val_loss: 0.1403\n",
      "Epoch 7/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1322 - val_loss: 0.1411\n",
      "Epoch 8/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1339 - val_loss: 0.1369\n",
      "Epoch 9/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1292 - val_loss: 0.1406\n",
      "Epoch 10/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1305 - val_loss: 0.1369\n",
      "Epoch 11/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1298 - val_loss: 0.1392\n",
      "Epoch 12/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1338 - val_loss: 0.1366\n",
      "Epoch 13/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1291 - val_loss: 0.1365\n",
      "Epoch 14/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1298 - val_loss: 0.1371\n",
      "Epoch 15/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1310 - val_loss: 0.1393\n",
      "Epoch 16/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1301 - val_loss: 0.1387\n",
      "Epoch 17/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1322 - val_loss: 0.1416\n",
      "Epoch 18/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1299 - val_loss: 0.1462\n",
      "Epoch 19/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1288 - val_loss: 0.1357\n",
      "Epoch 20/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1284 - val_loss: 0.1366\n",
      "Epoch 21/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1314 - val_loss: 0.1422\n",
      "Epoch 22/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1309 - val_loss: 0.1419\n",
      "Epoch 23/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1306 - val_loss: 0.1419\n",
      "Epoch 24/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1295 - val_loss: 0.1391\n",
      "Epoch 25/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1313 - val_loss: 0.1426\n",
      "Epoch 26/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1311 - val_loss: 0.1449\n",
      "Epoch 27/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1339 - val_loss: 0.1366\n",
      "Epoch 28/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1303 - val_loss: 0.1398\n",
      "Epoch 29/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1293 - val_loss: 0.1405\n",
      "Epoch 30/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1290 - val_loss: 0.1371\n",
      "Epoch 31/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1341 - val_loss: 0.1428\n",
      "Epoch 32/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1311 - val_loss: 0.1418\n",
      "Epoch 33/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1327 - val_loss: 0.1405\n",
      "Epoch 34/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1289 - val_loss: 0.1366\n",
      "Epoch 35/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1306 - val_loss: 0.1378\n",
      "Epoch 36/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1284 - val_loss: 0.1381\n",
      "Epoch 37/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1263 - val_loss: 0.1347\n",
      "Epoch 38/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1299 - val_loss: 0.1377\n",
      "Epoch 39/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1279 - val_loss: 0.1360\n",
      "Epoch 40/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1340 - val_loss: 0.1381\n",
      "Epoch 41/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1280 - val_loss: 0.1356\n",
      "Epoch 42/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1263 - val_loss: 0.1346\n",
      "Epoch 43/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1253 - val_loss: 0.1349\n",
      "Epoch 44/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1301 - val_loss: 0.1387\n",
      "Epoch 45/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1269 - val_loss: 0.1357\n",
      "Epoch 46/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1254 - val_loss: 0.1377\n",
      "Epoch 47/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1358 - val_loss: 0.1372\n",
      "Epoch 48/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1275 - val_loss: 0.1349\n",
      "Epoch 49/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1295 - val_loss: 0.1426\n",
      "Epoch 50/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1345 - val_loss: 0.1391\n",
      "Epoch 51/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1313 - val_loss: 0.1455\n",
      "Epoch 52/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1353 - val_loss: 0.1368\n",
      "Epoch 53/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1328 - val_loss: 0.1503\n",
      "Epoch 54/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1365 - val_loss: 0.1418\n",
      "Epoch 55/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1444 - val_loss: 0.1420\n",
      "Epoch 56/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1312 - val_loss: 0.1396\n",
      "Epoch 57/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1318 - val_loss: 0.1387\n",
      "Epoch 58/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1302 - val_loss: 0.1415\n",
      "Epoch 59/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1301 - val_loss: 0.1447\n",
      "Epoch 60/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1314 - val_loss: 0.1349\n",
      "Epoch 61/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1246 - val_loss: 0.1378\n",
      "Epoch 62/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1281 - val_loss: 0.1389\n",
      "Epoch 63/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1309 - val_loss: 0.1387\n",
      "Epoch 64/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1372 - val_loss: 0.1379\n",
      "Epoch 65/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1334 - val_loss: 0.1381\n",
      "Epoch 66/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1267 - val_loss: 0.1356\n",
      "Epoch 67/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1279 - val_loss: 0.1353\n",
      "Epoch 68/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1283 - val_loss: 0.1352\n",
      "Epoch 69/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1319 - val_loss: 0.1372\n",
      "Epoch 70/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1269 - val_loss: 0.1364\n",
      "Epoch 71/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1304 - val_loss: 0.1416\n",
      "Epoch 72/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1278 - val_loss: 0.1399\n",
      "Epoch 73/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1299 - val_loss: 0.1388\n",
      "Epoch 74/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1326 - val_loss: 0.1372\n",
      "Epoch 75/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1283 - val_loss: 0.1367\n",
      "Epoch 76/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1313 - val_loss: 0.1411\n",
      "Epoch 77/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1320 - val_loss: 0.1365\n",
      "Epoch 78/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1294 - val_loss: 0.1380\n",
      "Epoch 79/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1296 - val_loss: 0.1471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1304 - val_loss: 0.1368\n",
      "Epoch 81/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1318 - val_loss: 0.1457\n",
      "Epoch 82/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1353 - val_loss: 0.1416\n",
      "Epoch 83/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1314 - val_loss: 0.1397\n",
      "Epoch 84/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1316 - val_loss: 0.1364\n",
      "Epoch 85/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1282 - val_loss: 0.1475\n",
      "Epoch 86/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1391 - val_loss: 0.1384\n",
      "Epoch 87/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1345 - val_loss: 0.1427\n",
      "Epoch 88/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1284 - val_loss: 0.1371\n",
      "Epoch 89/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1275 - val_loss: 0.1376\n",
      "Epoch 90/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1295 - val_loss: 0.1377\n",
      "Epoch 91/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1303 - val_loss: 0.1369\n",
      "Epoch 92/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1270 - val_loss: 0.1354\n",
      "Epoch 93/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1284 - val_loss: 0.1385\n",
      "Epoch 94/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1290 - val_loss: 0.1376\n",
      "Epoch 95/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1274 - val_loss: 0.1399\n",
      "Epoch 96/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1307 - val_loss: 0.1404\n",
      "Epoch 97/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1312 - val_loss: 0.1366\n",
      "Epoch 98/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1299 - val_loss: 0.1364\n",
      "Epoch 99/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1278 - val_loss: 0.1346\n",
      "Epoch 100/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1259 - val_loss: 0.1376\n",
      "Epoch 101/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1307 - val_loss: 0.1423\n",
      "Epoch 102/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1314 - val_loss: 0.1380\n",
      "Epoch 103/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1329 - val_loss: 0.1437\n",
      "Epoch 104/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1348 - val_loss: 0.1405\n",
      "Epoch 105/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1279 - val_loss: 0.1373\n",
      "Epoch 106/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1309 - val_loss: 0.1394\n",
      "Epoch 107/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1283 - val_loss: 0.1401\n",
      "Epoch 108/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1349 - val_loss: 0.1476\n",
      "Epoch 109/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1301 - val_loss: 0.1389\n",
      "Epoch 110/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1308 - val_loss: 0.1528\n",
      "Epoch 111/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1309 - val_loss: 0.1402\n",
      "Epoch 112/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1272 - val_loss: 0.1358\n",
      "Epoch 113/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1279 - val_loss: 0.1454\n",
      "Epoch 114/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1282 - val_loss: 0.1349\n",
      "Epoch 115/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1328 - val_loss: 0.1459\n",
      "Epoch 116/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1354 - val_loss: 0.1551\n",
      "Epoch 117/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1334 - val_loss: 0.1436\n",
      "Epoch 118/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1293 - val_loss: 0.1388\n",
      "Epoch 119/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1329 - val_loss: 0.1464\n",
      "Epoch 120/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1351 - val_loss: 0.1391\n",
      "Epoch 121/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1275 - val_loss: 0.1400\n",
      "Epoch 122/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1297 - val_loss: 0.1378\n",
      "Epoch 123/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1298 - val_loss: 0.1455\n",
      "Epoch 124/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1299 - val_loss: 0.1359\n",
      "Epoch 125/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1272 - val_loss: 0.1382\n",
      "Epoch 126/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1264 - val_loss: 0.1347\n",
      "Epoch 127/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1327 - val_loss: 0.1391\n",
      "Epoch 128/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1303 - val_loss: 0.1361\n",
      "Epoch 129/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1269 - val_loss: 0.1390\n",
      "Epoch 130/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1292 - val_loss: 0.1364\n",
      "Epoch 131/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1262 - val_loss: 0.1352\n",
      "Epoch 132/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1260 - val_loss: 0.1350\n",
      "Epoch 133/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1260 - val_loss: 0.1380\n",
      "Epoch 134/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1273 - val_loss: 0.1380\n",
      "Epoch 135/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1275 - val_loss: 0.1351\n",
      "Epoch 136/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1277 - val_loss: 0.1388\n",
      "Epoch 137/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1285 - val_loss: 0.1362\n",
      "Epoch 138/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1291 - val_loss: 0.1384\n",
      "Epoch 139/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1276 - val_loss: 0.1375\n",
      "Epoch 140/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1296 - val_loss: 0.1396\n",
      "Epoch 141/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1310 - val_loss: 0.1376\n",
      "Epoch 142/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1284 - val_loss: 0.1347\n",
      "Epoch 143/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1287 - val_loss: 0.1354\n",
      "Epoch 144/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1267 - val_loss: 0.1375\n",
      "Epoch 145/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1268 - val_loss: 0.1359\n",
      "Epoch 146/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1291 - val_loss: 0.1388\n",
      "Epoch 147/10000\n",
      "10/40 [======>.......................] - ETA: 2s - loss: 0.1271"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder_72\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_history = autoencoder_72.fit(X_train, X_train,\n",
    "                                      batch_size=128,\n",
    "                                      epochs=10000,\n",
    "                                      shuffle=True,\n",
    "                                      validation_data=(X_val, X_val),\n",
    "                                      callbacks=[tensorboard_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84a8d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_llr = tf.keras.optimizers.Adam(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87e4049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_72.compile(optimizer=\"adam\", loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14f29104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "40/40 [==============================] - 5s 109ms/step - loss: 0.1360 - val_loss: 0.1403\n",
      "Epoch 2/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1390 - val_loss: 0.1541\n",
      "Epoch 3/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1405 - val_loss: 0.1499\n",
      "Epoch 4/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1405 - val_loss: 0.1484\n",
      "Epoch 5/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1460 - val_loss: 0.1494\n",
      "Epoch 6/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1474 - val_loss: 0.1491\n",
      "Epoch 7/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1397 - val_loss: 0.1415\n",
      "Epoch 8/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1357 - val_loss: 0.1481\n",
      "Epoch 9/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1374 - val_loss: 0.1443\n",
      "Epoch 10/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1325 - val_loss: 0.1398\n",
      "Epoch 11/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1340 - val_loss: 0.1406\n",
      "Epoch 12/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1404 - val_loss: 0.1675\n",
      "Epoch 13/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1471 - val_loss: 0.1433\n",
      "Epoch 14/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1382 - val_loss: 0.1528\n",
      "Epoch 15/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1347 - val_loss: 0.1387\n",
      "Epoch 16/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1352 - val_loss: 0.1391\n",
      "Epoch 17/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1412 - val_loss: 0.1506\n",
      "Epoch 18/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1390 - val_loss: 0.1397\n",
      "Epoch 19/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1351 - val_loss: 0.1466\n",
      "Epoch 20/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1353 - val_loss: 0.1475\n",
      "Epoch 21/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1431 - val_loss: 0.1486\n",
      "Epoch 22/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1453 - val_loss: 0.1465\n",
      "Epoch 23/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1427 - val_loss: 0.1427\n",
      "Epoch 24/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1342 - val_loss: 0.1364\n",
      "Epoch 25/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1380 - val_loss: 0.1487\n",
      "Epoch 26/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1459 - val_loss: 0.1551\n",
      "Epoch 27/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1491 - val_loss: 0.1469\n",
      "Epoch 28/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1376 - val_loss: 0.1396\n",
      "Epoch 29/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1427 - val_loss: 0.1486\n",
      "Epoch 30/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1347 - val_loss: 0.1390\n",
      "Epoch 31/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1341 - val_loss: 0.1522\n",
      "Epoch 32/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1397 - val_loss: 0.1698\n",
      "Epoch 33/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1438 - val_loss: 0.1469\n",
      "Epoch 34/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1412 - val_loss: 0.1446\n",
      "Epoch 35/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1425 - val_loss: 0.1465\n",
      "Epoch 36/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1396 - val_loss: 0.1459\n",
      "Epoch 37/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1416 - val_loss: 0.1434\n",
      "Epoch 38/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1396 - val_loss: 0.1529\n",
      "Epoch 39/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1459 - val_loss: 0.1539\n",
      "Epoch 40/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1339 - val_loss: 0.1435\n",
      "Epoch 41/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1413 - val_loss: 0.1481\n",
      "Epoch 42/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1407 - val_loss: 0.1495\n",
      "Epoch 43/10000\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 0.1477 - val_loss: 0.1505\n",
      "Epoch 44/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1525 - val_loss: 0.1413\n",
      "Epoch 45/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1412 - val_loss: 0.1458\n",
      "Epoch 46/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1366 - val_loss: 0.1523\n",
      "Epoch 47/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1375 - val_loss: 0.1455\n",
      "Epoch 48/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1324 - val_loss: 0.1443\n",
      "Epoch 49/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1381 - val_loss: 0.1426\n",
      "Epoch 50/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1429 - val_loss: 0.1456\n",
      "Epoch 51/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1371 - val_loss: 0.1428\n",
      "Epoch 52/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1418 - val_loss: 0.1477\n",
      "Epoch 53/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1371 - val_loss: 0.1376\n",
      "Epoch 54/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1314 - val_loss: 0.1497\n",
      "Epoch 55/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1409 - val_loss: 0.1413\n",
      "Epoch 56/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1352 - val_loss: 0.1386\n",
      "Epoch 57/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1314 - val_loss: 0.1391\n",
      "Epoch 58/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1363 - val_loss: 0.1442\n",
      "Epoch 59/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1394 - val_loss: 0.1386\n",
      "Epoch 60/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1440 - val_loss: 0.1420\n",
      "Epoch 61/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1476 - val_loss: 0.1475\n",
      "Epoch 62/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1438 - val_loss: 0.1513\n",
      "Epoch 63/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1427 - val_loss: 0.1543\n",
      "Epoch 64/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1413 - val_loss: 0.1485\n",
      "Epoch 65/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1386 - val_loss: 0.1524\n",
      "Epoch 66/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1382 - val_loss: 0.1411\n",
      "Epoch 67/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1420 - val_loss: 0.1544\n",
      "Epoch 68/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1423 - val_loss: 0.1507\n",
      "Epoch 69/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1374 - val_loss: 0.1440\n",
      "Epoch 70/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1399 - val_loss: 0.1402\n",
      "Epoch 71/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1323 - val_loss: 0.1384\n",
      "Epoch 72/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1327 - val_loss: 0.1543\n",
      "Epoch 73/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1439 - val_loss: 0.1447\n",
      "Epoch 74/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1504 - val_loss: 0.1453\n",
      "Epoch 75/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1416 - val_loss: 0.1443\n",
      "Epoch 76/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1425 - val_loss: 0.1499\n",
      "Epoch 77/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1411 - val_loss: 0.1444\n",
      "Epoch 78/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1351 - val_loss: 0.1502\n",
      "Epoch 79/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1401 - val_loss: 0.1425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1612 - val_loss: 0.1863\n",
      "Epoch 81/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1615 - val_loss: 0.1466\n",
      "Epoch 82/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1497 - val_loss: 0.1474\n",
      "Epoch 83/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1417 - val_loss: 0.1627\n",
      "Epoch 84/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1396 - val_loss: 0.1390\n",
      "Epoch 85/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1443 - val_loss: 0.1432\n",
      "Epoch 86/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1404 - val_loss: 0.1487\n",
      "Epoch 87/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1472 - val_loss: 0.1562\n",
      "Epoch 88/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1372 - val_loss: 0.1402\n",
      "Epoch 89/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1318 - val_loss: 0.1419\n",
      "Epoch 90/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1470 - val_loss: 0.1755\n",
      "Epoch 91/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1414 - val_loss: 0.1467\n",
      "Epoch 92/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1411 - val_loss: 0.1427\n",
      "Epoch 93/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1384 - val_loss: 0.1553\n",
      "Epoch 94/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1409 - val_loss: 0.1446\n",
      "Epoch 95/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1350 - val_loss: 0.1375\n",
      "Epoch 96/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1342 - val_loss: 0.1396\n",
      "Epoch 97/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1368 - val_loss: 0.1406\n",
      "Epoch 98/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1373 - val_loss: 0.1489\n",
      "Epoch 99/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1403 - val_loss: 0.1435\n",
      "Epoch 100/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1427 - val_loss: 0.1485\n",
      "Epoch 101/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1419 - val_loss: 0.1496\n",
      "Epoch 102/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1383 - val_loss: 0.1512\n",
      "Epoch 103/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1473 - val_loss: 0.1662\n",
      "Epoch 104/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1416 - val_loss: 0.1400\n",
      "Epoch 105/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1425 - val_loss: 0.1480\n",
      "Epoch 106/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1370 - val_loss: 0.1452\n",
      "Epoch 107/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1366 - val_loss: 0.1428\n",
      "Epoch 108/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1340 - val_loss: 0.1412\n",
      "Epoch 109/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1346 - val_loss: 0.1507\n",
      "Epoch 110/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1351 - val_loss: 0.1386\n",
      "Epoch 111/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1346 - val_loss: 0.1600\n",
      "Epoch 112/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1384 - val_loss: 0.1397\n",
      "Epoch 113/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1361 - val_loss: 0.1443\n",
      "Epoch 114/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1348 - val_loss: 0.1497\n",
      "Epoch 115/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1334 - val_loss: 0.1440\n",
      "Epoch 116/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1357 - val_loss: 0.1449\n",
      "Epoch 117/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1352 - val_loss: 0.1387\n",
      "Epoch 118/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1323 - val_loss: 0.1423\n",
      "Epoch 119/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1347 - val_loss: 0.1412\n",
      "Epoch 120/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1353 - val_loss: 0.1440\n",
      "Epoch 121/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1364 - val_loss: 0.1406\n",
      "Epoch 122/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1410 - val_loss: 0.1754\n",
      "Epoch 123/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1406 - val_loss: 0.1453\n",
      "Epoch 124/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1378 - val_loss: 0.1452\n",
      "Epoch 125/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1339 - val_loss: 0.1433\n",
      "Epoch 126/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1413 - val_loss: 0.1510\n",
      "Epoch 127/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1403 - val_loss: 0.1410\n",
      "Epoch 128/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1376 - val_loss: 0.1505\n",
      "Epoch 129/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1388 - val_loss: 0.1454\n",
      "Epoch 130/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1402 - val_loss: 0.1412\n",
      "Epoch 131/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1390 - val_loss: 0.1451\n",
      "Epoch 132/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1456 - val_loss: 0.1752\n",
      "Epoch 133/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1636 - val_loss: 0.1608\n",
      "Epoch 134/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1452 - val_loss: 0.1401\n",
      "Epoch 135/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1361 - val_loss: 0.1422\n",
      "Epoch 136/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1370 - val_loss: 0.1423\n",
      "Epoch 137/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1396 - val_loss: 0.1444\n",
      "Epoch 138/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1499 - val_loss: 0.1572\n",
      "Epoch 139/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1371 - val_loss: 0.1450\n",
      "Epoch 140/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1377 - val_loss: 0.1419\n",
      "Epoch 141/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1338 - val_loss: 0.1395\n",
      "Epoch 142/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1381 - val_loss: 0.1570\n",
      "Epoch 143/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1394 - val_loss: 0.1426\n",
      "Epoch 144/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1425 - val_loss: 0.1424\n",
      "Epoch 145/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1354 - val_loss: 0.1443\n",
      "Epoch 146/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1455 - val_loss: 0.1509\n",
      "Epoch 147/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1383 - val_loss: 0.1385\n",
      "Epoch 148/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1338 - val_loss: 0.1404\n",
      "Epoch 149/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1337 - val_loss: 0.1400\n",
      "Epoch 150/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1335 - val_loss: 0.1571\n",
      "Epoch 151/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1475 - val_loss: 0.1470\n",
      "Epoch 152/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1500 - val_loss: 0.1596\n",
      "Epoch 153/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1480 - val_loss: 0.1517\n",
      "Epoch 154/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1379 - val_loss: 0.1472\n",
      "Epoch 155/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1360 - val_loss: 0.1432\n",
      "Epoch 156/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1371 - val_loss: 0.1426\n",
      "Epoch 157/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1354 - val_loss: 0.1424\n",
      "Epoch 158/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1350 - val_loss: 0.1444\n",
      "Epoch 159/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1404 - val_loss: 0.1532\n",
      "Epoch 160/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1409 - val_loss: 0.1462\n",
      "Epoch 161/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1346 - val_loss: 0.1400\n",
      "Epoch 162/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1449 - val_loss: 0.1586\n",
      "Epoch 163/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1460 - val_loss: 0.1441\n",
      "Epoch 164/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1433 - val_loss: 0.1419\n",
      "Epoch 165/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1390 - val_loss: 0.1415\n",
      "Epoch 166/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1377 - val_loss: 0.1433\n",
      "Epoch 167/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1326 - val_loss: 0.1394\n",
      "Epoch 168/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1317 - val_loss: 0.1384\n",
      "Epoch 169/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1374 - val_loss: 0.1410\n",
      "Epoch 170/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1392 - val_loss: 0.1558\n",
      "Epoch 171/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1488 - val_loss: 0.1410\n",
      "Epoch 172/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1431 - val_loss: 0.1579\n",
      "Epoch 173/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1469 - val_loss: 0.1498\n",
      "Epoch 174/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1374 - val_loss: 0.1440\n",
      "Epoch 175/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1405 - val_loss: 0.1646\n",
      "Epoch 176/10000\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 0.1373 - val_loss: 0.1492\n",
      "Epoch 177/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1373 - val_loss: 0.1474\n",
      "Epoch 178/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1404 - val_loss: 0.1420\n",
      "Epoch 179/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1401 - val_loss: 0.1489\n",
      "Epoch 180/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1436 - val_loss: 0.1406\n",
      "Epoch 181/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1377 - val_loss: 0.1484\n",
      "Epoch 182/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1352 - val_loss: 0.1499\n",
      "Epoch 183/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1313 - val_loss: 0.1407\n",
      "Epoch 184/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1330 - val_loss: 0.1378\n",
      "Epoch 185/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1385 - val_loss: 0.1545\n",
      "Epoch 186/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1411 - val_loss: 0.1404\n",
      "Epoch 187/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1362 - val_loss: 0.1415\n",
      "Epoch 188/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1343 - val_loss: 0.1474\n",
      "Epoch 189/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1393 - val_loss: 0.1508\n",
      "Epoch 190/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1350 - val_loss: 0.1448\n",
      "Epoch 191/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1490 - val_loss: 0.1528\n",
      "Epoch 192/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1383 - val_loss: 0.1426\n",
      "Epoch 193/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1311 - val_loss: 0.1510\n",
      "Epoch 194/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1344 - val_loss: 0.1412\n",
      "Epoch 195/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1390 - val_loss: 0.1413\n",
      "Epoch 196/10000\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.1380 - val_loss: 0.1470\n",
      "Epoch 197/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1340 - val_loss: 0.1384\n",
      "Epoch 198/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1345 - val_loss: 0.1490\n",
      "Epoch 199/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1386 - val_loss: 0.1396\n",
      "Epoch 200/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1359 - val_loss: 0.1517\n",
      "Epoch 201/10000\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.1332 - val_loss: 0.1445\n",
      "Epoch 202/10000\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.1310 - val_loss: 0.1386\n",
      "Epoch 203/10000\n",
      "37/40 [==========================>...] - ETA: 0s - loss: 0.1311"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m autoencoder_72\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautoencoder_trivial/ckp/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder_72\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder_72.load_weights(\"autoencoder_trivial/ckp/\")\n",
    "training_history = autoencoder_72.fit(X_train, X_train,\n",
    "                                      batch_size=128,\n",
    "                                      epochs=10000,\n",
    "                                      shuffle=True,\n",
    "                                      validation_data=(X_val, X_val),\n",
    "                                      callbacks=[tensorboard_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4a072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
