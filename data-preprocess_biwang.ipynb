{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26042f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"./lib\")\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35250e60-00a3-49d2-b61a-88e044425038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataSets by loading files:['../data/perdigao_era5_2020.nc', '../data/perdigao_low_res_1H_2020.nc', '../data/perdigao_high_res_1H_2020.nc']\n",
      "Removing NAN indices along dimension:time\n",
      "WARNING! This will change the oringal DataSets!\n",
      "Searching NAN in DataSets: dict_keys(['perdigao_era5_2020', 'perdigao_low_res_1H_2020', 'perdigao_high_res_1H_2020'])...\n",
      "Checking nan pattern of variable: u100  for DataSet: perdigao_era5_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: v100  for DataSet: perdigao_era5_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: t2m  for DataSet: perdigao_era5_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: i10fg  for DataSet: perdigao_era5_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "NAN pattern along dimension: time, is CONSISTENT for all other coords, with absolute_height excluded\n",
      "Checking nan pattern of variable: std  for DataSet: perdigao_low_res_1H_2020\n",
      "Total number of NAN: (1446912,), along (157,) time indicies\n",
      "Checking nan pattern of variable: temp  for DataSet: perdigao_low_res_1H_2020\n",
      "Total number of NAN: (1446912,), along (157,) time indicies\n",
      "Checking nan pattern of variable: v  for DataSet: perdigao_low_res_1H_2020\n",
      "Total number of NAN: (1446912,), along (157,) time indicies\n",
      "Checking nan pattern of variable: vel  for DataSet: perdigao_low_res_1H_2020\n",
      "Total number of NAN: (1446912,), along (157,) time indicies\n",
      "NAN pattern along dimension: time, is CONSISTENT for all other coords, with absolute_height excluded\n",
      "Checking nan pattern of variable: std  for DataSet: perdigao_high_res_1H_2020\n",
      "Total number of NAN: (9363456,), along (254,) time indicies\n",
      "Checking nan pattern of variable: temp  for DataSet: perdigao_high_res_1H_2020\n",
      "Total number of NAN: (9363456,), along (254,) time indicies\n",
      "Checking nan pattern of variable: v  for DataSet: perdigao_high_res_1H_2020\n",
      "Total number of NAN: (9363456,), along (254,) time indicies\n",
      "Checking nan pattern of variable: vel  for DataSet: perdigao_high_res_1H_2020\n",
      "Total number of NAN: (9363456,), along (254,) time indicies\n",
      "NAN pattern along dimension: time, is CONSISTENT for all other coords, with absolute_height excluded\n",
      "nan indices to be removed are: [1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621\n",
      " 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1944 1945 1946 1947\n",
      " 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961\n",
      " 1962 1963 1964 1965 1966 1967 4165 4166 4167 4168 4169 4170 4171 4172\n",
      " 4173 4174 4175 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199\n",
      " 4453 4454 4455 4456 4457 4458 4459 4460 4461 4462 4463 4476 4477 4478\n",
      " 4479 4480 4481 4482 4483 4484 4485 4486 4487 4500 4501 4502 4503 4504\n",
      " 4505 4506 4507 4508 4509 4510 4511 4525 4526 4527 4528 4529 4530 4531\n",
      " 4532 4533 4534 4535 4669 4670 4671 4672 4673 4674 4675 4676 4677 4678\n",
      " 4679 4693 4694 4695 4696 4697 4698 4699 4700 4701 4702 4703 4718 4719\n",
      " 4720 4721 4722 4723 4724 4725 4726 4727 4740 4741 4742 4743 4744 4745\n",
      " 4746 4747 4748 4749 4750 4751 4764 4765 4766 4767 4768 4769 4770 4771\n",
      " 4772 4773 4774 4775 4788 4789 4790 4791 4792 4793 4794 4795 4796 4797\n",
      " 4798 4799 4813 4814 4815 4816 4817 4818 4819 4820 4821 4822 4823 4837\n",
      " 4838 4839 4840 4841 4842 4843 4844 4845 4846 4847 4980 4981 4982 4983\n",
      " 4984 4985 4986 4987 4988 4989 4990 4991 5053 5054 5055 5056 5057 5058\n",
      " 5059 5060 5061 5062 5063 5221 5222 5223 5224 5225 5226 5227 5228 5229\n",
      " 5230 5231 8568 8569 8570 8571 8572 8573 8574 8575 8576 8577 8578 8579\n",
      " 8580 8581 8582 8583 8584 8585 8586 8587 8588 8589 8590 8591]\n",
      "Checking if there's NAN in datasets:dict_keys(['perdigao_era5_2020', 'perdigao_low_res_1H_2020', 'perdigao_high_res_1H_2020'])\n",
      "Searching NAN in DataSets: dict_keys(['perdigao_era5_2020', 'perdigao_low_res_1H_2020', 'perdigao_high_res_1H_2020'])...\n",
      "Checking nan pattern of variable: u100  for DataSet: perdigao_era5_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: v100  for DataSet: perdigao_era5_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: t2m  for DataSet: perdigao_era5_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: i10fg  for DataSet: perdigao_era5_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "NAN pattern along dimension: time, is CONSISTENT for all other coords, with absolute_height excluded\n",
      "Checking nan pattern of variable: std  for DataSet: perdigao_low_res_1H_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: temp  for DataSet: perdigao_low_res_1H_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: v  for DataSet: perdigao_low_res_1H_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: vel  for DataSet: perdigao_low_res_1H_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "NAN pattern along dimension: time, is CONSISTENT for all other coords, with absolute_height excluded\n",
      "Checking nan pattern of variable: std  for DataSet: perdigao_high_res_1H_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: temp  for DataSet: perdigao_high_res_1H_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: v  for DataSet: perdigao_high_res_1H_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "Checking nan pattern of variable: vel  for DataSet: perdigao_high_res_1H_2020\n",
      "Total number of NAN: (0,), along (0,) time indicies\n",
      "NAN pattern along dimension: time, is CONSISTENT for all other coords, with absolute_height excluded\n",
      "NAN all get removed! Saving preprocessed data!\n",
      "Saving DataSet: perdigao_era5_2020 to ../data/preprocessed/perdigao_era5_2020.nc\n",
      "Saving DataSet: perdigao_low_res_1H_2020 to ../data/preprocessed/perdigao_low_res_1H_2020.nc\n",
      "Saving DataSet: perdigao_high_res_1H_2020 to ../data/preprocessed/perdigao_high_res_1H_2020.nc\n",
      "Loading again DataSet: perdigao_era5_2020 from ../data/preprocessed/perdigao_era5_2020.nc\n",
      "Loading again DataSet: perdigao_low_res_1H_2020 from ../data/preprocessed/perdigao_low_res_1H_2020.nc\n",
      "Loading again DataSet: perdigao_high_res_1H_2020 from ../data/preprocessed/perdigao_high_res_1H_2020.nc\n",
      "DataSets:dict_keys(['perdigao_era5_2020', 'perdigao_low_res_1H_2020', 'perdigao_high_res_1H_2020']) are all IO_safe.\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"../data/preprocessed/\"\n",
    "file_format= \"%s\"\n",
    "parameters = {\"nan_dim_along\":\"time\", \"nan_dim_irrelevant\":\"absolute_height\", \"output_folder\":output_folder,\"file_format\":file_format}\n",
    "list_of_data_set_path=['../data/perdigao_era5_2020.nc', '../data/perdigao_low_res_1H_2020.nc', '../data/perdigao_high_res_1H_2020.nc' ]\n",
    "data_preprocess(list_of_data_set_path, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf8733",
   "metadata": {},
   "source": [
    "# Compute the statistics of low and high resolution data for GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lr_u = ds_low_res[\"u\"].mean().to_numpy()\n",
    "mean_lr_v = ds_low_res[\"v\"].mean().to_numpy()\n",
    "mean_hr_u = ds_high_res[\"u\"].mean().to_numpy()\n",
    "mean_hr_v = ds_high_res[\"v\"].mean().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_lr_u)\n",
    "print(mean_lr_v)\n",
    "print(mean_hr_u)\n",
    "print(mean_hr_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604323a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev_lr_u = ds_low_res[\"u\"].std().to_numpy()\n",
    "stddev_lr_v = ds_low_res[\"v\"].std().to_numpy()\n",
    "stddev_hr_u = ds_high_res[\"u\"].std().to_numpy()\n",
    "stddev_hr_v = ds_high_res[\"v\"].std().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stddev_lr_u)\n",
    "print(stddev_lr_v)\n",
    "print(stddev_hr_u)\n",
    "print(stddev_hr_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8780bf",
   "metadata": {},
   "source": [
    "# Standardrize the dataset for GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_high_res_std_u = ( ds_high_res[\"u\"] - mean_hr_u ) / stddev_hr_u\n",
    "da_high_res_std_v = ( ds_high_res[\"v\"] - mean_hr_v ) / stddev_hr_v\n",
    "print(da_high_res_std_u.mean().to_numpy())\n",
    "print(da_high_res_std_v.mean().to_numpy())\n",
    "print(da_high_res_std_u.std().to_numpy())\n",
    "print(da_high_res_std_v.std().to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd63748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so weird... If I stack them with axis=-1, then result of std and mean will be incorrect!\n",
    "np_hr_std_pre = np.stack((da_high_res_std_u.to_numpy(), da_high_res_std_v.to_numpy()), axis=0)\n",
    "print(np_hr_std_pre.shape)\n",
    "\n",
    "np_hr_std = np_hr_std_pre.transpose(1,2,3,0)\n",
    "print(np_hr_std.shape)\n",
    "\n",
    "print(np_hr_std.std(axis=(0,1,2)))\n",
    "print(np_hr_std.mean(axis=(0,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec471d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_low_res_std_u = ( ds_low_res[\"u\"] - mean_lr_u ) / stddev_lr_u\n",
    "da_low_res_std_v = ( ds_low_res[\"v\"] - mean_lr_v ) / stddev_lr_v\n",
    "print(da_low_res_std_u.mean().to_numpy())\n",
    "print(da_low_res_std_v.mean().to_numpy())\n",
    "print(da_low_res_std_u.std().to_numpy())\n",
    "print(da_low_res_std_v.std().to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf465d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so weird... If I stack them with axis=-1, then result of std and mean will be incorrect!\n",
    "np_lr_std_pre = np.stack((da_low_res_std_u.to_numpy(), da_low_res_std_v.to_numpy()), axis=0)\n",
    "print(np_lr_std_pre.shape)\n",
    "\n",
    "np_lr_std = np_lr_std_pre.transpose(1,2,3,0)\n",
    "print(np_lr_std.shape)\n",
    "\n",
    "print(np_lr_std.std(axis=(0,1,2)))\n",
    "print(np_lr_std.mean(axis=(0,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_mean = np.array([mean_lr_u, mean_lr_v])\n",
    "hr_mean = np.array([mean_hr_u, mean_hr_v])\n",
    "lr_stddev = np.array([stddev_lr_u, stddev_lr_v])\n",
    "hr_stddev = np.array([stddev_hr_u, stddev_hr_v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('processed_data/np_gan_standard.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"np_lr\",  data=np_lr_std)\n",
    "    hf.create_dataset(\"np_hr\",  data=np_hr_std)\n",
    "    hf.create_dataset(\"np_lr_mean\",  data=lr_mean)\n",
    "    hf.create_dataset(\"np_hr_mean\",  data=hr_mean)\n",
    "    hf.create_dataset(\"np_lr_stddev\",  data=lr_stddev)\n",
    "    hf.create_dataset(\"np_hr_stddev\",  data=hr_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('processed_data/np_gan_standard.h5', 'r') as hf:\n",
    "    data_lr = hf['np_lr'][:]\n",
    "    data_lr_mean = hf['np_lr_mean'][:]\n",
    "    data_lr_stddev = hf['np_lr_stddev'][:]\n",
    "    data_hr = hf['np_hr'][:]\n",
    "    data_hr_mean = hf['np_hr_mean'][:]\n",
    "    data_hr_stddev = hf['np_hr_stddev'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_equal(np_lr_std, data_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa710d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_equal(np_hr_std, data_hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7114fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_lr_reopen = data_lr * data_lr_stddev + data_lr_mean\n",
    "print(np.max(np_lr_reopen[:,:,:,0] - ds_low_res[\"u\"].to_numpy()))\n",
    "print(np.max(np_lr_reopen[:,:,:,1] - ds_low_res[\"v\"].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_hr_reopen = data_hr * data_hr_stddev + data_hr_mean\n",
    "print(np.max(np_hr_reopen[:,:,:,0] - ds_high_res[\"u\"].to_numpy()))\n",
    "print(np.max(np_hr_reopen[:,:,:,1] - ds_high_res[\"v\"].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f3ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.0",
   "language": "python",
   "name": "tensorflow-2.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
